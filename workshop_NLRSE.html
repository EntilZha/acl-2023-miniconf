


<!DOCTYPE html>
<html lang="en">

<head>
  
  <!-- Required meta tags -->
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />


  <!-- External Javascript libs  -->
  <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
    integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
    integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
    crossorigin="anonymous"></script>


  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
    integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
    integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
    integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>

  <!-- https://developer.snapappointments.com/bootstrap-select/ -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/js/bootstrap-select.min.js"></script>

  <!-- Library libs -->
  <script src="static/js/typeahead.bundle.js"></script>

  <script src="https://craig.global.ssl.fastly.net/js/mousetrap/mousetrap.min.js?a4098"></script>

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.0/css/all.css"
    integrity="sha384-lZN37f5QGtY3VHgisS14W3ExzMWZxybE1SJSEsQp9S+oqd12jhcu+A56Ebc1zFSJ" crossorigin="anonymous">

  <!-- External CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
    integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

  <!-- External Fonts (no google for china) -->
  <link href="static/css/Lato.css" rel="stylesheet" />
  <link href="static/css/Exo.css" rel="stylesheet" />
  <link href="static/css/Cuprum.css" rel="stylesheet" />

  <link rel="stylesheet" href="static/css/main.css" />
  <link rel="stylesheet" href="static/css/chats-modal.css" />
  <link rel="stylesheet" href="static/css/lazy_load.css" />
  <link rel="stylesheet" href="static/css/typeahead.css" />
  <script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.3.200/build/pdf.min.js"></script>
  <script src="static/js/pdf_render.js"></script>

  <title>ACL2023: NLRSE</title>
  

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="180x180" href="static/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/favicon/favicon-16x16.png">
  <link rel="manifest" href="static/favicon/site.webmanifest">
  <link rel="mask-icon" href="static/favicon/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="static/favicon/favicon.ico">
  <meta name="msapplication-TileColor" content="#2d89ef">
  <meta name="msapplication-config" content="static/favicon/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>

<body>
  <!-- NAV -->
  
  <!-- ('https://2023.aclweb.org/faq/', 'FAQ'), -->
  <!-- ('https://2023.aclweb.org/committees/organization/', 'Organizers'), -->
  <!-- ('https://2023.aclweb.org/sponsors/', 'Sponsors') -->

  <!-- Add back ('livestream.html', 'Livestream'),
    ('about.html', 'Help'),
    ('plenary_sessions.html', 'Plenary'),
    ('livestream.html', 'Livestream'),
    (config.gather_town , 'Gather'),
    for a new conference
    -->

  <nav class="navbar sticky-top navbar-expand-lg navbar-light  bg-emnlp mr-auto customnav" id="main-nav">
    <div class="container">
      <a class="navbar-brand" href="index.html">
        <img class="logo" src="static/images/acl2023/acl-logo-2023.png" height="45px"
          width="auto" alt="ACL 2023" />
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
        <ul class="navbar-nav ml-auto">
          
          <li class="nav-item ">
            
            <a class="nav-link" href="index.html">Home</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="schedule.html">Schedule</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="sessions.html">Sessions</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="papers.html">Papers</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="tutorials.html">Tutorials</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="workshops.html">Workshops</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="map.html">Map</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://acl.rocket.chat" target="_blank"><u>Chat</u></a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://2023.aclweb.org/" target="_blank"><u>Main Site</u></a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://2023.aclweb.org/downloads/acl2023-handbook.pdf" target="_blank"><u>Handbook</u></a>
            
          </li>
          
        </ul>
      </div>
    </div>
  </nav>
  

  
  <!-- User Overrides -->
   

      
      <div class="container">
        
    <!-- Heading -->
    <div class="heading">
       
    </div>
    <div class="tabs pt-3">
      <!-- Tabs -->
      <div class="tabs pt-3">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      NLRSE
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      Organizers: 
      
      <a href="papers.html?filter=authors&search=Peter Clark&program=all"
        class="text-primary link-primary">Peter Clark</a>,
      
      <a href="papers.html?filter=authors&search=Ellie Pavlick&program=all"
        class="text-primary link-primary">Ellie Pavlick</a>,
      
      <a href="papers.html?filter=authors&search=Denny Zhou&program=all"
        class="text-primary link-primary">Denny Zhou</a>,
      
      <a href="papers.html?filter=authors&search=Noah Goodman&program=all"
        class="text-primary link-primary">Noah Goodman</a>,
      
      <a href="papers.html?filter=authors&search=Sarah Wiegreffe&program=all"
        class="text-primary link-primary">Sarah Wiegreffe</a>,
      
      <a href="papers.html?filter=authors&search=Felix Hill&program=all"
        class="text-primary link-primary">Felix Hill</a>
      
    </h3>
    <div class="text-muted text-center">
      With recent scaling of large pre-trained Transformer language models (LLMs), the scope of feasible NLP tasks has broadened. Significant recent work has focused on tasks that require some kind of natural language reasoning. A trajectory in question answering has led us from extraction-oriented datasets like SQuAD to “multi-hop” reasoning datasets like HotpotQA and StrategyQA. Although LLMs have shown remarkable performance on most NLP tasks, it is often unclear why their answers follow from what they know. To address this gap, a new class of explanation techniques has emerged which play an integral part in structuring the reasoning necessary to solve these datasets. For example, the chain-of-thought paradigm leverages explanations as vehicles for LLMs to mimic human reasoning processes. Entailment trees offer a way to ground multi-step reasoning in a collection of verifiable steps. Frameworks like SayCan bridge high-level planning in language and with low-level action trajectories. As a result, we see a confluence of methods blending explainable machine learning/NLP, classical AI (especially theorem proving), and cognitive science (how do humans structure explanations?). This workshop aims to bring together a diverse set of perspectives from these different traditions and attempt to establish common ground for how these various kinds of explanation structures can tackle a broad class of reasoning problems in natural language and beyond.
    </div>
    <div class="text-center p-3">
      
      <a href="https://nl-reasoning-workshop.github.io/" target="_blank" class="link-success">
        External Website
      </a>
      


    </div>

  </div>

  <!-- Schedule -->
  <!-- 
-->

  <div class="container" style="background-color:white; padding: 0px;">
    <div class="text-muted text-center">
      You can open
      the
      <a href="https://acl.rocket.chat/channel/workshop-NLRSE" target="_blank">
        #workshop-NLRSE
      </a>
      channel in separate windows.

    </div>

    <div class="row m-2">
      <div class="container" style="background-color:white; padding: 0px;">
        <!-- Chat -->
        <div id="gitter" class="slp">
          <iframe frameborder="0"
            src="https://acl.rocket.chat/channel/workshop-NLRSE?layout=embedded" height="700px"
            width="100%"></iframe>
        </div>
      </div>
    </div>
    <div class="row"><div class="col-12"><h3 class="text-center">Workshop Papers</h3></div></div>
    <div class="row" style='margin: 15px;'>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Logical Reasoning over Natural Language as Knowledge Representation: A Survey
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria</h5>
            <p class="card-text">Logical reasoning is central to human cognition and intelligence. Past research of logical reasoning within AI uses formal language as knowledge representation\textasciitilde{}(and symbolic reasoners). However, reasoning with formal language has proved challenging\textasciitilde{}(e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation\textasciitilde{}(and pretrained language models as reasoners), including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, desirable tasks \textbackslash{}\&amp; methods in the future, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods.</p>
            <a href="/paper_ACL_41.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, Smaranda Muresan</h5>
            <p class="card-text">Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL-E-2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models and Diffusion Models. We use GPT-3 with Chain-of-Thought prompting to generate text that represents a visual elaboration of the linguistic metaphor, containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models. Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-quality dataset containing 6,476 visual metaphors. Evaluation by professional illustrators show the promise of LLM-Diffusion Model collaboration for this task. We also perform an intrinsic and an extrinsic evaluation using a downstream task: visual entailment. Fine-tuning a state-of-the-art vision-language model on our dataset leads to 23-point improvement in accuracy compared to its performance when finetuned on SNLI-VE, a large-scale visual entailment dataset.</p>
            <a href="/paper_ACL_58.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Negated Complementary Commonsense using Large Language Models
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Navid Rezaei, Marek Reformat</h5>
            <p class="card-text">Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under: https://github.com/navidre/negated\_complementary\_commonsense.</p>
            <a href="/paper_ACL_59.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            The Role of Semantic Parsing in Understanding Procedural Text
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Hossein Rajaby Faghihi, Parisa Kordjamshidi, Choh Man Teng, James Allen</h5>
            <p class="card-text">In this paper, we investigate whether symbolic semantic representations, extracted from deep semantic parsers, can help to reason over the states of involved entities in a procedural text. We consider a deep semantic parser\textasciitilde{}(TRIPS) and semantic role labeling as two sources of semantic parsing knowledge. First, we propose PROPOLIS, a symbolic parsing-based procedural reasoning framework.Second, we integrate semantic parsing information into state-of-the-art neural models to conduct procedural reasoning.Our experiments indicate that explicitly incorporating such semantic knowledge improves procedural understanding. This paper presents new metrics for evaluating procedural reasoning tasks that clarify the challenges and identify differences among neural, symbolic, and integrated models.</p>
            <a href="/paper_ACL_61.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Interpretable Math Word Problem Solution Generation Via Step-by-step Planning
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Mengxue Zhang, Zichao Wang, Zhichao Yang, Weiqi Feng, Andrew Lan</h5>
            <p class="card-text">We study the problem of generating coherent and correct intermediate solution steps for math word problems (MWPs). Solutions to MWPs with step-by-step explanations are valuable, especially in education, to help students better comprehend problem-solving strategies. Most existing approaches narrowly focus on obtaining the final correct answer. A few recent approaches leverage intermediate solution steps to improve final answer correctness but often cannot generate coherent steps with a clear solution strategy. Contrary to existing work, we focus on improving the correctness and coherence of the intermediate solutions steps. We propose a step-by-step planning method for intermediate solution generation, which strategically plans the generation of the next solution step based on the MWP and the previous solution steps. Our approach first plans the next step by predicting the necessary math operation needed to proceed given history steps, then generates the next step, token-by-token, by prompting a language model with the predicted math operation. Experiments on the GSM8K dataset demonstrate that our method improves the accuracy and interpretability of the solution by both automatic metrics and human evaluation.</p>
            <a href="/paper_ACL_62.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            SCOTT: Self-Consistent Chain-of-Thought Distillation
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang Ren</h5>
            <p class="card-text">Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM&#39;s predictions or faithfully justify the decisions. In this work, we propose a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that while yielding comparable performance,  our method leads to a more faithful model than baselines. Further analysis shows that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales.</p>
            <a href="/paper_ACL_65.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Hierarchical Prompting Assists Large Language Model on Web Navigation
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Chi-fan Lo, Abishek Sridhar, Hao Zhu, Frank F. Xu, Shuyan Zhou</h5>
            <p class="card-text">Prompting has been utilized to exploit large language models (LLM) for sequential planning tasks within interactive settings. In this paper, we propose a novel prompting approach, Actor-Summarizer-Hierarchical prompting, for interactive web navigation. Diverging from previous prompting approaches that always put the full state (eg a web page) to the prompt, we propose to first construct an action-aware state which is more condensed and relevant with a dedicated summarizer prompt. The resulting state is concatenated to the summarized history and fed to an actor prompt to predict the next action. This hierarchical mechanism is especially useful since the full state of a step in web navigation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanism with the same LLM by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.</p>
            <a href="/paper_ACL_66.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Towards Reasoning in Large Language Models: Survey, Implication, and Reflection
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Jie Huang, Kevin Chen-chuan Chang</h5>
            <p class="card-text">Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.</p>
            <a href="/paper_ACL_67.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Alex Mei, Sharon Levy, William Yang Wang</h5>
            <p class="card-text">Users&#39; physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such texts may arise from everyday scenarios and are challenging to detect as harmful. We propose FARM, a novel framework that leverages external knowledge for trustworthy rationale generation in the context of safety. In particular, FARM foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. It then uses this knowledge to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. Our experiments show that FARM obtains state-of-the-art results on the SafeText dataset, showing absolute improvement in safety classification accuracy by 5.9 points.</p>
            <a href="/paper_ACL_68.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Teaching Large Language Models to Self-Debug
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Xinyun Chen, Maxwell Lin, Nathanael Schaerli, Denny Zhou</h5>
            <p class="card-text">Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging. In this work, we propose self-debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that self-debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, self-debugging with code explanation consistently improves the baseline by 2-3\%, and improves the prediction accuracy on problems of the hardest label by 9\%\$. On TransCoder and MBPP where unit tests are available, self-debugging can improve the baseline accuracy by 12\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, self-debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.</p>
            <a href="/paper_ACL_71.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            QAMPARI: A Benchmark for Open-domain Questions with Many Answers
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Samuel Amouyal, Tomer Wolfson, Ohad Rubin, Ori Yoran, Jonathan Herzig, Jonathan Berant</h5>
            <p class="card-text">Existing benchmarks for open-domain question answering (ODQA) typically focus on questions whose answers are all in a single paragraph. By contrast, many natural questions, such as &#34;What players were drafted by the Brooklyn Nets? have a long list of answers extracted from multiple paragraphs. Answering such questions requires retrieving and reading many passages from a large corpus. We introduce QAMPARI, an ODQA benchmark, where answers are lists of entities, spread across many paragraphs. We created QAMPARI by (a) generating questions with multiple answers from Wikipedia&#39;s knowledge graph and tables, (b) automatically pairing answers with supporting evidence in Wikipedia paragraphs, and (c) manually paraphrasing questions and validating each answer. Across a wide range of ODQA models, we find that QAMPARI is challenging in terms of both passage retrieval and answer generation, with models reaching an F1 score of 32.8 at best. We view QAMPARI as a valuable resource for ODQA research, which will aid to develop models that handle a broad range of question types, including single and multianswer questions.</p>
            <a href="/paper_ACL_72.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Jinheon Baek, Alham Fikri Aji, Amir Saffari</h5>
            <p class="card-text">Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user&#39;s question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48\% in average, across multiple LLMs of various sizes.</p>
            <a href="/paper_ACL_73.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Generative Multi-hop Retrieval
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Hyunji Lee, Sohee Yang, Hanseok Oh, Minjoon Seo</h5>
            <p class="card-text">A common practice for text retrieval is to use an encoder to map the documents and the query to a common vector space and perform a nearest neighbor search (NNS); multi-hop retrieval also often adopts the same paradigm, usually with a modification of iteratively reformulating the query vector so that it can retrieve different documents at each hop. However, such a bi-encoder approach has limitations in multi-hop settings; (1) the reformulated query gets longer as the number of hops increases, which further tightens the embedding bottleneck of the query vector, and (2) it is prone to error propagation. In this paper, we focus on alleviating these limitations in multi-hop settings by formulating the problem in a fully generative way. We propose an encoder-decoder model that performs multi-hop retrieval by simply generating the entire text sequences of the retrieval targets, which means the query and the documents interact in the language model&#39;s parametric space rather than L2 or inner product space as in the bi-encoder approach. Our approach, Generative Multi-hop Retrieval (GMR), consistently achieves comparable or higher performance than bi-encoder models in five datasets while demonstrating superior GPU memory and storage footprint.</p>
            <a href="/paper_ACL_74.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Causal Reasoning of Entities and Events in Procedural Texts
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Li Zhang, Hainiu Xu, Yue Yang, Shuyan Zhou, Weiqiu You, Manni Arora, Chris Callison-burch</h5>
            <p class="card-text">Entities and events are crucial to natural language reasoning and common in procedural texts. Existing work has focused either exclusively on entity state tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one would burn themselves by touching the pan), while these two tasks are often causally related. We propose CREPE, the first benchmark on causal reasoning of event plausibility and entity states. We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1. We boost model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code. By injecting the causal relations between entities and events as intermediate reasoning steps in our representation, we further boost the performance to .67 F1. Our findings indicate not only the challenge that CREPE brings for language models, but also the efficacy of code-like prompting combined with chain-of-thought prompting for multihop event reasoning.</p>
            <a href="/paper_ACL_75.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Can In-context Learners Learn a Reasoning Concept from Demonstrations?
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Michal Tefnik, Marek Kadlcik</h5>
            <p class="card-text">Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations.However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input.However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models&#39; ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.To disentangle models&#39; in-context learning ability independent of models&#39; memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in few-shot demonstrations.We find that smaller models are more sensitive to the presented concepts. While some of the models are able to benefit from concept-presenting demonstrations for each assessed concept, we find that none of the assessed in-context learners can benefit from all presented reasoning concepts consistently, leaving the in-context concept learning an open challenge.</p>
            <a href="/paper_ACL_76.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            DREAM: Improving Situational QA by First Elaborating the Situation
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Yuling Gu, Bhavana Dalvi Mishra, Peter Clark</h5>
            <p class="card-text">When people answer questions about a specific situation, e.g., &#34;I cheated on my mid-term exam last week. Was that wrong?, cognitive science suggests that they form a mental picture of that situation before answering. While we do not know how language models (LMs) answer such questions, we conjecture that they may answer more accurately if they are also provided with additional details about the question situation, elaborating the &#34;scene. To test this conjecture, we train a new model, DREAM, to answer questions that elaborate the scenes that situated questions are about, and then provide those elaborations as additional context to a question-answering (QA) model. We find that DREAM is able to create better scene elaborations (more accurate, useful, and consistent) than a representative state-of-the-art, zero-shot model (Macaw). We also find that using the scene elaborations as additional context improves the answer accuracy of a downstream QA system, including beyond that obtainable by simply further fine-tuning the QA system on DREAM&#39;s training data. These results suggest that adding focused elaborations about a situation can improve a system&#39;s reasoning about it, and may serve as an effective way of injecting new scenario-based knowledge into QA models.</p>
            <a href="/paper_ACL_77.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Cheng-yu Hsieh, Chun-liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-yu Lee, Tomas Pfister</h5>
            <p class="card-text">Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our 770M T5 model outperforms the 540B PaLM model using only 80\% of available data on a benchmark task.</p>
            <a href="/paper_ACL_78.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Effect Graph: Effect Relation Extraction for Explanation Generation
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Jonathan Kobbe, Ioana Hulpu, Heiner Stuckenschmidt</h5>
            <p class="card-text">Argumentation is an important means of communication. For describing especially arguments about consequences, the notion of effect relations has been introduced recently. We propose a method to extract effect relations from large text resources and apply it on encyclopedic and argumentative texts. By connecting the extracted relations, we generate a knowledge graph which we call effect graph. For evaluating the effect graph, we perform crowd and expert annotations and create a novel dataset. We demonstrate a possible use case of the effect graph by proposing a method for explaining arguments from consequences.</p>
            <a href="/paper_ACL_79.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Explaining Competitive-Level Programming Solutions using LLMs
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Jierui Li, Szymon Tworkowski, Yingying Wu, Raymond Mooney</h5>
            <p class="card-text">In this paper, we approach competitive-level programming problem-solving as a composite task of reasoning and code generation. We propose a novel method to automatically annotate natural language explanations to the \textless{}problem, solution\textgreater{} pairs. We show that despite poor performance in solving competitive-level programming problems, state-of-the-art LLMs exhibit a strong capacity in describing and explaining their solutions. Our explanation generation methodology can generate a structured solution explanation for the problem while containing the description and analysis. To evaluate the quality of the annotated explanations, we examine their effectiveness in two aspects: 1) satisfying the human programming expert who authored the oracle solution, and 2) aiding LLMs in solving problems more effectively. The experimental results on the CodeContests dataset demonstrate that while LLM GPT3.5&#39;s and GPT-4&#39;s abilities in describing the solution are comparable, GPT-4 shows a better understanding of the key idea behind the solution.</p>
            <a href="/paper_ACL_80.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Badr Alkhamissi, Siddharth Verma, Ping Yu, Zhijing Jin, Asli Celikyilmaz, Mona Diab</h5>
            <p class="card-text">We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model&#39;s performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4\%) and Analogical (+13.9\%) reasoning, as well as skills that exhibit negligible or negative effects.</p>
            <a href="/paper_ACL_82.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Boxin Wang, Wei Ping, Peng Xu, Lawrence Mcafee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao</h5>
            <p class="card-text">Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pretrained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, R ETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO ++, which largely improves the open-domain QA results of the original RETRO and significantly outperforms retrieval-augmented GPT across different model sizes. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models.</p>
            <a href="/paper_ACL_83.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Grounded physical language understanding with probabilistic programs and simulated worlds
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Cedegao Zhang, Lionel Wong, Gabriel Grand, Josh Tenenbaum</h5>
            <p class="card-text">Human language richly invokes our intuitive physical knowledge. We talk about physical objects, scenes, properties, and events; and we can make predictions and draw inferences about physical worlds described entirely in language. Understanding this everyday language requires inherently probabilistic reasoningover possible physical worlds invoked in language and over uncertainty inherent to those physical worlds. In this paper, we propose PiLoT, a neurosymbolic generative model that translates language into probabilistic programs grounded in a physics engine. Our model integrates a large language model to robustly parse language into program expressions and uses a probabilistic physics engine to support inferences over scenes described in language. We construct a linguistic reasoning benchmark based on prior psychophysics experiments that requires reasoning about physical outcomes based on linguistic scene descriptions. We show that PiLoT well predicts human judgments and outperforms baseline large language models across this battery of tasks.</p>
            <a href="/paper_ACL_87.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Designing harder benchmarks for evaluating zero-shot generalizability in Question Answering over Knowledge Bases
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Ritam Dutt, Sopan Khosla, Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah</h5>
            <p class="card-text">Most benchmarks for question answering on knowledge bases (KBQA) operate with the i.i.d. assumption. Recently, the GrailQA dataset was established to evaluate  zero-shot generalization capabilities of KBQA models. Reasonable performance of current KBQA systems on the zero-shot GrailQA split hints that the field might be moving towards more generalizable systems. In this work, we observe a bias in the GrailQA dataset towards simpler one or two-hop questions which results in an inaccurate assessment of the aforementioned prowess. We propose GrailQA++, a challenging zero-shot KBQA test set that contains a larger number of questions relying on complex reasoning.  We leverage the concept of reasoning paths to control the complexity of the questions and to ensure that our proposed test set has a fair distribution of simple and complex questions. Evaluating existing KBQA models on this new test set shows that they suffer a substantial drop in performance as compared to the GrailQA zero-shot split. This highlights the non-generalizability of existing models and the necessity for harder benchmarks. Our analysis reveals how reasoning paths can be used to understand complementary strengths of different KBQA models, and provide a deeper insight into model mispredictions.</p>
            <a href="/paper_ACL_90.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Deductive Additivity for Planning of Natural Language Proofs
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett</h5>
            <p class="card-text">Current natural language systems designed for multi-step claim validation typically operate in two phases: retrieve a set of relevant premise statements using heuristics (planning), then generate novel conclusions from those statements using a large language model (deduction). The planning step often requires expensive Transformer operations and does not scale to arbitrary numbers of premise statements. In this paper, we investigate whether efficient planning heuristic is possible via embedding spaces compatible with deductive reasoning. Specifically, we evaluate whether embedding spaces exhibit a property we call deductive additivity: the sum of premise statement embeddings should be close to embeddings of conclusions based on those premises. We explore multiple sources of off-the-shelf dense embeddings in addition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. We study embedding models both intrinsically, evaluating whether the property of deductive additivity holds, and extrinsically, using them to assist planning in natural language proof generation. Lastly, we create a dataset, Single-Step Reasoning Contrast (SSRC), to further probe performance on various reasoning types. Our findings suggest that while standard embedding methods frequently embed conclusions near the sums of their premises, they fall short of being effective heuristics and lack the ability to model certain categories of reasoning.</p>
            <a href="/paper_ACL_91.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Case-Based Reasoning with Language Models for Classification of Logical Fallacies
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Zhivar Sourati, Filip Ilievski, Hng-n Sandlin, Alain Mermoud</h5>
            <p class="card-text">The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewer retrieved cases, and that the size of the case database has a negligible effect on the performance. Finally, we dive deeper into the relationship between the properties of the retrieved cases and the model performance.</p>
            <a href="/paper_ACL_92.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Xiao Liu, Da Yin, Chen Zhang, Yansong Feng, Dongyan Zhao</h5>
            <p class="card-text">Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are better causal reasoners. We further intervene on the prompts from different aspects, and discover that the key point is the programming structure.</p>
            <a href="/paper_ACL_93.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Neural-symbolic Contrastive Learning for Cross-domain Inference
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Mingyue Liu, Jialin Yu, Hao Cui, Sara Uckelman, Yang Long</h5>
            <p class="card-text">It has been suggested in literature that large pre-trained language models (PLMs) are able to suppress human-level performance for natural language inference (NLI) tasks. However, the failure of learning the underlying generalizations and the inconsistency to small textual perturbations rise doubt about whether models rely on adopting shallow heuristics to guess the correct label. To mitigate this issue, we propose a neural-symbolic contrastive learning framework inspired by Inductive Logic Programming (ILP) to better capture logical relationships from data. Unlike the usual methods for NLI tasks, our approach represents data as logic programs, sets of logic rules. We aim to learn an embedding space in which  the examples share as various as possible textual information with as similar as possible underlying logical meanings that are close together, and vice versa. Experimental results affirm this approach&#39;s ability to enhance the model&#39;s transferability performance.</p>
            <a href="/paper_ACL_95.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Sushma Anand Akoju, Robert Vacareanu, Eduardo Blanco, Haris Riaz, Mihai Surdeanu</h5>
            <p class="card-text">We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014). To this end, we modify the original texts using a set of phrases  modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis. Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules. We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios. We found that the performance of NLI models under the zero-shot setting is poor, especially for modified sentences with negation and existential quantifiers. After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and universal modifiers.</p>
            <a href="/paper_ACL_97.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            STREET: A Multi-Task Structured Reasoning and Explanation Benchmark
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henghui Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, William Yang Wang, Zhiheng Huang</h5>
            <p class="card-text">We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark. Unlike most existing question-answering (QA) datasets, we expect models to not only answer questions, but also produce step-by-step structured explanations describing how premises in the question are used to produce intermediate conclusions that can prove the correctness of a certain answer. We perform extensive evaluation with popular language models such as few-shot prompting GPT-3 and fine-tuned T5. We find that these models still lag behind human performance when producing such structured reasoning steps. We believe this work will provide a way for the community to better train and test systems on multi-step reasoning and explanations in natural language.</p>
            <a href="/paper_ACL_98.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Complementary Explanations for Effective In-Context Learning
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Veselin Stoyanov, Greg Durrett, Ramakanth Pasunuru</h5>
            <p class="card-text">Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in- context learning performance across three real- world tasks on multiple LLMs.</p>
            <a href="/paper_ACL_99.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
    </div>


    <script src="static/js/time-extend.js"></script>
    <script>
      $(document).ready(() => {
        add_local_tz('.session_times');
      })
    </script>

    
      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=None"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "None");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-left">
          <img src="static/images/acl2023/acl-logo-2023.png" height="45px"
            width="auto" align="center">
          <span class="lead">ACL 2023</span>
        </p>
        <p class="float-right"><a href="#" class="text-dark">Back to Top</a></p>
        <p class="text-center">© 2023 Association for Computational Linguistics</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
    <div class="modal left fade" id="chatsModal" tabindex="" role="dialog" aria-labelledby="exampleModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-dialog-scrollable" role="document" style="max-height: 100% !important;">
        <div class="modal-content" style="max-height: 100% !important;">
            <div class="modal-header">
                <h4 class="modal-title" id="exampleModalLabel">Active Chats</h4>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
            </div>
            <div class="modal-body">
                <div style="margin-bottom: 1em;">
    <div class="stats-last-update text-muted" id="highly-active-chats-last-update"></div>
    <a id="highly-active-chats-btn-refresh" href="#" class="text-primary" style="display: none;">Refresh now</a>
</div>

<div id="highly-active-chats-list">
</div>

<div class="text-center" id="highly-active-chats-progress-bar" style="margin-bottom: 80em;">
    <div class="spinner-border text-primary" style="margin-top: 3em; width: 3rem; height: 3rem;" role="status">
      <span class="sr-only">Loading...</span>
    </div>
</div>

<br/>
<p class="text-muted">
    <strong>How it works: </strong>
    We calculate the number of new messages for every channel in the last N seconds. Then, we sort them descendingly.
    Channels with no new messages will be randomly shuffled. Please note that the number of messages might not be accurate.
</p>

<script src="static/js/highly-active-chats.js"></script>
<script>
    let channel_stats_server = "https://emnlp2020-channels-stats.azure-api.net"
    $(document).ready( function () {
        $('[data-toggle="tooltip"]').tooltip();
        //load_stats();
    });
</script>

            </div>

            <div class="modal-footer">
                <button type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button>
            </div>
        </div>
    </div>
</div>
</body>

</html>