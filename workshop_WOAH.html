


<!DOCTYPE html>
<html lang="en">

<head>
  
  <!-- Required meta tags -->
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />


  <!-- External Javascript libs  -->
  <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
    integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
    integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
    crossorigin="anonymous"></script>


  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
    integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
    integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
    integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>

  <!-- https://developer.snapappointments.com/bootstrap-select/ -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/js/bootstrap-select.min.js"></script>

  <!-- Library libs -->
  <script src="static/js/typeahead.bundle.js"></script>

  <script src="https://craig.global.ssl.fastly.net/js/mousetrap/mousetrap.min.js?a4098"></script>

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.0/css/all.css"
    integrity="sha384-lZN37f5QGtY3VHgisS14W3ExzMWZxybE1SJSEsQp9S+oqd12jhcu+A56Ebc1zFSJ" crossorigin="anonymous">

  <!-- External CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
    integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

  <!-- External Fonts (no google for china) -->
  <link href="static/css/Lato.css" rel="stylesheet" />
  <link href="static/css/Exo.css" rel="stylesheet" />
  <link href="static/css/Cuprum.css" rel="stylesheet" />

  <link rel="stylesheet" href="static/css/main.css" />
  <link rel="stylesheet" href="static/css/chats-modal.css" />
  <link rel="stylesheet" href="static/css/lazy_load.css" />
  <link rel="stylesheet" href="static/css/typeahead.css" />
  <script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.3.200/build/pdf.min.js"></script>
  <script src="static/js/pdf_render.js"></script>

  <title>ACL2023: WOAH</title>
  

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="180x180" href="static/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/favicon/favicon-16x16.png">
  <link rel="manifest" href="static/favicon/site.webmanifest">
  <link rel="mask-icon" href="static/favicon/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="static/favicon/favicon.ico">
  <meta name="msapplication-TileColor" content="#2d89ef">
  <meta name="msapplication-config" content="static/favicon/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>

<body>
  <!-- NAV -->
  
  <!-- ('https://2023.aclweb.org/faq/', 'FAQ'), -->
  <!-- ('https://2023.aclweb.org/committees/organization/', 'Organizers'), -->
  <!-- ('https://2023.aclweb.org/sponsors/', 'Sponsors') -->

  <!-- Add back ('livestream.html', 'Livestream'),
    ('about.html', 'Help'),
    ('plenary_sessions.html', 'Plenary'),
    ('livestream.html', 'Livestream'),
    (config.gather_town , 'Gather'),
    for a new conference
    -->

  <nav class="navbar sticky-top navbar-expand-lg navbar-light  bg-emnlp mr-auto customnav" id="main-nav">
    <div class="container">
      <a class="navbar-brand" href="index.html">
        <img class="logo" src="static/images/acl2023/acl-logo-2023.png" height="45px"
          width="auto" alt="ACL 2023" />
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
        <ul class="navbar-nav ml-auto">
          
          <li class="nav-item ">
            
            <a class="nav-link" href="index.html">Home</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="schedule.html">Schedule</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="sessions.html">Sessions</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="papers.html">Papers</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="tutorials.html">Tutorials</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="workshops.html">Workshops</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="map.html">Map</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://acl.rocket.chat" target="_blank"><u>Chat</u></a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://app.gather.town/app/5OscLswEEuYf2Psb/ACL2023" target="_blank"><u>GatherTown</u></a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://2023.aclweb.org/" target="_blank"><u>Main Site</u></a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://2023.aclweb.org/downloads/acl2023-handbook.pdf" target="_blank"><u>Handbook</u></a>
            
          </li>
          
        </ul>
      </div>
    </div>
  </nav>
  

  
  <!-- User Overrides -->
   

      
      <div class="container">
        
    <!-- Heading -->
    <div class="heading">
       
    </div>
    <div class="tabs pt-3">
      <!-- Tabs -->
      <div class="tabs pt-3">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      WOAH
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      Organizers: 
      
      <a href="papers.html?filter=authors&search=Yi-Ling Chung&program=all"
        class="text-primary link-primary">Yi-Ling Chung</a>,
      
      <a href="papers.html?filter=authors&search=Aida Mostafazadeh Davani&program=all"
        class="text-primary link-primary">Aida Mostafazadeh Davani</a>,
      
      <a href="papers.html?filter=authors&search=Debora Nozza&program=all"
        class="text-primary link-primary">Debora Nozza</a>,
      
      <a href="papers.html?filter=authors&search=Paul Röttger&program=all"
        class="text-primary link-primary">Paul Röttger</a>,
      
      <a href="papers.html?filter=authors&search=Zeerak Talat&program=all"
        class="text-primary link-primary">Zeerak Talat</a>
      
    </h3>
    <div class="text-muted text-center">
      The goal of The Workshop on Online Abuse and Harms (WOAH) is to advance research that develops, interrogates and applies computational methods for detecting, classifying and modelling online abuse.
    </div>
    <div class="text-center p-3">
      
      <a href="https://www.workshopononlineabuse.com/" target="_blank" class="link-success">
        External Website
      </a>
      


    </div>

  </div>

  <!-- Schedule -->
  <!-- 
-->

  <div class="container" style="background-color:white; padding: 0px;">
    <div class="text-muted text-center">
      You can open
      the
      <a href="https://acl.rocket.chat/channel/workshop-WOAH" target="_blank">
        #workshop-WOAH
      </a>
      channel in separate windows.

    </div>

    <div class="row m-2">
      <div class="container" style="background-color:white; padding: 0px;">
        <!-- Chat -->
        <div id="gitter" class="slp">
          <iframe frameborder="0"
            src="https://acl.rocket.chat/channel/workshop-WOAH?layout=embedded" height="700px"
            width="100%"></iframe>
        </div>
      </div>
    </div>
    <div class="row"><div class="col-12"><h3 class="text-center">Workshop Papers</h3></div></div>
    <div class="row" style='margin: 15px;'>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            DeTexD: A Benchmark Dataset for Delicate Text Detection
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Artem Chernodub, Serhii Yavnyi, Oleksii Sliusarenko, Jade Razzaghi, Yichen Mo, Knar Hovakimyan</h5>
            <p class="card-text">Over the past few years, much research has been conducted to identify and regulate toxic language. However, few studies have addressed a broader range of sensitive texts that are not necessarily overtly toxic. In this paper, we introduce and define a new category of sensitive text called &#34;delicate text.&#34; We provide the taxonomy of delicate text and present a detailed annotation scheme. We annotate DeTexD, the first benchmark dataset for delicate text detection. The significance of the difference in the definitions is highlighted by the relative performance deltas between models trained each definitions and corpora and evaluated on the other. We make publicly available the DeTexD Benchmark dataset, annotation guidelines, and baseline model for delicate text detection.</p>
            <a href="/paper_ACL_14.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Towards Safer Communities: Detecting Aggression and Offensive Language in Code-Mixed Tweets to Combat Cyberbullying
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Nazia Nafis, Diptesh Kanojia, Naveen Saini, Rudra Murthy</h5>
            <p class="card-text">Cyberbullying is a serious societal issue widespread on various channels and platforms, particularly social networking sites. Such platforms have proven to be exceptionally fertile grounds for such behavior. The dearth of high-quality training data for multilingual and low-resource scenarios, data that can accurately capture the nuances of social media conversations, often poses a roadblock to this task. This paper attempts to tackle cyberbullying, specifically its two most common manifestations - aggression and offensiveness. We present a novel, manually annotated dataset of a total of 10,000 English and Hindi-English code-mixed tweets, manually annotated for aggression detection and offensive language detection tasks. Our annotations are supported by inter-annotator agreement scores of 0.67 and 0.74 for the two tasks, indicating substantial agreement. We perform comprehensive fine-tuning of pre-trained language models (PTLMs) using this dataset to check its efficacy. Our challenging test sets show that the best models achieve macro F1-scores of 67.87 and 65.45 on the two tasks, respectively. Further, we perform cross-dataset transfer learning to benchmark our dataset against existing aggression and offensive language datasets. We also present a detailed quantitative and qualitative analysis of errors in prediction, and with this paper, we publicly release the novel dataset, code, and models.</p>
            <a href="/paper_ACL_15.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Towards Weakly-Supervised Hate Speech Classification Across Datasets
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Yiping Jin, Leo Wanner, Vishakha Kadam, Alexander Shvets</h5>
            <p class="card-text">As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.</p>
            <a href="/paper_ACL_16.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Distance from Unimodality: Assessing Polarized Opinions in Abusive Language Detection
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: John Pavlopoulos, Aristidis Likas</h5>
            <p class="card-text">The ground truth in classification tasks is often approximated by the fraction of annotators who classified an item as belonging to the positive class. Instances for which this fraction is equal to or above 50\textbackslash{}\% are considered positive, including however ones that receive polarized opinions. This is a problematic encoding convention that disregards the potentially polarized nature of opinions and which is often employed to estimate abusive language. We present the distance from unimodality (DFU), a measure that estimates the extent of polarization on the distribution of opinions and which correlates well with human judgment. By applying DFU to posts crowd-annotated for toxicity, we found that polarized opinions are more likely by annotators originating from different countries. Also, we show that DFU can be exploited as an objective function to train models to predict whether a post will provoke polarized opinions in the future.</p>
            <a href="/paper_ACL_19.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Flor Miriam Plaza-del-arco, Debora Nozza, Dirk Hovy</h5>
            <p class="card-text">Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. 
This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.</p>
            <a href="/paper_ACL_25.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Benchmarking Offensive and Abusive Language in Dutch Tweets
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Tommaso Caselli, Hylke Van Der Veen</h5>
            <p class="card-text">We present an extensive evaluation of different fine-tuned models to detect instances of offensive and abusive language in Dutch across three benchmarks: a standard held-out test, a task- agnostic functional benchmark, and a dynamic test set. We also investigate the use of data cartography to identify high quality training data. Our results show a relatively good quality of the manually annotated data used to train the models while highlighting some critical weakness. We have also found a good portability of trained models along the same language phenomena. As for the data cartography, we have found a positive impact only on the functional benchmark and when selecting data per annotated dimension rather than using the entire training material.</p>
            <a href="/paper_ACL_26.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Relationality and Offensive Speech: A Research Agenda
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Razvan Amironesei, Mark Diaz</h5>
            <p class="card-text">We draw from the framework of relationality as a pathway for modeling social relations to address gaps in text classification, generally, and offensive language classification, specifically. We use minoritized language, such as queer speech, to motivate a need for understanding and modeling social relations–both among individuals and among their social communities. We then point to socio-ethical style as a research area for inferring and measuring social relations as well as propose additional questions to structure future research on operationalizing social context.</p>
            <a href="/paper_ACL_27.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Cross-Platform and Cross-Domain Abusive Language Detection with Supervised Contrastive Learning
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Md Tawkat Islam Khondaker, Muhammad Abdul-mageed, Laks Lakshmanan, V.s.</h5>
            <p class="card-text">The prevalence of abusive language on different online platforms has been a major concern that raises the need for automated cross-platform abusive language detection. However, prior works focus on concatenating data from multiple platforms, inherently adopting Empirical Risk Minimization (ERM) method. In this work, we address this challenge from the perspective of domain generalization objective. We design SCL-Fish, a supervised contrastive learning integrated meta-learning algorithm to detect abusive language on unseen platforms. Our experimental analysis shows that SCL-Fish achieves better performance over ERM and the existing state-of-the-art models. We also show that SCL-Fish is data-efficient and achieves comparable performance with the large-scale pre-trained models upon finetuning for the abusive language detection task.</p>
            <a href="/paper_ACL_29.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Auditing YouTube Content Moderation in Low Resource Language Settings
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Hellina Hailu Nigatu, Inioluwa Raji</h5>
            <p class="card-text">Warning: this paper contains content that some may find disturbing.

While there has been increasing attention paid to the potential harms perpetuated by online platforms, most academic work on the subject centers on one narrow context: Western communities in primarily English language settings. Yet, social media platforms like YouTube support users globally and provide content in several languages, including low-resourced languages. In this study, we investigate this context via a mixed methods approach: collecting and analysing search and recommendation data from YouTube in low-resource language settings and conducting semi-structured interviews with YouTube users who speak low-resourced languages in Ethiopia. Our early findings indicate the failure of current content moderation schemes for low-resource languages and the further infliction and distribution of harm to marginalized communities through recommendation systems.</p>
            <a href="/paper_ACL_30.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            ExtremeBB: A Database for Large-Scale Research into Online Hate, Harassment, the Manosphere and Extremism
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Anh V. Vu, Lydia Wilson, Yi Ting Chua, Ilia Shumailov, Ross Anderson</h5>
            <p class="card-text">We introduce ExtremeBB, a textual database of over 53.5M posts made by 38.5k users on 12 extremist bulletin board forums promoting online hate, harassment, the manosphere and other forms of extremism. It enables large-scale analyses of qualitative and quantitative historical trends going back two decades: measuring hate speech and toxicity; tracing the evolution of different strands of extremist ideology; tracking the relationships between online subcultures, extremist behaviours, and real-world violence; and monitoring extremist communities in near real time. This can shed light not only on the spread of problematic ideologies but also the effectiveness of interventions. ExtremeBB comes with a robust ethical data-sharing regime that allows us to share data with academics worldwide. Since 2020, access has been granted to 49 licensees in 16 research groups from 12 institutions.</p>
            <a href="/paper_ACL_33.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Aporophobia: An Overlooked Type of Toxic Language Targeting the Poor
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Svetlana Kiritchenko, Georgina Curto Rex, Isar Nejadgholi, Kathleen C. Fraser</h5>
            <p class="card-text">While many types of hate speech and online toxicity have been the focus of extensive research in NLP, toxic language stigmatizing poor people has been mostly disregarded. Yet, aporophobia, a social bias against the poor, is a common phenomenon online, which can be psychologically damaging as well as hindering poverty reduction policy measures. We demonstrate that aporophobic attitudes are indeed present in social media and argue that the existing NLP datasets and models are inadequate to effectively address this problem. Efforts toward designing specialized resources and novel socio-technical mechanisms for confronting aporophobia are needed.</p>
            <a href="/paper_ACL_34.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Problematic Webpage Identification: A Trilogy of Hatespeech, Search Engines and GPT
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Ojasvin Sood, Sandipan Dandapat</h5>
            <p class="card-text">In this paper, we introduce a fine-tuned transformer-based model focused on problematic webpage classification to identify webpages promoting hate and violence of various forms. Due to the unavailability of labelled problematic webpage data, first we propose a novel webpage data collection strategy which leverages well-studied short-text hate speech datasets. We have introduced a custom GPT-4 few-shot prompt annotation scheme taking various webpage features to label the prohibitively expensive webpage annotation task. The resulting annotated data is used to build our problematic webpage classification model. We report the accuracy (87.6\% F1-score) of our webpage classification model and conduct a detailed comparison of it against other state-of-the-art hate speech classification model on problematic webpage identification task. Finally, we have showcased the importance of various webpage features in identifying a problematic webpage.</p>
            <a href="/paper_ACL_35.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Concept-Based Explanations to Test for False Causal Relationships Learned by Abusive Language Classifiers
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Isar Nejadgholi, Svetlana Kiritchenko, Kathleen C. Fraser, Esma Balkir</h5>
            <p class="card-text">Classifiers tend to learn a false causal relationship between an over-represented concept and a label, which can result in over-reliance on the concept and compromised classification accuracy. It is imperative to have methods in place that can compare different models and identify over-reliances on specific concepts. We consider three well-known abusive language classifiers trained on large English datasets and focus on the concept of negative emotions, which is an important signal but should not be learned as a sufficient feature for the label of abuse. Motivated by the definition of global sufficiency, we first examine the unwanted dependencies learned by the classifiers by assessing their accuracy on a challenge set across all decision thresholds. Further, recognizing that a challenge set might not always be available, we introduce concept-based explanation metrics to assess the influence of the concept on the labels. These explanations allow us to compare classifiers regarding the degree of false global sufficiency they have learned between a concept and a label.</p>
            <a href="/paper_ACL_38.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            &#34;Female Astronaut: Because sandwiches won&#39;t make themselves up there”: Towards Multimodal misogyny detection in memes
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Smriti Singh, Amritha Haridasan, Raymond Mooney</h5>
            <p class="card-text">A rise in the circulation of memes has led to the spread of a new form of multimodal hateful content. Unfortunately, the degree of hate women receive on the internet is disproportionately skewed against them. This, combined with the fact that multimodal misogyny is more challenging to detect as opposed to traditional text-based misogyny, signifies that the task of identifying misogynistic memes online is one of utmost importance. To this end, the MAMI dataset was released, consisting of 12000 memes annotated for misogyny and four sub-classes of misogyny - shame, objectification, violence and stereotype. While this balanced dataset is widely cited, we find that the task itself remains largely unsolved. Thus, in our work, we investigate the performance of multiple models in an effort to analyse whether domain specific pretraining helps model performance. We also investigate why even state of the art models find this task so challenging, and whether domain-specific pretraining can help. Our results show that pretraining BERT on hateful memes and leveraging an attention based approach with ViT outperforms state of the art models by more than 10\%. Further, we provide insight into why these models may be struggling with this task with an extensive qualitative analysis of random samples from the test set.</p>
            <a href="/paper_ACL_39.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Conversation Derailment Forecasting with Graph Convolutional Networks
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Enas Altarawneh, Ameeta Agrawal, Michael Jenkin, Manos Papagelis</h5>
            <p class="card-text">Online conversations are particularly susceptible to derailment, which can manifest itself in the form of toxic communication patterns like disrespectful comments or verbal abuse. Forecasting conversation derailment  predicts signs of derailment in advance enabling proactive moderation of conversations. Current state-of-the-art approaches to address this problem rely on sequence models that treat dialogues as text streams. We propose a novel model based on a graph convolutional neural network that considers dialogue user dynamics and the influence of public perception on conversation utterances. Through empirical evaluation, we show that our model effectively captures conversation dynamics and outperforms the state-of-the-art models on the CGA and CMV benchmark datasets by 1.5\textbackslash{}\% and 1.7\textbackslash{}\%, respectively.</p>
            <a href="/paper_ACL_40.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Resources for Automated Identification of Online Gender-Based Violence: A Systematic Review
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Gavin Abercrombie, Aiqi Jiang, Poppy Gerrard-abbott, Ioannis Konstas, Verena Rieser</h5>
            <p class="card-text">Online Gender-Based Violence (GBV), such as misogynistic abuse is an increasingly prevalent problem that technological approaches have struggled to address.Through the lens of the GBV framework, which is rooted in social science and policy, we systematically review 63 available resources for automated identification of such language. We find the datasets are limited in a number of important ways, such as their lack of theoretical grounding and stakeholder input, static nature, and focus on certain media platforms. Based on this review, we recommend development of future resources rooted in sociological expertise and
centering stakeholder voices, namely GBV experts and people with lived experience of GBV.</p>
            <a href="/paper_ACL_42.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Disentangling Disagreements on Offensiveness: A Cross-Cultural Study
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Aida Mostafazadeh Davani, Mark Diaz, Dylan Baker, Vinodkumar Prabhakaran</h5>
            <p class="card-text">What is deemed offensive depends inherently on the socio-cultural contexts within which those assessments are made. However, the subjective nature of this task is often overlooked in traditional NLP studies. While there has been recent work on socio-demographic correlates of offensiveness, the contribution of socio-cultural factors to perceiving offensiveness, when considered as a moral judgement, across the globe is still under-explored. 
We summarize the findings from a cross-cultural study of offensiveness annotations by 4295 participants from 21 different countries across 8 geo-cultural regions. Our results show that (1) perceptions of offensiveness vary significantly across geo-cultural regions, despite controlling for gender, age, and socio-economic status, and (2) these differences are significantly mediated by annotators&#39; individual moral concerns that vary across cultures.</p>
            <a href="/paper_ACL_45.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Evaluating the Effectiveness of Natural Language Inference for Hate Speech Detection in Languages with Limited Labeled Data
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Janis Goldzycher, Moritz Preisig, Chantal Amrhein, Gerold Schneider</h5>
            <p class="card-text">Most research on hate speech detection has focused on English where a sizeable amount of labeled training data is available. However, to expand hate speech detection into more languages, approaches that require minimal training data are needed. In this paper, we test whether natural language inference (NLI) models which perform well in zero- and few-shot settings can benefit hate speech detection performance in scenarios where only a limited amount of labeled data is available in the target language. Our evaluation on five languages demonstrates large performance improvements of NLI fine-tuning over direct fine-tuning in the target language. However, the effectiveness of previous work that proposed intermediate fine-tuning on English data is hard to match. Only in settings where the English training data does not match the test domain, can our customised NLI-formulation outperform intermediate fine-tuning on English. Based on our extensive experiments, we propose a set of recommendations for hate speech detection in languages where minimal labeled training data is available.</p>
            <a href="/paper_ACL_46.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            HOMO-MEX: A Mexican Spanish Annotated Corpus for LGBT+phobia Detection on Twitter
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Juan Vásquez, Scott Andersen, Gemma Bel-enguix, Helena Gómez-adorno, Sergio-luis Ojeda-trueba</h5>
            <p class="card-text">In the past few years, the NLP community has actively worked on detecting LGBT+Phobia in online spaces, using textual data publicly available Most of these are for the English language and its variants since it is the most studied language by the NLP community. Nevertheless, efforts towards creating corpora in other languages are active worldwide. Despite this, the Spanish language is an understudied language regarding digital LGBT+Phobia. The only corpus we found in the literature was for the Peninsular Spanish dialects, which use LGBT+phobic terms different than those in the Mexican dialect. For this reason, we present Homo-MEX, a novel corpus for detecting LGBT+Phobia in Mexican Spanish. In this paper, we describe our data-gathering and annotation process. Also, we present a classification benchmark using various traditional machine learning algorithms and two pre-trained deep learning models to showcase our corpus classification potential.</p>
            <a href="/paper_ACL_50.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Factoring Hate Speech: A New Annotation Framework to Study Hate Speech in Social Media
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Gal Ron, Effi Levi, Odelia Oshri, Shaul Shenhav</h5>
            <p class="card-text">In this work we propose a novel annotation scheme which factors hate speech into five separate discursive categories. To evaluate our scheme, we construct a corpus of over 2.9M Twitter posts containing hateful expressions directed at Jews, and annotate a sample dataset of 1,050 tweets. We present a statistical analysis of the annotated dataset as well as discuss annotation examples, and conclude by discussing promising directions for future work.</p>
            <a href="/paper_ACL_51.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Toward Disambiguating the Definitions of Abusive, Offensive, Toxic, and Uncivil Comments
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Pia Pachinger, Julia Neidhardt, Allan Hanbury, Anna Planitzer</h5>
            <p class="card-text">The definitions of abusive, offensive, toxic and uncivil comments used for annotating corpora for automated content moderation are highly intersected and researchers call for their disambiguation. We summarize the definitions of these terms as they appear in 23 papers across different fields. 
We compare examples given for uncivil, offensive, and toxic comments, attempting to foster more unified scientific resources. Additionally, we stress that the term incivility that frequently appears in social science literature has hardly been mentioned in the literature we analyzed that focuses on computational linguistics and natural language processing.</p>
            <a href="/paper_ACL_53.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            A Cross-Lingual Study of Homotransphobia on Twitter
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Davide Locatelli, Greta Damo, Debora Nozza</h5>
            <p class="card-text">We analyze homotransphobic content on Twitter across seven different languages, highlighting how this pervasive issue manifests in distinct cultural contexts worldwide. By developing a comprehensive taxonomy to classify homotransphobic speech, our study establishes a crucial foundation for building effective models to identify and counter such harmful speech on online platforms.</p>
            <a href="/paper_ACL_54.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Harmful Language Datasets: An Assessment of Robustness
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Katerina Korre, John Pavlopoulos, Jeffrey Sorensen, Léo Laugier, Ion Androutsopoulos, Lucas Dixon, Alberto Barrón-cedeño</h5>
            <p class="card-text">The automated detection of harmful language has been of great importance for the online world, especially with the growing importance of social media and, consequently, polarisation. There are many open challenges to high quality detection of harmful text, from dataset creation to generalisable application, thus calling for more systematic studies. In this paper, we explore re-annotation as a means of examining the robustness of already existing labelled datasets, showing that, despite using alternative definitions, the inter-annotator agreement remains very inconsistent, highlighting the intrinsically subjective and variable nature of the task. In addition, we build automatic toxicity detectors using the existing datasets, with their original labels, and we evaluate them on our multi-definition and multi-source datasets. Surprisingly, while other studies show that hate speech detection models perform better on data that are derived from the same distribution as the training set, our analysis demonstrates this is not necessarily true.</p>
            <a href="/paper_ACL_55.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Dimosthenis Antypas, Jose Camacho-Collados</h5>
            <p class="card-text">The automatic detection of hate speech online is an active research area in NLP. Most of the studies to date are based on social media datasets that contribute to the creation of hate speech detection models trained on them. However, data creation processes contain their own biases, and models inherently learn from these dataset-specific biases. In this paper, we perform a large-scale cross-dataset comparison where we fine-tune language models on different hate speech detection datasets. This analysis shows how some datasets are more generalizable than others when used as training data. Crucially, our experiments show how combining hate speech detection datasets can contribute to the development of robust hate speech detection models. This robustness holds even when controlling by data size and compared with the best individual datasets.</p>
            <a href="/paper_ACL_57.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Identity Construction in a Misogynist Incels Forum
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Michael Yoder, Chloe Perry, David Brown, Kathleen Carley, Meredith Pruden</h5>
            <p class="card-text">Online communities of involuntary celibates (incels) are a prominent source of misogynist hate speech. In this paper, we use quantitative text and network analysis approaches to examine how identity groups are discussed on incels.is, the largest black-pilled incels forum. We find that this community produces a wide range of novel identity terms and, while terms for women are most common, mentions of other minoritized identities are increasing. An analysis of the associations made with identity groups suggests an essentialist ideology where physical appearance, as well as gender and racial hierarchies, determine human value. We discuss implications for research into automated misogynist hate speech detection.</p>
            <a href="/paper_ACL_7.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] Responsibility Perspective Transfer for Italian Femicide News
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Gosse Minnema, Huiyuan Lai, Benedetta Muscato, Malvina Nissim</h5>
            <p class="card-text">Different ways of linguistically expressing the same real-world event can lead to different perceptions of what happened. Previous work has shown that different descriptions of gender-based violence (GBV) influence the reader&#39;s perception of who is to blame for the violence, possibly reinforcing stereotypes which see the victim as partly responsible, too. As a contribution to raise awareness on perspective-based writing, and to facilitate access to alternative perspectives, we introduce the novel task of automatically rewriting GBV descriptions as a means to alter the perceived level of blame on the perpetrator. We present a quasi-parallel dataset of sentences with low and high perceived responsibility levels for the perpetrator, and experiment with unsupervised (mBART-based), zero-shot and few-shot (GPT3-based) methods for rewriting sentences. We evaluate our models using a questionnaire study and a suite of automatic metrics.</p>
            <a href="/paper_ACL_F1.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] The State of Profanity Obfuscation in Natural Language Processing Scientific Publications
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Debora Nozza, Dirk Hovy</h5>
            <p class="card-text">Work on hate speech has made considering rude and harmful examples in scientific publications inevitable. This situation raises various problems, such as whether or not to obscure profanities. While science must accurately disclose what it does, the unwarranted spread of hate speech can harm readers and increases its internet frequency. While maintaining publications&#39;&#39; professional appearance, obfuscating profanities makes it challenging to evaluate the content, especially for non-native speakers.
Surveying 150 ACL papers, we discovered that obfuscation is usually used for English but not other languages, and even then, quite unevenly.
We discuss the problems with obfuscation and suggest a multilingual community resource called PrOf with a Python module to standardize profanity obfuscation processes. We believe PrOf can help scientific publication policies to make hate speech work accessible and comparable, irrespective of language.</p>
            <a href="/paper_ACL_F10.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap</h5>
            <p class="card-text">Warning: This paper contains content that may be offensive or upsetting. 
Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance &#34;your English is very good” may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. 

We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. 

To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement&#39;s offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.</p>
            <a href="/paper_ACL_F11.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Eddie Ungless, Bjorn Ross, Anne Lauscher</h5>
            <p class="card-text">Cutting-edge image generation has been praised for producing high-quality images, suggesting a ubiquitous future in a variety of applications. However, initial studies have pointed to the potential for harm due to predictive bias, reflecting and potentially reinforcing cultural stereotypes. In this work, we are the first to investigate how multimodal models handle diverse gender identities. Concretely, we conduct a thorough analysis in which we compare the output of three image generation models for prompts containing cisgender vs. non-cisgender identity terms. Our findings demonstrate that certain non-cisgender identities are consistently (mis)represented as less human, more stereotyped and more sexualised. We complement our experimental analysis with (a) a survey among non-cisgender individuals and (b) a series of interviews, to establish which harms affected individuals anticipate, and how they would like to be represented. We find respondents are particularly concerned about misrepresentation, and the potential to drive harmful behaviours and beliefs. Simple heuristics to limit offensive content are widely rejected, and instead respondents call for community involvement, curated training data and the ability to customise. These improvements could pave the way for a future where change is led by the affected community, and technology is used to positively &#39;&#39;[portray] queerness in ways that we haven&#39;t even thought of&#39;&#39;&#39; rather than reproducing stale, offensive stereotypes.</p>
            <a href="/paper_ACL_F12.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] Scientific Fact-Checking: A Survey of Resources and Approaches
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Juraj Vladika, Florian Matthes</h5>
            <p class="card-text">The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant attention due to the growing importance of scientific and health discussions on online platforms. Automated scientific fact-checking methods based on NLP can help combat the spread of misinformation, assist researchers in knowledge discovery, and help individuals understand new scientific breakthroughs. In this paper, we present a comprehensive survey of existing research in this emerging field and its related tasks. We provide a task description, discuss the construction process of existing datasets, and analyze proposed models and approaches. Based on our findings, we identify intriguing challenges and outline potential future directions to advance the field.</p>
            <a href="/paper_ACL_F2.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] A New Task and Dataset on Detecting Attacks on Human Rights Defenders
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Shihao Ran, Di Lu, Aoife Cahill, Joel Tetreault, Alejandro Jaimes</h5>
            <p class="card-text">The ability to conduct retrospective analyses of attacks on human rights defenders over time and by location is important for humanitarian organizations to better understand historical or ongoing human rights violations and thus better manage the global impact of such events. We hypothesize that NLP can support such efforts by quickly processing large collections of news articles to detect and summarize the characteristics of attacks on human rights defenders. To that end, we propose a new dataset for detecting Attacks on Human Rights Defenders (HRDsAttack) consisting of crowdsourced annotations on 500 online news articles. The annotations include fine-grained information about the type and location of the attacks, as well as information about the victim(s). We demonstrate the usefulness of the dataset by using it to train and evaluate baseline models on several sub-tasks to predict the annotated characteristics.</p>
            <a href="/paper_ACL_F3.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] ClaimDiff: Comparing and Contrasting Claims on Contentious Issues
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Miyoung Ko, Ingyu Seong, Hwaran Lee, Joonsuk Park, Minsuk Chang, Minjoon Seo</h5>
            <p class="card-text">With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one&#39;s argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose ClaimDIff, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In ClaimDiff, we provide human-labeled 2,941 claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19% absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided comparison.</p>
            <a href="/paper_ACL_F4.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] Which Examples Should be Multiply Annotated? Active Learning When Annotators May Disagree
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Connor Baumler, Anna Sotnikova, Hal Daumé III</h5>
            <p class="card-text"></p>
            <a href="/paper_ACL_F5.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] Playing the Part of the Sharp Bully: Generating Adversarial Examples for Implicit Hate Speech Detection
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Nicolas Ocampo, Elena Cabrio, Serena Villata</h5>
            <p class="card-text">Research on abusive content detection on social media has primarily focused on explicit forms of hate speech (HS), that are often identifiable by recognizing hateful words and expressions. Messages containing linguistically subtle and implicit forms of hate speech still constitute an open challenge for automatic hate speech detection. In this paper, we propose a new framework for generating adversarial implicit HS short-text messages using Auto-regressive Language Models. Moreover, we propose a strategy to group the generated implicit messages in complexity levels (EASY, MEDIUM, and HARD categories) characterizing how challenging these messages are for supervised classifiers. Finally, relying on (Dinan et al., 2019; Vidgen et al., 2021), we propose a ``build it, break it, fix it&#39;&#39;, training scheme using HARD messages showing how iteratively retraining on HARD messages substantially leverages SOTA models&#39; performances on implicit HS benchmarks.</p>
            <a href="/paper_ACL_F6.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] Disagreement Matters: Preserving Label Diversity by Jointly Modeling Item and Annotator Label Distributions with DisCo
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Tharindu Cyril Weerasooriya, Alexander Ororbia, Raj Bhensadadia, Ashiqur KhudaBukhsh, Christopher Homan</h5>
            <p class="card-text">Annotator disagreement is common whenever human judgment is needed for supervised learning. It is conventional to assume that one label per item represents ground truth. However, this obscures minority opinions, if present. We regard ``ground truth&#39;&#39; as the distribution of all labels that a population of annotators could produce, if asked (and of which we only have a small sample). We next introduce DisCo (Distribution from Context), a simple neural model that learns to predict this distribution. The model takes annotator-item pairs, rather than items alone, as input, and performs inference by aggregating over all annotators. Despite its simplicity, our experiments show that, on six benchmark datasets, our model is competitive with, and frequently outperforms, other, more complex models that either do not model specific annotators or were not designed for label distribution learning.</p>
            <a href="/paper_ACL_F7.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] It’s not Sexually Suggestive; It’s Educative | Separating Sex Education from Suggestive Content on TikTok videos
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Enfa George, Mihai Surdeanu</h5>
            <p class="card-text">We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator&#39;s point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex education videos on TikTok. Children&#39;s exposure to sexually suggestive videos has been shown to have adversarial effects on their development (Collins et al. 2017). Meanwhile, virtual sex education, especially on subjects that are more relevant to the LGBTQIA+ community, is very valuable (Mitchell et al. 2014). The platform&#39;s current system removes/punishes some of both types of videos, even though they serve different purposes. Our dataset contains video URLs, and it is also audio transcribed. To validate its importance, we explore two transformer-based models for classifying the videos. Our preliminary results suggest that the task of distinguishing between these types of videos is learnable but challenging. These experiments suggest that this dataset is meaningful and invites further study on the subject.</p>
            <a href="/paper_ACL_F8.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            [Findings] Debiasing should be Good and Bad
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Robert Morabito, Jad Kabbara, Ali Emami</h5>
            <p class="card-text">Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? We used such considerations to devise three criteria for our new protocol: Specification Polarity, Specification Importance, and Domain Transferability. As a case study, we apply our protocol to a popular debiasing method, Self-Debiasing, and compare it to  one we propose, called Instructive Debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. We show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked.</p>
            <a href="/paper_ACL_F9.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
    </div>


    <script src="static/js/time-extend.js"></script>
    <script>
      $(document).ready(() => {
        add_local_tz('.session_times');
      })
    </script>

    
      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-9X8H03BYC2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-9X8H03BYC2");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-left">
          <img src="static/images/acl2023/acl-logo-2023.png" height="45px"
            width="auto" align="center">
          <span class="lead">ACL 2023</span>
        </p>
        <p class="float-right"><a href="#" class="text-dark">Back to Top</a></p>
        <p class="text-center">© 2023 Association for Computational Linguistics</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
    <div class="modal left fade" id="chatsModal" tabindex="" role="dialog" aria-labelledby="exampleModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-dialog-scrollable" role="document" style="max-height: 100% !important;">
        <div class="modal-content" style="max-height: 100% !important;">
            <div class="modal-header">
                <h4 class="modal-title" id="exampleModalLabel">Active Chats</h4>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
            </div>
            <div class="modal-body">
                <div style="margin-bottom: 1em;">
    <div class="stats-last-update text-muted" id="highly-active-chats-last-update"></div>
    <a id="highly-active-chats-btn-refresh" href="#" class="text-primary" style="display: none;">Refresh now</a>
</div>

<div id="highly-active-chats-list">
</div>

<div class="text-center" id="highly-active-chats-progress-bar" style="margin-bottom: 80em;">
    <div class="spinner-border text-primary" style="margin-top: 3em; width: 3rem; height: 3rem;" role="status">
      <span class="sr-only">Loading...</span>
    </div>
</div>

<br/>
<p class="text-muted">
    <strong>How it works: </strong>
    We calculate the number of new messages for every channel in the last N seconds. Then, we sort them descendingly.
    Channels with no new messages will be randomly shuffled. Please note that the number of messages might not be accurate.
</p>

<script src="static/js/highly-active-chats.js"></script>
<script>
    let channel_stats_server = "https://emnlp2020-channels-stats.azure-api.net"
    $(document).ready( function () {
        $('[data-toggle="tooltip"]').tooltip();
        //load_stats();
    });
</script>

            </div>

            <div class="modal-footer">
                <button type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button>
            </div>
        </div>
    </div>
</div>
</body>

</html>