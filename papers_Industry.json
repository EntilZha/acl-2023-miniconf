[{"abstract":"Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 15MB and 30ms on devices. We propose a series of principles for model compression when combined with quantization. Further, we implement an engine that is friendly to INT8 and decoding. With the co-design of model and engine, compared with the existing system, we speed up 47.0x and save 99.5\\% of memory with only 11.6\\% loss of BLEU. Our code will be publicly available after the anonymity period.","anthology_url":"https://aclanthology.org/2023.acl-industry.36","authors":["Ye Lin","Xiaohui Wang","Zhexi Zhang","Mingxuan Wang","Tong Xiao","Jingbo Zhu"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I100","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.36.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] MobileNMT: Enabling Translation in 15MB and 30ms","tldr":"Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuse...","track":"Industry","underline_id":79809,"underline_url":"https://underline.io/events/395/posters/15254/poster/79809-mobilenmt-enabling-translation-in-15mb-and-30ms","video_url":null},{"abstract":"Multi-document summarization is gaining more and more attention recently and serves as an invaluable tool to obtain key facts among a large information pool. In this paper, we proposed a multi-document hybrid summarization approach, which simultaneously generates a human-readable summary and extracts corresponding key evidences based on multi-doc inputs. To fulfill that purpose, we crafted a salient representation learning method to induce latent salient features, which are effective for joint evidence extraction and summary generation. In order to train this model, we conducted multi-task learning to optimize a composited loss, constructed over extractive and abstractive sub-components in a hierarchical way. We implemented the system based on a ubiquiotously adopted transformer architecture and conducted experimental studies on multiple datasets across two domains, achieving superior performance over the baselines.","anthology_url":"https://aclanthology.org/2023.acl-industry.37","authors":["Min Xiao"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I102","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.37.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Multi-doc Hybrid Summarization via Salient Representation Learning","tldr":"Multi-document summarization is gaining more and more attention recently and serves as an invaluable tool to obtain key facts among a large information pool. In this paper, we proposed a multi-document hybrid summarization approach, which simultaneously generates a human-readable summary and extract...","track":"Industry","underline_id":79810,"underline_url":"https://underline.io/events/395/posters/15254/poster/79810-multi-doc-hybrid-summarization-via-salient-representation-learning","video_url":null},{"abstract":"Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100\\% accurate labels is difficult, thus many datasets are accompanied by label noise at different levels. Previous work has shown that existing noise-handling methods could not improve the peak performance of BERT on noisy datasets, and might even deteriorate it. In this paper, we propose SaFER, a robust and efficient fine-tuning framework for BERT-based text classifiers, combating label noises without access to any clean data for training or validation. Utilizing a label-agnostic early-stopping strategy and self-supervised learning, our proposed framework achieves superior performance in terms of both accuracy and speed on multiple text classification benchmarks. The trained model is finally fully deployed in several industrial biomedical literature mining tasks and demonstrates high effectiveness and efficiency.","anthology_url":"https://aclanthology.org/2023.acl-industry.38","authors":["Zhenting Qi","Xiaoyu Tan","Chao Qu","Yinghui Xu","Yuan Qi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I104","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.38.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels","tldr":"Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100\\% accurate labels is difficult, thus many datasets are accompanied by label noise a...","track":"Industry","underline_id":79797,"underline_url":"https://underline.io/events/395/posters/15240/poster/79797-safer-a-robust-and-efficient-framework-for-fine-tuning-bert-based-classifier-with-noisy-labels","video_url":null},{"abstract":"In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemical language understanding benchmark datasets consisted of tasks for both patent and literature articles provided by industrial organization. All the datasets are internally made by chemists from scratch. Finally, we evaluate the datasets on the various language models based on BERT and RoBERTa, and demonstrate the model performs better when the domain of the pretrained models are closer to chemistry domain. We provide baselines for our benchmark as 0.8054 in average, and we hope this benchmark is used by many researchers in both industry and academia.","anthology_url":"https://aclanthology.org/2023.acl-industry.39","authors":["Yunsoo Kim","Hyuk Ko","Jane Lee","Hyun Young Heo","Jinyoung Yang","Sungsoo Lee","Kyu-hwang Lee"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I107","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.39.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79819/poster_document/4113d44d613aad88c4187c143109900f.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79819/slideshow/23a49303cc699beb7353a694e17a6459.pdf","title":"[Industry] Chemical Language Understanding Benchmark","tldr":"In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemica...","track":"Industry","underline_id":79819,"underline_url":"https://underline.io/events/395/posters/15240/poster/79819-industry-chemical-language-understanding-benchmark","video_url":null},{"abstract":"Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or communities whose main focus is languages with less accessible resources (e.g., non-English). This paper explores the sequence-to-sequence (seq2seq) language model architecture as a more practical and compute-efficient alternative to the decoder-oriented approach (e.g., GPT-3), accompanied by novel findings in compute-optimality analyses. We successfully trained billion-scale Korean-language seq2seq language models that strongly outperform other competitive models in Korean benchmarks. Moreover, we demonstrate that such language models can be more efficiently utilized by employing a heavy pre-finetuning strategy, by showcasing a case study on dialog-task adaptation. Our case study shows that adopting language models with more readily available domain-specific unlabeled data greatly improves fine-tuning data efficiency in low-resource settings.","anthology_url":"https://aclanthology.org/2023.acl-industry.40","authors":["Dongju Park","Soonwon Ka","Kang Min Yoo","Gichang Lee","Jaewook Kang"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I109","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.40.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] HyperT5: Towards Compute-Efficient Korean Language Modeling","tldr":"Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or commu...","track":"Industry","underline_id":79833,"underline_url":"https://underline.io/events/395/posters/15240/poster/79833-hypert5-towards-compute-efficient-korean-language-modeling","video_url":null},{"abstract":"Ambiguity is a major obstacle to providing services based on sentence classification. However, because of the structural limitations of the service, there may not be sufficient contextual information to resolve the ambiguity. In this situation, we focus on ambiguity detection so that service design considering ambiguity is possible. We utilize similarity in a semantic space to detect ambiguity in service scenarios and training data. In addition, we apply task-specific embedding to improve performance. Our results demonstrate that ambiguities and resulting labeling errors in training data or scenarios can be detected. Additionally, we confirm that it can be used to debug services","anthology_url":"https://aclanthology.org/2023.acl-industry.41","authors":["Jong Myoung Kim","Young-jun Lee","Sangkeun Jung","Ho-jin Choi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I110","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.41.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Semantic Ambiguity Detection in Sentence Classification using Task-Specific Embeddings","tldr":"Ambiguity is a major obstacle to providing services based on sentence classification. However, because of the structural limitations of the service, there may not be sufficient contextual information to resolve the ambiguity. In this situation, we focus on ambiguity detection so that service design ...","track":"Industry","underline_id":79792,"underline_url":"https://underline.io/events/395/posters/15254/poster/79792-semantic-ambiguity-detection-in-sentence-classification-using-task-specific-embeddings","video_url":null},{"abstract":"Data drift is the change in model input data that is one of the key factors leading to machine learning models performance degradation over time. Monitoring drift helps detecting these issues and preventing their harmful consequences. Meaningful drift interpretation is a fundamental step towards effective re-training of the model. In this study we propose an end-to-end framework for reliable model-agnostic change-point detection and interpretation in large task-oriented dialog systems, proven effective in multiple customer deployments. We evaluate our approach and demonstrate its benefits with a novel variant of intent classification training dataset, simulating customer requests to a dialog system. We make the data publicly available.","anthology_url":"https://aclanthology.org/2023.acl-industry.42","authors":["Ella Rabinovich","Matan Vetzler","Samuel Ackerman","Ateret Anaby Tavor"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-6_-industry-(oral)"],"id":"I111","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.42.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79765/poster/1e9afa15c1ef0484093123ed7d988c98.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Reliable and Interpretable Drift Detection in Streams of Short Texts","tldr":"Data drift is the change in model input data that is one of the key factors leading to machine learning models performance degradation over time. Monitoring drift helps detecting these issues and preventing their harmful consequences. Meaningful drift interpretation is a fundamental step towards eff...","track":"Industry","underline_id":79765,"underline_url":"https://underline.io/events/395/sessions/15266/lecture/79765-industry-reliable-and-interpretable-drift-detection-in-streams-of-short-texts","video_url":null},{"abstract":"Leveraging representations from pre-trained transformer-based encoders achieves state-of-the-art performance on numerous NLP tasks.\nLarger encoders can improve accuracy for spoken language understanding (SLU) but are challenging to use given the inference latency constraints of online systems (especially on CPU machines).\nWe evaluate using a larger 170M parameter BERT encoder that shares representations across languages, domains and tasks for SLU compared to using smaller 17M parameter BERT encoders with language-, domain- and task-decoupled finetuning.\nRunning inference with a larger shared encoder on GPU is latency neutral and reduces infrastructure cost compared to running inference for decoupled smaller encoders on CPU machines.\nThe larger shared encoder reduces semantic error rates by 4.62\\% for test sets representing user requests to voice-controlled devices and 5.79\\% on the tail of the test sets on average across four languages.","anthology_url":"https://aclanthology.org/2023.acl-industry.43","authors":["Jonathan Hueser","Judith Gaspers","Thomas Gueudre","Chandana Prakash","Jin Cao","Daniil Sorokin","Quynh Do","Nicolas Anastassacos","Tobias Falke","Turan Gojayev"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-6_-industry-(oral)"],"id":"I112","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.43.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79767/poster/e53127a2e02cfa5ecf42ef1b65a0a3b7.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding","tldr":"Leveraging representations from pre-trained transformer-based encoders achieves state-of-the-art performance on numerous NLP tasks.\nLarger encoders can improve accuracy for spoken language understanding (SLU) but are challenging to use given the inference latency constraints of online systems (espec...","track":"Industry","underline_id":79767,"underline_url":"https://underline.io/events/395/sessions/15266/lecture/79767-industry-sharing-encoder-representations-across-languages-domains-and-tasks-in-large-scale-spoken-language-understanding","video_url":null},{"abstract":"In this work, we present a natural language processing (NLP) pipeline for the identification, extraction and linking of Research Infrastructure (RI) used in scientific publications. Links between scientific equipment and publications where the equipment was used can support multiple use cases, such as evaluating the impact of RI investment, and supporting Open Science and research reproducibility. These links can also be used to establish a profile of the RI portfolio of each institution and associate each equipment with scientific output. The system we are describing here is already in production, and has been used to address real business use cases, some of which we discuss in this paper. The computational pipeline at the heart of the system comprises both supervised and unsupervised modules to detect the usage of research equipment by processing the full text of the articles. Additionally, we have created a knowledge graph of RI, which is utilized to annotate the articles with metadata. Finally, examples of the business value of the insights made possible by this NLP pipeline are illustrated.","anthology_url":"https://aclanthology.org/2023.acl-industry.44","authors":["Seyed Amin Tabatabaei","Georgios Cheirmpos","Marius Doornenbal","Alberto Zigoni","Veronique Moore","Georgios Tsatsaronis"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-3_-industry-(oral)"],"id":"I119","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.44.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79763/poster/a438d8d5be8c70e3e4386a56b049b9a0.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Annotating Research Infrastructure in Scientific Papers: An NLP-driven Approach","tldr":"In this work, we present a natural language processing (NLP) pipeline for the identification, extraction and linking of Research Infrastructure (RI) used in scientific publications. Links between scientific equipment and publications where the equipment was used can support multiple use cases, such ...","track":"Industry","underline_id":79763,"underline_url":"https://underline.io/events/395/sessions/15229/lecture/79763-industry-annotating-research-infrastructure-in-scientific-papers-an-nlp-driven-approach","video_url":null},{"abstract":"In search engines, query expansion (QE) is a crucial technique to improve search experience. Previous studies often rely on long-term search log mining, which leads to slow updates and is sub-optimal for time-sensitive news searches. In this work, we present Event-Centric Query Expansion (EQE), the QE system used in a famous Chinese search engine. EQE utilizes a novel event retrieval framework that consists of four stages, i.e., event collection, event reformulation, semantic retrieval and online ranking, which can select the best expansion from a significant amount of potential events rapidly and accurately. Specifically, we first collect and filter news headlines from websites. Then we propose a generation model that incorporates contrastive learning and prompt-tuning techniques to reformulate these headlines to concise candidates. Additionally, we fine-tune a dual-tower semantic model to serve as an encoder for event retrieval and explore a two-stage contrastive training approach to enhance the accuracy of event retrieval. Finally, we rank the retrieved events and select the optimal one as QE, which is then used to improve the retrieval of event-related documents. Through offline analysis and online A/B testing, we observed that the EQE system has significantly improved many indicators compared to the baseline. The system has been deployed in a real production environment and serves hundreds of millions of users.","anthology_url":"https://aclanthology.org/2023.acl-industry.45","authors":["Yanan Zhang","Weijie Cui","Yangfan Zhang","Xiaoling Bai","Zhe Zhang","Jin Ma","Xiang Chen","Tianhua Zhou"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I120","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.45.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79774/poster_document/24c84a8bf92c3edc7390fc0a48c836a5.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79774/slideshow/b88eac1969c21a57a2c40ff2a3736089.pptx","title":"[Industry] Event-Centric Query Expansion in Web Search","tldr":"In search engines, query expansion (QE) is a crucial technique to improve search experience. Previous studies often rely on long-term search log mining, which leads to slow updates and is sub-optimal for time-sensitive news searches. In this work, we present Event-Centric Query Expansion (EQE), the ...","track":"Industry","underline_id":79774,"underline_url":"https://underline.io/events/395/posters/15240/poster/79774-industry-event-centric-query-expansion-in-web-search","video_url":null},{"abstract":"As e-commerce platforms develop different business lines, a special but challenging product categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time. \nIn order to unify the categorization process and ensure efficiency, we propose a two-stage taxonomy-agnostic framework that relies solely on calculating the semantic relatedness between product titles and category names in the vector space. \nTo further enhance domain transferability and better exploit cross-domain data, we design two plug-in modules: a heuristic mapping scorer and a pretrained contrastive ranking module with the help of meta concepts, which represent \nkeyword knowledge shared across domains.\nComprehensive offline experiments show that our method outperforms strong baselines\non three dynamic multi-domain product categorization (DMPC) tasks,\nand online experiments reconfirm its efficacy with a\n5\\% increase on seasonal purchase revenue. Related datasets will be released.","anthology_url":"https://aclanthology.org/2023.acl-industry.46","authors":["Shansan Gong","Zelin Zhou","Shuo Wang","Fengjiao Chen","Xiujie Song","Xuezhi Cao","Yunsen Xian","Kenny Zhu"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I124","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.46.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79780/poster_document/cfa173d7dd9f87287c313af40ace8842.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79780/slideshow/f24123118cdd0de84274f10a35898c1e.pptx","title":"[Industry] Transferable and Efficient: Unifying Dynamic Multi-Domain Product Categorization","tldr":"As e-commerce platforms develop different business lines, a special but challenging product categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time. \nIn order to unify the categorization process and ensure efficienc...","track":"Industry","underline_id":79780,"underline_url":"https://underline.io/events/395/posters/15240/poster/79780-industry-transferable-and-efficient-unifying-dynamic-multi-domain-product-categorization","video_url":null},{"abstract":"Space program agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems.\nKnowledge Base (KB) databases are an effective way of storing and accessing such information to scale.\nIn this work we present a system, developed for the European Space Agency, that can answer complex natural language queries, to support engineers in accessing the information contained in a KB that models the orbital space debris environment. Our system is based on a pipeline which first generates a program sketch from a natural language question, then specializes the sketch into a concrete query program with mentions of entities, attributes and relations, and finally executes the program against the database.\nThis pipeline decomposition approach enables us to train the system by leveraging out-of-domain data and semi-synthetic data generated by GPT-3, thus reducing overfitting and shortcut learning even with limited amount of in-domain training data.","anthology_url":"https://aclanthology.org/2023.acl-industry.47","authors":["Paul Darm","Antonio Valerio Miceli Barone","Shay B. Cohen","Annalisa Riccardi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I125","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.47.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] DISCOSQA: A Knowledge Base Question Answering System for Space Debris based on Program Induction","tldr":"Space program agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems.\nKnowledge Base (KB) databases are an effective way of storing and accessing such information to scale.\nIn this work we present a system, ...","track":"Industry","underline_id":79806,"underline_url":"https://underline.io/events/395/posters/15254/poster/79806-discosqa-a-knowledge-base-question-answering-system-for-space-debris-based-on-program-induction","video_url":null},{"abstract":"Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists of two off-the-shelf methods for improving PLMs' early exiting. We first address the issues of training a multi-exit PLM, the backbone model for early exiting. We propose the novel architecture of block-wise bypasses, which can alleviate the conflicts in jointly training multiple intermediate classifiers and thus improve the overall performances of multi-exit PLM while introducing negligible additional flops to the model. Second, we propose a novel divergence-based early exiting (DGE) mechanism, which obtains early exiting signals by comparing the predicted distributions of two adjacent layers' exits. Extensive experiments on three proprietary datasets and three GLUE benchmark tasks demonstrate that our method can obtain a better speedup-performance trade-off than the existing baseline methods.\\textbackslash{}footnote\\{Code will be made publicly available to the research community upon acceptance.\\}","anthology_url":"https://aclanthology.org/2023.acl-industry.48","authors":["Wei Zhu","Peng Wang","Yuan Ni","Guotong Xie","Xiaoling Wang"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-3_-industry-(oral)"],"id":"I128","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.48.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79759/poster/88a8c27522f86608c16ec48eb1757ea4.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting","tldr":"Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists ...","track":"Industry","underline_id":79759,"underline_url":"https://underline.io/events/395/sessions/15229/lecture/79759-industry-badge-speeding-up-bert-inference-after-deployment-via-block-wise-bypasses-and-divergence-based-early-exiting","video_url":null},{"abstract":"Patent applicants write patent specifications\nthat describe embodiments of inventions.\nSome embodiments are claimed for a patent,\nwhile others may be unclaimed\ndue to strategic considerations.\nUnclaimed embodiments may be extracted by\napplicants later and claimed in\ncontinuing applications to\ngain advantages over competitors.\nDespite being essential for corporate intellectual property (IP) strategies,\nunclaimed embodiment extraction is conducted manually,\nand little research has been conducted on its automation.\nThis paper presents a novel task of\nunclaimed embodiment extraction (UEE)\nand a novel dataset for the task.\nOur experiments with Transformer-based models\ndemonstrated\nthat the task was challenging as it required\nconducting natural language inference on\npatent specifications, which consisted of\ntechnical, long, syntactically and semantically\ninvolved sentences.\nWe release the dataset and code \nto foster this new area of research.","anthology_url":"https://aclanthology.org/2023.acl-industry.3","authors":["Chikara Hashimoto","Gautam Kumar","Shuichiro Hashimoto","Jun Suzuki"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I13","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.3.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79817/poster_document/1337b36956fcc5f947568abb6dd0f496.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79817/slideshow/066ca93bcfcf96ba969b9c443dffcfcc.pdf","title":"[Industry] Hunt for Buried Treasures: Extracting Unclaimed Embodiments from Patent Specifications","tldr":"Patent applicants write patent specifications\nthat describe embodiments of inventions.\nSome embodiments are claimed for a patent,\nwhile others may be unclaimed\ndue to strategic considerations.\nUnclaimed embodiments may be extracted by\napplicants later and claimed in\ncontinuing applications to\ngain a...","track":"Industry","underline_id":79817,"underline_url":"https://underline.io/events/395/posters/15240/poster/79817-industry-hunt-for-buried-treasures-extracting-unclaimed-embodiments-from-patent-specifications","video_url":null},{"abstract":"Maritime security requires full-time monitoring of the situation, mainly based on technical data (radar, AIS) but also from OSINT-like inputs (e.g., newspapers). Some threats to the operational reliability of this maritime surveillance, such as malicious actors, introduce discrepancies between hard and soft data (sensors and texts), either by tweaking their AIS emitters or by emitting false information on pseudo-newspapers.\n\nMany techniques exist to identify these pieces of false information, including using knowledge base population techniques to build a structured view of the information. This paper presents a use case for suspect data identification in a maritime setting. The proposed system UMBAR ingests data from sensors and texts, processing them through an information extraction step, in order to feed a Knowledge Base and finally perform coherence checks between the extracted facts.","anthology_url":"https://aclanthology.org/2023.acl-industry.49","authors":["Maxime Prieur","Souhir Gahbiche","Guillaume Gadek","Sylvain Gatepaille","Kilian Vasnier","Valerian Justine"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-3_-industry-(oral)"],"id":"I131","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.49.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79760/poster/02483b7e3f750e640fca47f7da092602.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] K-pop and fake facts: from texts to smart alerting for maritime security","tldr":"Maritime security requires full-time monitoring of the situation, mainly based on technical data (radar, AIS) but also from OSINT-like inputs (e.g., newspapers). Some threats to the operational reliability of this maritime surveillance, such as malicious actors, introduce discrepancies between hard ...","track":"Industry","underline_id":79760,"underline_url":"https://underline.io/events/395/sessions/15229/lecture/79760-industry-k-pop-and-fake-facts-from-texts-to-smart-alerting-for-maritime-security","video_url":null},{"abstract":"The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vector representations of a given text. With a growing number of APIs at our disposal, in this paper, our goal is to analyze semantic embedding APIs in realistic retrieval scenarios in order to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we wish to investigate the capabilities of existing APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate the embedding APIs on two standard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective on English, in contrast to the standard practice, i.e., employing them as first-stage retrievers. For non-English retrieval, re-ranking still improves the results, but a hybrid model with BM25 works best albeit at a higher cost. We hope our work lays the groundwork for thoroughly evaluating APIs that are critical in search and more broadly, in information retrieval.","anthology_url":"https://aclanthology.org/2023.acl-industry.50","authors":["Ehsan Kamalloo","Xinyu Zhang","Odunayo Ogundepo","Nandan Thakur","David Alfonso-hermelo","Mehdi Rezagholizadeh","Jimmy Lin"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I134","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.50.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79779/poster_document/e48280b2a64f16a24ccfa513f6ddd113.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Evaluating Embedding APIs for Information Retrieval","tldr":"The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vecto...","track":"Industry","underline_id":79779,"underline_url":"https://underline.io/events/395/posters/15254/poster/79779-evaluating-embedding-apis-for-information-retrieval","video_url":null},{"abstract":"Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. \nParticularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradient learning have been shown to struggle in such setups or have limitations like memory buffers, and being restricted to specific domains that disable its usage in real-world scenarios. For this reason, we present a fully differentiable architecture based on the Mixture of Experts model, that enables the training of high-performance classifiers when examples from each class are presented separately. We conducted exhaustive experiments that proved its applicability in various domains and ability to learn online in production environments. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods.","anthology_url":"https://aclanthology.org/2023.acl-industry.51","authors":["Mateusz W\u00f3jcik","Witold Ko\u015bciukiewicz","Mateusz Baran","Tomasz Kajdanowicz","Adam Gonczarek"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I135","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.51.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform","tldr":"Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. \nParticularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradien...","track":"Industry","underline_id":79787,"underline_url":"https://underline.io/events/395/posters/15254/poster/79787-domain-agnostic-neural-architecture-for-class-incremental-continual-learning-in-document-processing-platform","video_url":null},{"abstract":"In real-world systems, an important requirement for model updates is to avoid regressions in user experience caused by flips of previously correct classifications to incorrect ones. Multiple techniques for that have been proposed in the recent literature. In this paper, we apply one such technique, focal distillation, to model updates in a goal-oriented dialog system and assess its usefulness in practice. In particular, we evaluate its effectiveness for key language understanding tasks, including sentence classification and sequence labeling tasks, we further assess its effect when applied to repeated model updates over time, and test its compatibility with mislabeled data. Our experiments on a public benchmark and data from a deployed dialog system demonstrate that focal distillation can substantially reduce regressions, at only minor drops in accuracy, and that it further outperforms naive supervised training in challenging mislabeled data and label expansion settings.","anthology_url":"https://aclanthology.org/2023.acl-industry.52","authors":["Andrea Caciolai","Verena Weber","Tobias Falke","Alessandro Pedrani","Davide Bernardi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-6_-industry-(oral)"],"id":"I139","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.52.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79768/poster/9550f352f48d79a83d735e7e585305a0.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Regression-Free Model Updates for Spoken Language Understanding","tldr":"In real-world systems, an important requirement for model updates is to avoid regressions in user experience caused by flips of previously correct classifications to incorrect ones. Multiple techniques for that have been proposed in the recent literature. In this paper, we apply one such technique, ...","track":"Industry","underline_id":79768,"underline_url":"https://underline.io/events/395/sessions/15266/lecture/79768-industry-regression-free-model-updates-for-spoken-language-understanding","video_url":null},{"abstract":"Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose 'MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the 'MultiArith' dataset (78.7\\% -\\textgreater{} 92.5\\%) evaluated using 175B parameter GPT-based LLM.","anthology_url":"https://aclanthology.org/2023.acl-industry.4","authors":["Shima Imani","Liang Du","Harsh Shrivastava"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I14","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.4.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79772/poster_document/6878702800f4d6dc568b38f1f5e6a769.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79772/slideshow/61521c4fff4ff46382c4c6f647ee9d1a.pptx","title":"[Industry] MathPrompter: Mathematical Reasoning using Large Language Models","tldr":"Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. ...","track":"Industry","underline_id":79772,"underline_url":"https://underline.io/events/395/posters/15254/poster/79772-industry-mathprompter-mathematical-reasoning-using-large-language-models","video_url":null},{"abstract":"Bias in machine learning models can be an issue when the models are trained on particular types of data that do not generalize well, causing under performance in certain groups of users. In this work, we focus on reducing the bias related to new customers in a digital voice assistant system. It is observed that natural language understanding models often have lower performance when dealing with requests coming from new users rather than experienced users. To mitigate this problem, we propose a framework that consists of two phases (1) a fixing phase with four active learning strategies used to identify important samples coming from new users, and (2) a self training phase where a teacher model trained from the first phase is used to annotate semi-supervised samples to expand the training data with relevant cohort utterances. We explain practical strategies that involve an identification of representative cohort-based samples through density clustering as well as employing implicit customer feedbacks to improve new customers' experience. We demonstrate the effectiveness of our approach in a real world large scale voice assistant system for two languages, German and French through both offline experiments as well as A/B testings.","anthology_url":"https://aclanthology.org/2023.acl-industry.53","authors":["Dieu-thu Le","Gabriela Hernandez","Bei Chen","Melanie Bradford"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I140","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.53.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Reducing cohort bias in natural language understanding systems with targeted self-training scheme","tldr":"Bias in machine learning models can be an issue when the models are trained on particular types of data that do not generalize well, causing under performance in certain groups of users. In this work, we focus on reducing the bias related to new customers in a digital voice assistant system. It is o...","track":"Industry","underline_id":79793,"underline_url":"https://underline.io/events/395/posters/15254/poster/79793-reducing-cohort-bias-in-natural-language-understanding-systems-with-targeted-self-training-scheme","video_url":null},{"abstract":"Content moderation on social media is governed by policies that are intricate and frequently updated with evolving world events. However, automated content moderation systems often restrict easy adaptation to policy changes and are expected to learn policy intricacies from limited amounts of labeled data, which make effective policy compliance challenging.  We propose to model content moderation as a binary question answering problem where the questions validate the loosely coupled themes constituting a policy. A decision logic is applied on top to aggregate the theme-specific validations. This way the questions pass theme information to a transformer network as explicit policy prompts, that in turn enables explainability. This setting further allows for faster adaptation to policy updates by leveraging zero-shot capabilities of pre-trained transformers. We showcase improved recall for our proposed method at 95\\textbackslash{}\\% precision on two proprietary datasets of social media posts and comments respectively annotated under curated Hate Speech and Commercial Spam policies.","anthology_url":"https://aclanthology.org/2023.acl-industry.54","authors":["Sankha Subhra Mullick","Mohan Bhambhani","Suhit Sinha","Akshat Mathur","Somya Gupta","Jidnya Shah"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I141","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.54.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79771/poster_document/9297d08868ed5d9507c1ed537366ed10.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79771/slideshow/fb94daee76c51f3ecd6106ec949ca412.pdf","title":"[Industry] Content Moderation for Evolving Policies using Binary Question Answering","tldr":"Content moderation on social media is governed by policies that are intricate and frequently updated with evolving world events. However, automated content moderation systems often restrict easy adaptation to policy changes and are expected to learn policy intricacies from limited amounts of labeled...","track":"Industry","underline_id":79771,"underline_url":"https://underline.io/events/395/posters/15240/poster/79771-industry-content-moderation-for-evolving-policies-using-binary-question-answering","video_url":null},{"abstract":"Item categorization (IC) aims to classify product descriptions into leaf nodes in a categorical taxonomy, which is a key technology used in a wide range of applications. Along with the fact that most datasets often has a long-tailed distribution, classification performances on tail labels tend to be poor due to scarce supervision, causing many issues in real-life applications. To address IC task's long-tail issue, K-positive contrastive loss (KCL) is proposed on image classification task and can be applied on the IC task when using text-based contrastive learning, e.g., SimCSE. However, one shortcoming of using KCL has been neglected in previous research: false negative (FN) instances may harm the KCL's representation learning. To address the FN issue in the KCL, we proposed to re-weight the positive pairs in the KCL loss with a regularization that the sum of weights should be constrained to K+1 as close as possible. After controlling FN instances with the proposed method, IC performance has been further improved and is superior to other LT-addressing methods.","anthology_url":"https://aclanthology.org/2023.acl-industry.55","authors":["Tianqi Wang","Lei Chen","Xiaodan Zhu","Younghun Lee","Jing Gao"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I146","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.55.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Weighted Contrastive Learning With False Negative Control to Help Long-tailed Product Classification","tldr":"Item categorization (IC) aims to classify product descriptions into leaf nodes in a categorical taxonomy, which is a key technology used in a wide range of applications. Along with the fact that most datasets often has a long-tailed distribution, classification performances on tail labels tend to be...","track":"Industry","underline_id":79798,"underline_url":"https://underline.io/events/395/posters/15254/poster/79798-weighted-contrastive-learning-with-false-negative-control-to-help-long-tailed-product-classification","video_url":null},{"abstract":"Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, \\textbackslash{}texttt\\{ToxicTrap\\}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. \\textbackslash{}texttt\\{ToxicTrap\\} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal function designs allow \\textbackslash{}texttt\\{ToxicTrap\\} to identify weaknesses in both multiclass and multilabel toxic language detectors. Our empirical results show that SOTA toxicity text classifiers are indeed vulnerable to the proposed attacks, attaining over 98\\textbackslash{}\\% attack success rates in multilabel cases. We also show how a vanilla adversarial training and its improved version can help increase robustness of a toxicity detector even against unseen attacks.","anthology_url":"https://aclanthology.org/2023.acl-industry.56","authors":["Dmitriy Bespalov","Sourav Bhabesh","Yi Xiang","Liutong Zhou","Yanjun Qi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I148","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.56.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79816/slideshow/341dabe8e2bb0bd25a501f677041930f.pdf","title":"[Industry] Towards Building a Robust Toxicity Predictor","tldr":"Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, \\textbackslash{}texttt\\{ToxicTrap\\}, introducing small word-level perturbations to fo...","track":"Industry","underline_id":79816,"underline_url":"https://underline.io/events/395/posters/15240/poster/79816-towards-building-a-robust-toxicity-predictor","video_url":null},{"abstract":"Recently, self-learning methods based on user satisfaction metrics and contextual bandits have shown promising results to enable consistent improvements in conversational AI systems. However, directly targeting such metrics by off-policy bandit learning objectives often increases the risk of making abrupt policy changes that break the current user experience. In this study, we introduce a scalable framework for supporting fine-grained exploration targets for individual domains via user-defined constraints. For example, we may want to ensure fewer policy deviations in business-critical domains such as shopping, while allocating more exploration budget to domains such as music. We present a novel meta-gradient learning approach that is scalable and practical to address this problem. The proposed method adjusts constraint violation penalty terms adaptively through a meta objective that encourages balanced constraint satisfaction across domains. We conducted extensive experiments on a real-world conversational AI and using a set of realistic constraint benchmarks. The proposed approach has been deployed in production for a large-scale commercial assistant, enabling the best balance between the policy value and constraint satisfaction rate.","anthology_url":"https://aclanthology.org/2023.acl-industry.5","authors":["Mohammad Kachuee","Sungjin Lee"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I15","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.5.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Constrained Policy Optimization for Controlled Self-Learning in Conversational AI Systems","tldr":"Recently, self-learning methods based on user satisfaction metrics and contextual bandits have shown promising results to enable consistent improvements in conversational AI systems. However, directly targeting such metrics by off-policy bandit learning objectives often increases the risk of making ...","track":"Industry","underline_id":79830,"underline_url":"https://underline.io/events/395/posters/15240/poster/79830-constrained-policy-optimization-for-controlled-self-learning-in-conversational-ai-systems","video_url":null},{"abstract":"In recent years, the utilization of Artificial Intelligence (AI) in the contact center industry is on the rise. One area where AI can have a significant impact is in the coaching of contact center agents. By analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor. In this paper, we present \"AI Coach Assis\", which leverages the pre-trained transformer-based language models to determine whether a given call is coachable or not based on the quality assurance (QA) queries/questions asked by the contact center managers or supervisors. The system was trained and evaluated on a large dataset collected from real-world contact centers and provides an efficient and effective way to determine which calls are most relevant for coaching purposes. Extensive experimental evaluation demonstrates the potential of AI Coach Assist to improve the coaching process, resulting in enhancing the performance of contact center agents.","anthology_url":"https://aclanthology.org/2023.acl-industry.57","authors":["Md Tahmid Rahman Laskar","Cheng Chen","Xue-yong Fu","Mahsa Azizi","Shashi Bhushan","Simon Corston-oliver"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I156","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.57.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] AI Coach Assist: An Automated Approach for Call Recommendation in Contact Centers for Agent Coaching","tldr":"In recent years, the utilization of Artificial Intelligence (AI) in the contact center industry is on the rise. One area where AI can have a significant impact is in the coaching of contact center agents. By analyzing call transcripts, AI can quickly determine which calls are most relevant for coach...","track":"Industry","underline_id":79789,"underline_url":"https://underline.io/events/395/posters/15254/poster/79789-ai-coach-assist-an-automated-approach-for-call-recommendation-in-contact-centers-for-agent-coaching","video_url":null},{"abstract":"Large pre-trained language models based on transformer architecture\u0192have drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4\\% and 97.8\\% the performance of mBERT on MTOP and multiATIS, while using 170x less parameters. Our model consistently beats the state-of-the-art of tiny models (pQRNN), which is twice as large, by a margin up to 7.8\\% on MTOP.","anthology_url":"https://aclanthology.org/2023.acl-industry.6","authors":["Francesco Fusco","Damian Pascual","Peter Staar","Diego Antognini"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-3_-industry-(oral)"],"id":"I16","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.6.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79758/poster/9e3cd2f4a859adf77153d52deb4f9f95.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] pNLP-Mixer: an Efficient all-MLP Architecture for Language","tldr":"Large pre-trained language models based on transformer architecture\u0192have drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and infere...","track":"Industry","underline_id":79758,"underline_url":"https://underline.io/events/395/sessions/15229/lecture/79758-industry-pnlp-mixer-an-efficient-all-mlp-architecture-for-language","video_url":null},{"abstract":"Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summarization. Common approaches to create dense representations include training domain-specific embeddings with self-supervised setups or using sentence encoder models trained over similarity tasks. In contrast to static embeddings, sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, but impose significant computational costs. In this paper, we propose a fully unsupervised approach to text encoding that consists of training small character-based models with the objective of reconstructing large pre-trained embedding matrices. Models trained with this approach can not only match the quality of sentence encoders in technical domains, but are 5 times smaller and up to 10 times faster, even on high-end GPUs.","anthology_url":"https://aclanthology.org/2023.acl-industry.7","authors":["Francesco Fusco","Diego Antognini"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I17","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.7.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Extracting Text Representations for Terms and Phrases in Technical Domains","tldr":"Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summ...","track":"Industry","underline_id":79795,"underline_url":"https://underline.io/events/395/posters/15254/poster/79795-extracting-text-representations-for-terms-and-phrases-in-technical-domains","video_url":null},{"abstract":"Query rewriting (QR) is an important technique for user friction (i.e. recovering ASR error or system error) reduction and contextual carryover (i.e. ellipsis and co-reference) in conversational AI systems. Recently, generation-based QR models have achieved promising results on these two tasks separately. Although these two tasks have many similarities such as they both use the previous dialogue along with the current request as model input, there is no unified model to solve them jointly. To this end, we propose a unified contextual query rewriting model that unifies QR for both reducing friction and contextual carryover purpose. Moreover, we involve multiple auxiliary tasks such as trigger prediction and NLU interpretation tasks to boost the performance of the rewrite. We leverage the text-to-text unified framework which uses independent tasks with weighted loss to account for task importance. Then we propose new unified multitask learning strategies including a sequential model which outputs one sentence for multi-tasks, and a hybrid model where some tasks are independent and some tasks are sequentially generated. Our experimental results demonstrate the effectiveness of the proposed unified learning methods.","anthology_url":"https://aclanthology.org/2023.acl-industry.58","authors":["Yingxue Zhou","Jie Hao","Mukund Rungta","Yang Liu","Eunah Cho","Xing Fan","Yanbin Lu","Vishal Vasudevan","Kellen Gillespie","Zeynab Raeesy"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I171","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.58.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79781/poster_document/cb2230f1dad4457d6246761f6c936a97.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Unified Contextual Query Rewriting","tldr":"Query rewriting (QR) is an important technique for user friction (i.e. recovering ASR error or system error) reduction and contextual carryover (i.e. ellipsis and co-reference) in conversational AI systems. Recently, generation-based QR models have achieved promising results on these two tasks separ...","track":"Industry","underline_id":79781,"underline_url":"https://underline.io/events/395/posters/15254/poster/79781-unified-contextual-query-rewriting","video_url":null},{"abstract":"E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain contextual insights about users' true shopping intents. Therefore, modeling such contextual information is critical to a better query rewriting model. However, existing query rewriting models ignore users' history behaviors and consider only the instant search query, which is often a short string offering limited information about the true shopping intent.\nWe propose an end-to-end context-aware query rewriting model to bridge this gap, which takes the search context into account. Specifically, our model builds a session graph using the history search queries and their contained words. We then employ a graph attention mechanism that models cross-query relations and computes contextual information of the session. The model subsequently calculates session representations by combining the contextual information with the instant search query using an aggregation network. The session representations are then decoded to generate rewritten queries. Empirically, we demonstrate the superiority of our method to state-of-the-art approaches under various metrics.","anthology_url":"https://aclanthology.org/2023.acl-industry.59","authors":["Simiao Zuo","Qingyu Yin","Haoming Jiang","Shaohui Xi","Bing Yin","Chao Zhang","Tuo Zhao"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-3_-industry-(oral)"],"id":"I173","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.59.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79761/poster/c08e592ba2d96aaf96ec0ae5faddbbea.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Context-Aware Query Rewriting for Improving Users' Search Experience on E-commerce Websites","tldr":"E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain cont...","track":"Industry","underline_id":79761,"underline_url":"https://underline.io/events/395/sessions/15229/lecture/79761-industry-context-aware-query-rewriting-for-improving-users-search-experience-on-e-commerce-websites","video_url":null},{"abstract":"Large-scale pre-trained text-image models with dual-encoder architectures (such as CLIP) are typically adopted for various vision-language applications, including text-image retrieval. However, these models are still less practical on edge devices or for real-time situations, due to the substantial indexing and inference time and the large consumption of computational resources. Although knowledge distillation techniques have been widely utilized for uni-modal model compression, how to expand them to the situation when the numbers of modalities and teachers/students are doubled has been rarely studied. In this paper, we conduct comprehensive experiments on this topic and propose the fully-Connected knowledge interaction graph (Coca) technique for cross-modal pre-training distillation. Based on our findings, the resulting CocaCLIP achieves SOTA performances on the widely-used Flickr30K and MSCOCO benchmarks under the lightweight setting. An industry application of our method on an e-commercial platform further demonstrates the significant effectiveness of CocaCLIP.","anthology_url":"https://aclanthology.org/2023.acl-industry.8","authors":["Jiapeng Wang","Chengyu Wang","Xiaodan Wang","Jun Huang","Lianwen Jin"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I18","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.8.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] CocaCLIP: Exploring Distillation of Fully-Connected Knowledge Interaction Graph for Lightweight Text-Image Retrieval","tldr":"Large-scale pre-trained text-image models with dual-encoder architectures (such as CLIP) are typically adopted for various vision-language applications, including text-image retrieval. However, these models are still less practical on edge devices or for real-time situations, due to the substantial ...","track":"Industry","underline_id":79831,"underline_url":"https://underline.io/events/395/posters/15240/poster/79831-cocaclip-exploring-distillation-of-fully-connected-knowledge-interaction-graph-for-lightweight-text-image-retrieval","video_url":null},{"abstract":"We train and deploy language models (LMs) with federated learning (FL) and differential privacy (DP) in Google Keyboard (Gboard). The recent DP-Follow the Regularized Leader (DP-FTRL) algorithm is applied to achieve meaningfully formal DP guarantees without requiring uniform sampling of clients. \nTo provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation of training. \nWith the help of pretraining on public data, we trained and deployed more than fifteen Gboard LMs that achieve high utility and \\$\\textbackslash{}rho-\\$zCDP privacy guarantees with \\$\\textbackslash{}rho \\textbackslash{}in (0.3, 2)\\$, with one model additionally trained with secure aggregation.\nWe summarize our experience and provide concrete suggestions on DP training for practitioners.","anthology_url":"https://aclanthology.org/2023.acl-industry.60","authors":["Zheng Xu","Yanxiang Zhang","Galen Andrew","Christopher Choquette","Peter Kairouz","Brendan Mcmahan","Jesse Rosenstock","Yuanbo Zhang"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I186","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.60.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79777/poster_document/147f6a8c20b71af2c477a0ea45e77d99.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79777/slideshow/db4525e9b3b360c9b5f2e3b2f9ee3b95.pdf","title":"[Industry] Federated Learning of Gboard Language Models with Differential Privacy","tldr":"We train and deploy language models (LMs) with federated learning (FL) and differential privacy (DP) in Google Keyboard (Gboard). The recent DP-Follow the Regularized Leader (DP-FTRL) algorithm is applied to achieve meaningfully formal DP guarantees without requiring uniform sampling of clients. \nTo...","track":"Industry","underline_id":79777,"underline_url":"https://underline.io/events/395/posters/15254/poster/79777-industry-federated-learning-of-gboard-language-models-with-differential-privacy","video_url":null},{"abstract":"Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in fine-tuning tasks. We present RadLing, a continuously pretrained language model using ELECTRA-small architecture, trained using over 500K radiology reports that can compete with state-of-the-art results for fine tuning tasks in radiology domain. Our main contribution in this paper is knowledge-aware masking which is an taxonomic knowledge-assisted pre-training task that dynamically masks tokens to inject knowledge during pretraining. In addition, we also introduce an knowledge base-aided vocabulary extension to adapt the general tokenization vocabulary to radiology domain.","anthology_url":"https://aclanthology.org/2023.acl-industry.61","authors":["Rikhiya Ghosh","Oladimeji Farri","Sanjeev Kumar Karn","Manuela Danu","Ramya Vunikili","Larisa Micu"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I187","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.61.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] RadLing: Towards Efficient Radiology Report Understanding","tldr":"Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in fine-tuning...","track":"Industry","underline_id":79799,"underline_url":"https://underline.io/events/395/posters/15254/poster/79799-radling-towards-efficient-radiology-report-understanding","video_url":null},{"abstract":"In a typical call center, only up to 8\\% of callers\nleave a Customer Satisfaction (CSAT) survey\nresponse at the end of the call, and these tend to\nbe customers with strongly positive or negative\nexperiences. To manage this data sparsity and\nresponse bias, we outline a predictive CSAT\ndeep learning algorithm that infers CSAT on\nthe 1-5 scale on inbound calls to the call center\nwith minimal latency. The key metric to maximize is the precision for CSAT = 1 (lowest\nCSAT). We maximize this metric in two ways.\nFirst, reframing the problem\nas a binary class, rather than five-class problem during model fine-tuning, and then mapping binary outcomes back to five classes using\ntemperature-scaled model probabilities. Second, using soft labels to represent the classes. \nThe\nresult is a production model able to support key\ncustomer workflows with high accuracy over\nmillions of calls a month.","anthology_url":"https://aclanthology.org/2023.acl-industry.62","authors":["Etienne Manderscheid","Matthias Lee"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I188","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.62.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Predicting Customer Satisfaction with Soft Labels for Ordinal Classification","tldr":"In a typical call center, only up to 8\\% of callers\nleave a Customer Satisfaction (CSAT) survey\nresponse at the end of the call, and these tend to\nbe customers with strongly positive or negative\nexperiences. To manage this data sparsity and\nresponse bias, we outline a predictive CSAT\ndeep learning a...","track":"Industry","underline_id":79790,"underline_url":"https://underline.io/events/395/posters/15254/poster/79790-predicting-customer-satisfaction-with-soft-labels-for-ordinal-classification","video_url":null},{"abstract":"Recent work has shown that large-scale annotated datasets are essential for training state-of-the-art Question Answering (QA) models.\nUnfortunately, creating this data is expensive and requires a huge amount of annotation work. An alternative and cheaper source of supervision is given by feedback data collected from deployed QA systems.\nThis data can be collected from tens of millions of user with no additional cost, for real-world QA services, e.g., Alexa, Google Home, and etc. The main drawback is the noise affecting feedback on individual examples. \nRecent literature on QA systems has shown the benefit of training models even with noisy feedback. However, these studies have multiple limitations: (i) they used uniform random noise to simulate feedback responses, which is typically an unrealistic approximation as noise follows specific patterns, depending on target examples and users; and (ii) they do not show how to aggregate feedback for improving training signals.\nIn this paper, we first collect a large scale (16M) QA dataset with real feedback sampled from the QA traffic of a popular Virtual Assistant.\nSecond, we use this data to develop two strategies for filtering unreliable users and thus de-noise feedback: (i) ranking users with an automatic classifier, and (ii) aggregating feedback over similar instances and comparing users between each other. Finally, we train QA models on our filtered feedback data, showing a significant improvement over the state of the art.","anthology_url":"https://aclanthology.org/2023.acl-industry.63","authors":["Liang Wang","Ivano Lauriola","Alessandro Moschitti"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-6_-industry-(oral)"],"id":"I189","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.63.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79764/poster/5d75b6abb765170c8db2d3e2b69291b4.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Accurate Training of Web-based Question Answering Systems with Feedback from Ranked Users","tldr":"Recent work has shown that large-scale annotated datasets are essential for training state-of-the-art Question Answering (QA) models.\nUnfortunately, creating this data is expensive and requires a huge amount of annotation work. An alternative and cheaper source of supervision is given by feedback da...","track":"Industry","underline_id":79764,"underline_url":"https://underline.io/events/395/sessions/15266/lecture/79764-industry-accurate-training-of-web-based-question-answering-systems-with-feedback-from-ranked-users","video_url":null},{"abstract":"In a task-oriented dialogue system, joint intent detection and slot filling for multi-intent utterances become meaningful since users tend to query more. The current state-of-the-art studies choose to process multi-intent utterances through a single joint model of sequence labelling and multi-label classification, which cannot generalize to utterances with more intents than training samples. Meanwhile, it lacks the ability to assign slots to each corresponding intent. To overcome these problems, we propose a Split-Parsing Method (SPM) for joint multiple intent detection and slot filling, which is a two-stage method. It first splits an input sentence into multiple sub-sentences which contain a single-intent, and then a joint single intent detection and slot filling model is applied to parse each sub-sentence recurrently. Finally, we integrate the parsed results. The sub-sentence split task is also treated as a sequence labelling problem with only one entity-label, which can effectively generalize to a sentence with more intents unseen in the training set. Experimental results on three multi-intent datasets show that our method obtains substantial improvements over different baselines.","anthology_url":"https://aclanthology.org/2023.acl-industry.64","authors":["Sheng Jiang","Su Zhu","Ruisheng Cao","Qingliang Miao","Kai Yu"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I191","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.64.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79818/slideshow/ff4042d6f60e6d91c7a027ff73d1a2b5.pptx","title":"[Industry] SPM: A Split-Parsing Method for Joint Multi-Intent Detection and Slot Filling","tldr":"In a task-oriented dialogue system, joint intent detection and slot filling for multi-intent utterances become meaningful since users tend to query more. The current state-of-the-art studies choose to process multi-intent utterances through a single joint model of sequence labelling and multi-label ...","track":"Industry","underline_id":79818,"underline_url":"https://underline.io/events/395/posters/15240/poster/79818-industry-spm-a-split-parsing-method-for-joint-multi-intent-detection-and-slot-filling","video_url":null},{"abstract":"Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering from error propagation and inefficient decoding. In this work, we propose a unified non-autoregressive generation (NAG) framework for general NER tasks, referred to as NAG-NER. First, we propose to generate entities as a set instead of a sequence, avoiding error propagation. Second, we propose incorporating NAG in NER tasks for efficient decoding by treating each entity as a target sequence. Third, to enhance the generation performances of the NAG decoder, we employ the NAG encoder to detect potential entity mentions. Extensive experiments show that our NAG-NER model outperforms the state-of-the-art generative NER models on three benchmark NER datasets of different types and two of our proprietary NER tasks.\\textbackslash{}footnote\\{Code will be publicly available to the research community upon acceptance.\\}","anthology_url":"https://aclanthology.org/2023.acl-industry.65","authors":["Xinpeng Zhang","Ming Tan","Jingfan Zhang","Wei Zhu"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I196","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.65.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] NAG-NER: a Unified Non-Autoregressive Generation Framework for Various NER Tasks","tldr":"Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering fro...","track":"Industry","underline_id":79800,"underline_url":"https://underline.io/events/395/posters/15254/poster/79800-nag-ner-a-unified-non-autoregressive-generation-framework-for-various-ner-tasks","video_url":null},{"abstract":"Misspelled search queries in e-commerce can lead to empty or irrelevant products. Besides inadvertent typing mistakes, most spell mistakes occur because the user does not know the correct spelling, hence typing it as it is pronounced colloquially. This colloquial typing creates countless misspelling patterns for a single correct query. In this paper, we first systematically analyze and group different spell errors into error classes and then leverage the state-of-the-art Transformer model for contextual spell correction. We overcome the constraint of limited human labelled data by proposing novel synthetic data generation techniques for voluminous generation of training pairs needed by data hungry Transformers, without any human intervention. We further utilize weakly supervised data coupled with curriculum learning strategies to improve on tough spell mistakes without regressing on the easier ones. We show significant improvements from our model on human labeled data and online A/B experiments against multiple state-of-art models.","anthology_url":"https://aclanthology.org/2023.acl-industry.66","authors":["Vishal Kakkar","Chinmay Sharma","Madhura Pande","Surender Kumar"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I197","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.66.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Search Query Spell Correction with Weak Supervision in E-commerce","tldr":"Misspelled search queries in e-commerce can lead to empty or irrelevant products. Besides inadvertent typing mistakes, most spell mistakes occur because the user does not know the correct spelling, hence typing it as it is pronounced colloquially. This colloquial typing creates countless misspelling...","track":"Industry","underline_id":79814,"underline_url":"https://underline.io/events/395/posters/15254/poster/79814-search-query-spell-correction-with-weak-supervision-in-e-commerce","video_url":null},{"abstract":"Well-formed context aware image captions and tags in enterprise content such as marketing material are critical to ensure their brand presence and content recall. Manual creation and updates to ensure the same is non trivial given the scale and the tedium towards this task. We propose a new unified Vision-Language (VL) model based on the One For All (OFA) model, with a focus on context-assisted image captioning where the caption is generated based on both the image and its context. Our approach aims to overcome the context-independent (image and text are treated independently) nature of the existing approaches. We exploit context by pretraining our model with datasets of three tasks- news image captioning where the news article is the context, contextual visual entailment, and keyword extraction from the context. The second pretraining task is a new VL task, and we construct and release two datasets for the task with  1.1M and 2.2K data instances.  Our system achieves state-of-the-art results with an improvement of up to 8.34 CIDEr score on the benchmark news image captioning datasets. To the best of our knowledge, ours is the first effort at incorporating contextual information in pretraining the models for the VL tasks.","anthology_url":"https://aclanthology.org/2023.acl-industry.67","authors":["Abisek Rajakumar Kalarani","Pushpak Bhattacharyya","Niyati Chhaya","Sumit Shekhar"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-6_-industry-(oral)"],"id":"I199","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.67.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79769/poster_document/eb002a251cb4f184ef42b88b0e182913.pdf","preview_image":"https://assets.underline.io/lecture/79769/poster/7c8e85fd55b5fd8110545b66b906f5c1.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79769/slideshow/666ce1cc49afdd41b952152883a43d0a.pdf","title":"[Industry] \"Let's not Quote out of Context\": Unified Vision-Language Pretraining for Context Assisted Image Captioning","tldr":"Well-formed context aware image captions and tags in enterprise content such as marketing material are critical to ensure their brand presence and content recall. Manual creation and updates to ensure the same is non trivial given the scale and the tedium towards this task. We propose a new unified ...","track":"Industry","underline_id":79769,"underline_url":"https://underline.io/events/395/sessions/15266/lecture/79769-industry-let-s-not-quote-out-of-context-unified-vision-language-pretraining-for-context-assisted-image-captioning","video_url":null},{"abstract":"This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of WWH in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.","anthology_url":"https://aclanthology.org/2023.acl-industry.68","authors":["Deuksin Kwon","Sunwoo Lee","Ki Hyun Kim","Seojin Lee","Taeyoon Kim","Eric Davis"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I201","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.68.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79784/poster_document/0b95d3ccabaa63daf1cf84bad4f5fd45.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] What, When, and How to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue","tldr":"This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approa...","track":"Industry","underline_id":79784,"underline_url":"https://underline.io/events/395/posters/15254/poster/79784-what-when-and-how-to-ground-designing-user-persona-aware-conversational-agents-for-engaging-dialogue","video_url":null},{"abstract":"Relevance in E-commerce Product Search is crucial for providing customers with accurate results that match their query intent. With recent advancements in NLP and Deep Learning, Transformers have become the default choice for relevance classification tasks. In such a setting, the relevance model uses query text and product title as input features, and estimates if the product is relevant for the customer query. While cross-attention in Transformers enables a more accurate relevance prediction in such a setting, its high evaluation latency makes it unsuitable for real-time predictions in which thousands of products must be evaluated against a user query within few milliseconds. To address this issue, we propose CUPID: a Curriculum learning based real-time Prediction using Distillation that utilizes knowledge distillation within a curriculum learning setting to learn a simpler architecture that can be evaluated within low latency budgets. In a bi-lingual relevance prediction task, our approach shows an 302 bps improvement on English and 676 bps improvement for low-resource Arabic, while maintaining the low evaluation latency on CPUs.","anthology_url":"https://aclanthology.org/2023.acl-industry.69","authors":["Arindam Bhattacharya","Ankith Ms","Ankit Gandhi","Vijay Huddar","Atul Saroop","Rahul Bhagat"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I205","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.69.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] CUPID: Curriculum Learning Based Real-Time Prediction using Distillation","tldr":"Relevance in E-commerce Product Search is crucial for providing customers with accurate results that match their query intent. With recent advancements in NLP and Deep Learning, Transformers have become the default choice for relevance classification tasks. In such a setting, the relevance model use...","track":"Industry","underline_id":79801,"underline_url":"https://underline.io/events/395/posters/15254/poster/79801-cupid-curriculum-learning-based-real-time-prediction-using-distillation","video_url":null},{"abstract":"Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech that can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant answers, leading to bad user experiences. We analyze failed QA requests to identify core challenges: lexical gaps, proposition types, complex syntactic structure, and high specificity. We propose a Semantic Question Reformulation (SURF) model offering three linguistically-grounded operations (repair, syntactic reshaping, generalization) to rewrite questions to facilitate answering. Offline evaluation on 1M unanswered questions from a leading voice assistant shows that SURF significantly improves answer rates: up to 24\\% of previously unanswered questions obtain relevant answers (75\\%). Live deployment shows positive impact for millions of customers with unanswered questions; explicit relevance feedback shows high user satisfaction.","anthology_url":"https://aclanthology.org/2023.acl-industry.70","authors":["Pedro Faustini","Zhiyu Chen","Besnik Fetahu","Oleg Rokhlenko","Shervin Malmasi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-6_-industry-(oral)"],"id":"I207","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.70.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79766/poster/0efe4ad5116ef5058c9660dd832e8cf3.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Answering Unanswered Questions through Semantic Reformulations in Spoken QA","tldr":"Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech that can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant a...","track":"Industry","underline_id":79766,"underline_url":"https://underline.io/events/395/sessions/15266/lecture/79766-industry-answering-unanswered-questions-through-semantic-reformulations-in-spoken-qa","video_url":null},{"abstract":"Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe on Flan-T5 yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions is also very competitive.","anthology_url":"https://aclanthology.org/2023.acl-industry.71","authors":["Soham Parikh","Mitul Tiwari","Prashil Tumbade","Quaizar Vohra"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I208","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.71.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79778/poster_document/53f14782aeb2c9df78c1472ac4c8b7b6.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79778/slideshow/0aa712bfc3ebba7aa3fbb02a8df1a9d0.pptx","title":"[Industry] Exploring Zero and Few-shot Techniques for Intent Classification","tldr":"Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classificati...","track":"Industry","underline_id":79778,"underline_url":"https://underline.io/events/395/posters/15254/poster/79778-industry-exploring-zero-and-few-shot-techniques-for-intent-classification","video_url":null},{"abstract":"Voice assistants help users make phone calls, send messages, create events, navigate and do a lot more. However assistants  have limited capacity to understand their users' context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phone numbers, addresses, email addresses, urls, and dates on their phone screens. We focus on reference understanding, which is particularly interesting when, similar to visual grounding, there are multiple similar texts on screen. We collect a dataset and propose a lightweight general purpose model for this novel experience. Since consuming pixels directly is expensive, our system is designed to rely only on text extracted from the UI. Our model is modular, offering flexibility, better interpretability and efficient run time memory.","anthology_url":"https://aclanthology.org/2023.acl-industry.72","authors":["Shruti Bhargava","Anand Dhoot","Ing-marie Jonsson","Hoang Long Nguyen","Alkesh Patel","Hong Yu","Vincent Renkens"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I213","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.72.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Referring to Screen Texts with Voice Assistants","tldr":"Voice assistants help users make phone calls, send messages, create events, navigate and do a lot more. However assistants  have limited capacity to understand their users' context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phon...","track":"Industry","underline_id":79815,"underline_url":"https://underline.io/events/395/posters/15254/poster/79815-referring-to-screen-texts-with-voice-assistants","video_url":null},{"abstract":"Frequently Asked Question (FAQ) retrieval aims at retrieving question-answer pairs for a given a user query. Integrating FAQ retrieval with product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Providing FAQ content without disrupting user's shopping experience poses challenges on deciding when and how to show FAQ results. \n\nOur proposed intent-aware FAQ retrieval consists of (1) an intent classifier that predicts whether the query is looking for an FAQ; (2) a reformulation model that rewrites query into a natural question. \nOffline evaluation demonstrates that our approach improves 12\\% in Hit@1 on retrieving ground-truth FAQs, while reducing latency by 95\\% compared to baseline systems. These improvements are further validated by real user feedback, where more than 99\\% of users consider FAQs displayed on top of product search results is helpful. Overall, our findings show promising directions for integrating FAQ retrieval into product search at scale.","anthology_url":"https://aclanthology.org/2023.acl-industry.73","authors":["Zhiyu Chen","Jason Choi","Besnik Fetahu","Oleg Rokhlenko","Shervin Malmasi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I215","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.73.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search","tldr":"Frequently Asked Question (FAQ) retrieval aims at retrieving question-answer pairs for a given a user query. Integrating FAQ retrieval with product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Pro...","track":"Industry","underline_id":79807,"underline_url":"https://underline.io/events/395/posters/15254/poster/79807-generate-then-retrieve-intent-aware-faq-retrieval-in-product-search","video_url":null},{"abstract":"Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained VLMs. We benchmark and reveal practical challenges in adapting these VLMs to image ad understanding. We propose a simple feature adaptation strategy to effectively fuse multimodal information for image ads and further empower it with knowledge of real-world entities. We hope our study draws more attention to image ad understanding which is broadly relevant to the advertising industry.","anthology_url":"https://aclanthology.org/2023.acl-industry.74","authors":["Zhiwei Jia","Pradyumna Narayana","Arjun Akula","Garima Pruthi","Hao Su","Sugato Basu","Varun Jampani"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I222","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.74.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models","tldr":"Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational...","track":"Industry","underline_id":79802,"underline_url":"https://underline.io/events/395/posters/15254/poster/79802-kafa-rethinking-image-ad-understanding-with-knowledge-augmented-feature-adaptation-of-vision-language-models","video_url":null},{"abstract":"Identifying granular and actionable topics from customer questions (CQ) posted on e-commerce websites helps surface the missing information expected by customers on the product detail page (DP), provide insights to brands and sellers on what critical product information that the customers are looking before making a purchase decision and helps enrich the catalog quality to improve the overall customer experience (CX). We propose a weakly supervised Hierarchical Multi-task Classification Framework (HMCF) to identify topics from customer questions at various granularities. Complexity lies in creating a list of granular topics (taxonomy) for 1000s of product categories and building a scalable classification system. To this end, we introduce a clustering based Taxonomy Creation and Data Labeling (TCDL) module for creating taxonomy and labelled data with minimal supervision. Using TCDL module, taxonomy and labelled data creation task reduces to 2 hours as compared to 2 weeks of manual efforts by a subject matter expert. For classification, we propose a two level HMCF that performs multi-class classification to identify coarse level-1 topic and leverages NLI based label-aware approach to identify granular level-2 topic. We showcase that HMCF (based on BERT and NLI) a) achieves absolute improvement of 13\\% in Top-1 accuracy over single-task non-hierarchical baselines b) learns a generic domain invariant function that can adapt to constantly evolving taxonomy (open label set) without need of re-training. c) reduces model deployment efforts significantly since it needs only one model that caters to 1000s of product categories.","anthology_url":"https://aclanthology.org/2023.acl-industry.75","authors":["Jitenkumar Rana","Promod Yenigalla","Chetan Aggarwal","Sandeep Sricharan Mukku","Manan Soni","Rashmi Patange"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I225","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.75.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79782/poster_document/0f9bbe9e64a5f1d8aec09291a13b4c77.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Weakly supervised hierarchical multi-task classification of customer questions","tldr":"Identifying granular and actionable topics from customer questions (CQ) posted on e-commerce websites helps surface the missing information expected by customers on the product detail page (DP), provide insights to brands and sellers on what critical product information that the customers are lookin...","track":"Industry","underline_id":79782,"underline_url":"https://underline.io/events/395/posters/15254/poster/79782-weakly-supervised-hierarchical-multi-task-classification-of-customer-questions","video_url":null},{"abstract":"Automated digitization of prescription images is a critical prerequisite to scale digital healthcare services such as online pharmacies. This is challenging in emerging markets since prescriptions are not digitized at source and patients lack the medical expertise to interpret prescriptions to place orders. In this paper, we present prescription digitization system for online medicine ordering built with minimal supervision. Our system uses a modular pipeline comprising a mix of ML and rule-based components for (a) image to text extraction, (b) segmentation into blocks and medication items, (c) medication attribute extraction, (d) matching against medicine catalog, and (e) shopping cart building. Our approach efficiently utilizes multiple signals like layout, medical ontologies, and semantic embeddings via LayoutLMv2 model to yield substantial improvement relative to strong baselines on medication attribute extraction. Our pipeline achieves +5.9\\% gain in precision@3 and +5.6\\% in recall@3 over catalog-based fuzzy matching baseline for shopping cart building for printed prescriptions.","anthology_url":"https://aclanthology.org/2023.acl-industry.76","authors":["Megha Sharma","Tushar Vatsal","Srujana Merugu","Aruna Rajan"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I227","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.76.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Automated Digitization of Unstructured Medical Prescriptions","tldr":"Automated digitization of prescription images is a critical prerequisite to scale digital healthcare services such as online pharmacies. This is challenging in emerging markets since prescriptions are not digitized at source and patients lack the medical expertise to interpret prescriptions to place...","track":"Industry","underline_id":79829,"underline_url":"https://underline.io/events/395/posters/15240/poster/79829-automated-digitization-of-unstructured-medical-prescriptions","video_url":null},{"abstract":"Various Vision-Language Pre-training (VLP) models (e.g., CLIP, BLIP) have sprung up and dramatically advanced the benchmarks for public general-domain datasets (e.g., COCO, Flickr30k). Such models usually learn the cross-modal alignment from large-scale well-aligned image-text datasets without leveraging external knowledge. Adapting these models to downstream applications in specific domains like fashion requires fine-grained in-domain image-text corpus, which are usually less semantically aligned and in small scale that requires efficient pre-training strategies. In this paper, we propose a knowledge-guided fashion-domain language-image pre-training (FLIP) framework that focuses on learning fine-grained representations in e-commerce domain and utilizes external knowledge (i.e., product attribute schema), to improve the pre-training efficiency. Experiments demonstrate that FLIP outperforms previous state-of-the-art  VLP models  on Amazon data and on the Fashion-Gen dataset by large margins. FLIP has been successfully deployed in the Amazon catalog system to backfill missing attributes and improve the customer shopping experience.","anthology_url":"https://aclanthology.org/2023.acl-industry.9","authors":["Qinjin Jia","Yang Liu","Daoping Wu","Shaoyuan Xu","Huidong Liu","Jinmiao Fu","Roland Vollgraf","Bryan Wang"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I27","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.9.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] KG-FLIP: Knowledge-guided Fashion-domain Language-Image Pre-training for E-commerce","tldr":"Various Vision-Language Pre-training (VLP) models (e.g., CLIP, BLIP) have sprung up and dramatically advanced the benchmarks for public general-domain datasets (e.g., COCO, Flickr30k). Such models usually learn the cross-modal alignment from large-scale well-aligned image-text datasets without lever...","track":"Industry","underline_id":79785,"underline_url":"https://underline.io/events/395/posters/15254/poster/79785-kg-flip-knowledge-guided-fashion-domain-language-image-pre-training-for-e-commerce","video_url":null},{"abstract":"Due to the democratization of e-commerce, many product companies are listing their goods for online shopping. For periodic buying within a domain such as Grocery, consumers are generally inclined to buy certain brands of products.\nDue to a large non-English speaking population in India, we observe a significant percentage of code-mix Hinglish search queries e.g., sasta atta. An intuitive approach to dealing with code-mix queries is to train an encoder-decoder model to translate the query to English to perform the search. However, the problem becomes non-trivial when the brand names themselves have Hinglish names and possibly have a literal English translation. In such queries, only the context (non-brand name) Hinglish words needs to be translated. In this paper, we propose a simple yet effective modification to the transformer training to preserve/correct Grocery brand names in the output while selectively translating the context words. To achieve this, we use an additional dataset of popular Grocery brand names. Brand names are added as tokens to the model vocabulary, and the token embeddings are randomly initialized. Further, we introduce a Brand loss in training the translation model. Brand loss is a cross entropy loss computed using a denoising auto-encoder objective with brand name data. We warm-start the training from a public pre-trained checkpoint (such as BART/T5) and further adapt it for query translation using the domain data. The proposed model is generic and can be used with English as well as code-mix Hinglish queries alleviating the need for language detection. To reduce the latency of the model for the production deployment, we use knowledge distillation and quantization. Experimental evaluation indicates that the proposed approach improves translation results by preserving/correcting English/Hinglish brand names. After positive results with A/B testing, the model is currently deployed in production.","anthology_url":"https://aclanthology.org/2023.acl-industry.10","authors":["Mandar Kulkarni","Nikesh Garera","Anusua Trivedi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I29","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.10.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Domain-specific transformer models for query translation","tldr":"Due to the democratization of e-commerce, many product companies are listing their goods for online shopping. For periodic buying within a domain such as Grocery, consumers are generally inclined to buy certain brands of products.\nDue to a large non-English speaking population in India, we observe a...","track":"Industry","underline_id":79827,"underline_url":"https://underline.io/events/395/posters/15240/poster/79827-domain-specific-transformer-models-for-query-translation","video_url":null},{"abstract":"In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in the deployment. It includes the balance between performance and cost, segmentation ambiguity due to domain diversity and vague words boundary, and multi-grained segmentation. In this context, we propose a simple yet effective approach, namely CWSeg, to augment PLM-based schemes by developing cohort training and versatile decoding strategies. Extensive experiments on benchmark datasets demonstrate the efficiency and generalization of our approach. The corresponding segmentation system is also implemented for practical usage and the demo is recorded.","anthology_url":"https://aclanthology.org/2023.acl-industry.1","authors":["Dedong Li","Rui Zhao","Fei Tan"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I3","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.1.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] CWSeg: An Efficient and General Approach to Chinese Word Segmentation","tldr":"In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in ...","track":"Industry","underline_id":79794,"underline_url":"https://underline.io/events/395/posters/15254/poster/79794-cwseg-an-efficient-and-general-approach-to-chinese-word-segmentation","video_url":null},{"abstract":"To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A chatbot should be able to understand and answer a variety of such queries to help users with relevant information. One of the important modules in the chatbot is automated intent identification, i.e., understanding the user's intention from the query text. \nDue to non-English speaking users interacting with the chatbot, we often get a significant percentage of code mix queries and queries with grammatical errors, which makes the problem more challenging. \nThis paper proposes a simple yet competent Semi-Supervised Learning (SSL) approach for label-efficient intent classification. We use a small labeled corpus and relatively larger unlabeled query data to train a transformer model. For training the model with labeled data, we explore supervised MixUp data augmentation. To train with unlabeled data, we explore label consistency with dropout noise. We experiment with different pre-trained transformer architectures, such as BERT and sentence-BERT. \nExperimental results demonstrate that the proposed approach significantly improves over the supervised baseline, even with a limited labeled set. A variant of the model is currently deployed in production.","anthology_url":"https://aclanthology.org/2023.acl-industry.11","authors":["Mandar Kulkarni","Kyung Kim","Nikesh Garera","Anusua Trivedi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I30","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.11.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Label efficient semi-supervised conversational intent classification","tldr":"To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A c...","track":"Industry","underline_id":79825,"underline_url":"https://underline.io/events/395/posters/15240/poster/79825-label-efficient-semi-supervised-conversational-intent-classification","video_url":null},{"abstract":"Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers' questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product information available in English. To study this practical industrial task, we present xPQA, a large-scale annotated cross-lingual PQA dataset in 12 languages, and report results in (1) candidate ranking, to select the best English candidate containing the information to answer a non-English question; and (2) answer generation, to generate a natural-sounding non-English answer based on the selected English candidate.\nWe evaluate various approaches involving machine translation at runtime or offline, leveraging multilingual pre-trained LMs, and including or excluding xPQA training data. We find that in-domain data is essential as cross-lingual rankers trained on other domains perform poorly on the PQA task, and that translation-based approaches are most effective for candidate ranking while multilingual finetuning works best for answer generation. Still, there remains a significant performance gap between the English and the cross-lingual test sets.","anthology_url":"https://aclanthology.org/2023.acl-industry.12","authors":["Xiaoyu Shen","Akari Asai","Bill Byrne","Adria De Gispert"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I32","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.12.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] xPQA: Cross-Lingual Product Question Answering in 12 Languages","tldr":"Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers' questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product inf...","track":"Industry","underline_id":79832,"underline_url":"https://underline.io/events/395/posters/15254/poster/79832-xpqa-cross-lingual-product-question-answering-in-12-languages","video_url":null},{"abstract":"Fake news detection has been a critical task for maintaining the health of the online news ecosystem. However, very few existing works consider the temporal shift issue caused by the rapidly-evolving nature of news data in practice, resulting in significant performance degradation when training on past data and testing on future data. In this paper, we observe that the appearances of news events on the same topic may display discernible patterns over time, and posit that such patterns can assist in selecting training instances that could make the model adapt better to future data. Specifically, we design an effective framework FTT (Forecasting Temporal Trends), which could forecast the temporal distribution patterns of news data and then guide the detector to fast adapt to future distribution. Experiments on the real-world temporally split dataset demonstrate the superiority of our proposed framework.","anthology_url":"https://aclanthology.org/2023.acl-industry.13","authors":["Beizhe Hu","Qiang Sheng","Juan Cao","Yongchun Zhu","Danding Wang","Zhengjia Wang","Zhiwei Jin"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I36","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.13.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79821/poster_document/fa57638f5848831ae70bbd59a080ebbe.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79821/slideshow/c45978b300695c4f01ca39023f6078ae.pdf","title":"[Industry] Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection","tldr":"Fake news detection has been a critical task for maintaining the health of the online news ecosystem. However, very few existing works consider the temporal shift issue caused by the rapidly-evolving nature of news data in practice, resulting in significant performance degradation when training on p...","track":"Industry","underline_id":79821,"underline_url":"https://underline.io/events/395/posters/15240/poster/79821-industry-learn-over-past-evolve-for-future-forecasting-temporal-trends-for-fake-news-detection","video_url":null},{"abstract":"Getting a good understanding of the user intent is vital for e-commerce applications to surface the right product to a given customer query. Query Understanding (QU) systems are essential for this purpose, and many e-commerce providers are working on complex solutions that need to be data efficient and able to capture early emerging market trends. Query Attribute Understanding (QAU) is a sub-component of QU that involves extracting named attributes from user queries and linking them to existing e-commerce entities such as brand, material, color, etc. While extracting named entities from text has been extensively explored in the literature, QAU requires specific attention due to the nature of the queries, which are often short, noisy, ambiguous, and constantly evolving. \nThis paper makes three contributions to QAU. First, we propose a novel end-to-end approach that jointly solves Named Entity Recognition (NER) and Entity Linking (NEL) and enables open-world reasoning for QAU. Second, we introduce a novel method for utilizing product graphs to enhance the representation of query entities. Finally, we present a new dataset constructed from public sources that can be used to evaluate the performance of future QAU systems.","anthology_url":"https://aclanthology.org/2023.acl-industry.14","authors":["Thomas Ricatte","Donato Crisostomi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I41","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.14.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] AVEN-GR: Attribute Value Extraction and Normalization using product GRaphs","tldr":"Getting a good understanding of the user intent is vital for e-commerce applications to surface the right product to a given customer query. Query Understanding (QU) systems are essential for this purpose, and many e-commerce providers are working on complex solutions that need to be data efficient ...","track":"Industry","underline_id":79770,"underline_url":"https://underline.io/events/395/posters/15254/poster/79770-aven-gr-attribute-value-extraction-and-normalization-using-product-graphs","video_url":null},{"abstract":"Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods. To overcome these challenges, we propose GKD, a general knowledge distillation framework that supports distillation on larger-scale PLMs using various distillation methods. With GKD, developers can build larger distillation models on memory-limited GPUs and easily switch and combine different distillation methods within a single framework. Experimental results show that GKD can support the distillation of at least 100B-scale PLMs and 25 mainstream methods on 8 NVIDIA A100 (40GB) GPUs.","anthology_url":"https://aclanthology.org/2023.acl-industry.15","authors":["Shicheng Tan","Weng Lam Tam","Yuanchun Wang","Wenwen Gong","Shu Zhao","Peng Zhang","Jie Tang"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I42","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.15.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model","tldr":"Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world indu...","track":"Industry","underline_id":79824,"underline_url":"https://underline.io/events/395/posters/15240/poster/79824-gkd-an-general-knowledge-distillation-framework-for-large-scale-pre-trained-language-model","video_url":null},{"abstract":"Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (VLP) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained interactions between objects from different modalities are far from well-established. This issue becomes more severe in the e-commerce domain, which lacks sufficient training data and fine-grained cross-modal knowledge. To alleviate the problem, this paper proposes a novel e-commerce knowledge-enhanced VLP model FashionKLIP. We first automatically establish a multi-modal conceptual knowledge graph from large-scale e-commerce image-text data, and then inject the prior knowledge into the VLP model to align across modalities at the conceptual level. The experiments conducted on a public benchmark dataset demonstrate that FashionKLIP effectively enhances the performance of e-commerce image-text retrieval upon state-of-the-art VLP models by a large margin. The application of the method in real industrial scenarios also proves the feasibility and efficiency of FashionKLIP.","anthology_url":"https://aclanthology.org/2023.acl-industry.16","authors":["Xiaodan Wang","Chengyu Wang","Lei Li","Zhixu Li","Ben Chen","Linbo Jin","Jun Huang","Yanghua Xiao","Ming Gao"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I43","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.16.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] FashionKLIP: Enhancing E-Commerce Image-Text Retrieval with Fashion Multi-Modal Conceptual Knowledge Graph","tldr":"Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (VLP) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained inter...","track":"Industry","underline_id":79796,"underline_url":"https://underline.io/events/395/posters/15240/poster/79796-fashionklip-enhancing-e-commerce-image-text-retrieval-with-fashion-multi-modal-conceptual-knowledge-graph","video_url":null},{"abstract":"Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of interest in a spoken utterance. We investigate improving joint IC and NER models using entity contrastive learning that attempts to cluster similar entities together in a learned representation space. We compare a full virtual assistant system trained using entity contrastive learning to a production baseline system that does not use contrastive learning. We present both offline results, using retrospective test sets, as well as live online results from an A/B test that compared the two systems. In both the offline and online settings, entity contrastive training improved overall performance against production baselines. Furthermore, we provide a detailed analysis of learned entity embeddings, including both qualitative analysis via dimensionality-reduced visualizations and quantitative analysis by computing alignment and uniformity metrics. We show that entity contrastive learning improves alignment metrics and produces well-formed embedding clusters in representation space.","anthology_url":"https://aclanthology.org/2023.acl-industry.17","authors":["Jonathan Rubin","Jason Crowley","George Leung","Morteza Ziyadi","Maria Minakova"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I45","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.17.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Entity Contrastive Learning in a Large-Scale Virtual Assistant System","tldr":"Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of ...","track":"Industry","underline_id":79788,"underline_url":"https://underline.io/events/395/posters/15254/poster/79788-entity-contrastive-learning-in-a-large-scale-virtual-assistant-system","video_url":null},{"abstract":"Product catalogs, conceptually in the form of text-rich tables, are self-reported by individual retailers and thus inevitably contain noisy facts. Verifying such textual attributes in product catalogs is essential to improve their reliability. However, popular methods for processing free-text content, such as pre-trained language models, are not particularly effective on structured tabular data since they are typically trained on free-form natural language texts. In this paper, we present Tab-Cleaner, a model designed to handle error detection over text-rich tabular data following a pre-training / fine-tuning paradigm. We train Tab-Cleaner on a real-world Amazon Product Catalog table w.r.t millions of products and show improvements over state-of-the-art methods by 16\\textbackslash{}\\% on PR AUC over attribute applicability classification task and by 11\\textbackslash{}\\%  on PR AUC over attribute value validation task.","anthology_url":"https://aclanthology.org/2023.acl-industry.18","authors":["Kewei Cheng","Xian Li","Zhengyang Wang","Chenwei Zhang","Binxuan Huang","Yifan Ethan Xu","Xin Luna Dong","Yizhou Sun"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I46","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.18.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Tab-Cleaner: Weakly Supervised Tabular Data Cleaning via Pre-training for E-commerce Catalog","tldr":"Product catalogs, conceptually in the form of text-rich tables, are self-reported by individual retailers and thus inevitably contain noisy facts. Verifying such textual attributes in product catalogs is essential to improve their reliability. However, popular methods for processing free-text conten...","track":"Industry","underline_id":79828,"underline_url":"https://underline.io/events/395/posters/15240/poster/79828-tab-cleaner-weakly-supervised-tabular-data-cleaning-via-pre-training-for-e-commerce-catalog","video_url":null},{"abstract":"Measurement of interaction quality is a critical task for the improvement of large-scale spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the proposed evaluation model shows better domain generalization ability compared to the baselines. On the basis of these results, we argue that having high-quality human-annotated data is an important component of evaluating interaction quality for large industrial-scale voice assistant platforms.","anthology_url":"https://aclanthology.org/2023.acl-industry.19","authors":["Abishek Komma","Nagesh Panyam Chandrasekarasastry","Timothy Leffel","Anuj Goyal","Angeliki Metallinou","Spyros Matsoukas","Aram Galstyan"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I47","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.19.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs","tldr":"Measurement of interaction quality is a critical task for the improvement of large-scale spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately followi...","track":"Industry","underline_id":79811,"underline_url":"https://underline.io/events/395/posters/15254/poster/79811-toward-more-accurate-and-generalizable-evaluation-metrics-for-task-oriented-dialogs","video_url":null},{"abstract":"Existing conversational question answering (CQA) datasets have been usually constructed from unstructured texts in English. In this paper, we propose Tab-CQA, a tabular CQA dataset created from Chinese financial reports that are extracted from listed companies in a wide range of different sectors in the past 30 years. From these reports, we select 2,463 tables, and manually generate 2,463 conversations with 35,494 QA pairs. Additionally,  we select 4,578 tables, from which 4,578 conversations with 73,595 QA pairs are automatically created via a template-based method. With the manually- and automatically-generated conversations, Tab-CQA contains answerable and unanswerable questions. For the answerable questions, we further diversify them to cover a wide range of skills, e.g., table retrieval, fact checking, numerical reasoning, so as to accommodate real-world scenarios. We further propose two different tabular CQA models, a text-based model and an operation-based model, and evaluate them on Tab-CQA. Experiment results show that Tab-CQA is a very challenging dataset, where a huge performance gap exists between human and neural models. We will publicly release Tab-CQA as a benchmark testbed to promote further research on Chinese tabular CQA.","anthology_url":"https://aclanthology.org/2023.acl-industry.20","authors":["Chuang Liu","Junzhuo Li","Deyi Xiong"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I54","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.20.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79820/poster_document/1156ee7b8427fcc529e3743ba81c8a1b.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Tab-CQA: A Tabular Conversational Question Answering Dataset on Financial Reports","tldr":"Existing conversational question answering (CQA) datasets have been usually constructed from unstructured texts in English. In this paper, we propose Tab-CQA, a tabular CQA dataset created from Chinese financial reports that are extracted from listed companies in a wide range of different sectors in...","track":"Industry","underline_id":79820,"underline_url":"https://underline.io/events/395/posters/15240/poster/79820-tab-cqa-a-tabular-conversational-question-answering-dataset-on-financial-reports","video_url":null},{"abstract":"Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs. To this end, we present KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47\\%p on average for HyperClova (30B and 82B), and GPT-3.","anthology_url":"https://aclanthology.org/2023.acl-industry.21","authors":["Hwaran Lee","Seokhee Hong","Joonsuk Park","Takyoung Kim","Gunhee Kim","Jung-woo Ha"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I55","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.21.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79776/poster_document/89608b79720bb64a5b3d5e00751a4196.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications","tldr":"Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea ...","track":"Industry","underline_id":79776,"underline_url":"https://underline.io/events/395/posters/15254/poster/79776-kosbi-a-dataset-for-mitigating-social-bias-risks-towards-safer-large-language-model-applications","video_url":null},{"abstract":"Through an online customer service application, we have collected many conversations between customer service agents and customers. Building a knowledge production system can help reduce the labor cost of maintaining the FAQ database for the customer service chatbot, whose core module is question answering (QA) on these conversations. However, most existing researches focus on document-based QA tasks, and there is a lack of researches on conversation-based QA and related datasets, especially in Chinese language. The challenges of conversation-based QA include: 1) answers may be scattered among multiple dialogue turns; 2) understanding complex dialogue contexts is more complicated than documents. To address these challenges, we propose a multi-span extraction model on this task and introduce continual pre-training and multi-task learning schemes to further improve model performance. To validate our approach, we construct two Chinese datasets using dialogues as the knowledge source, namely cs-qaconv and kd-qaconv, respectively. Experimental results demonstrate that the proposed model outperforms the baseline on both datasets. The online application also verifies the effectiveness of our method. The dataset kd-qaconv will be released publicly for research purposes.","anthology_url":"https://aclanthology.org/2023.acl-industry.22","authors":["Changlin Yang","Siye Liu","Sen Hu","Wangshu Zhang","Teng Xu","Jing Zheng"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I60","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.22.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Improving Knowledge Production Efficiency With Question Answering on Conversation","tldr":"Through an online customer service application, we have collected many conversations between customer service agents and customers. Building a knowledge production system can help reduce the labor cost of maintaining the FAQ database for the customer service chatbot, whose core module is question an...","track":"Industry","underline_id":79826,"underline_url":"https://underline.io/events/395/posters/15240/poster/79826-improving-knowledge-production-efficiency-with-question-answering-on-conversation","video_url":null},{"abstract":"Datasets used to train deep learning models in industrial settings often exhibit skewed distributions with some samples repeated a large number of times.\nThis paper presents a simple yet effective solution to reduce the increased burden of repeated computation on redundant datasets.\nOur approach eliminates duplicates at the batch level, without altering the data distribution observed by the model, making it model-agnostic and easy to implement as a plug-and-play module. \nWe also provide a mathematical expression to estimate the reduction in training time that our approach provides. \nThrough empirical evidence, we show that our approach significantly reduces training times on various models across datasets with varying redundancy factors, without impacting their performance on the Named Entity Recognition task, both on publicly available datasets and in real industrial settings.\nIn the latter, the approach speeds training by up to 87\\%, and by 46\\% on average, with a drop in model performance of 0.2\\% relative at worst.\nWe finally release a modular and reusable codebase to further advance research in this area.","anthology_url":"https://aclanthology.org/2023.acl-industry.23","authors":["Donato Crisostomi","Andrea Caciolai","Alessandro Pedrani","Kay Rottmann","Alessandro Manzotti","Enrico Palumbo","Davide Bernardi"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I65","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.23.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Mitigating the Burden of Redundant Datasets via Batch-Wise Unique Samples and Frequency-Aware Losses","tldr":"Datasets used to train deep learning models in industrial settings often exhibit skewed distributions with some samples repeated a large number of times.\nThis paper presents a simple yet effective solution to reduce the increased burden of repeated computation on redundant datasets.\nOur approach eli...","track":"Industry","underline_id":79812,"underline_url":"https://underline.io/events/395/posters/15254/poster/79812-mitigating-the-burden-of-redundant-datasets-via-batch-wise-unique-samples-and-frequency-aware-losses","video_url":null},{"abstract":"Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context windows, Large Language Models (LLMs) are a natural fit for this use case. However, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model's utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM --- prompt engineering, fine-tuning, and knowledge distillation --- using feedback from the brand's customer service agents. We find that the usability of a model's responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space.","anthology_url":"https://aclanthology.org/2023.acl-industry.24","authors":["Kristen Howell","Gwen Christian","Pavel Fomitchov","Gitit Kehat","Julianne Marzulla","Leanne Rolston","Jadin Tredup","Ilana Zimmerman","Ethan Selfridge","Joseph Bradley"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I66","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.24.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Distilled Language Models are economically efficient for the enterprise. ...mostly.","tldr":"Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context wind...","track":"Industry","underline_id":79803,"underline_url":"https://underline.io/events/395/posters/15254/poster/79803-distilled-language-models-are-economically-efficient-for-the-enterprise-mostly","video_url":null},{"abstract":"On-device automatic speech recognition systems face several challenges compared to server-based systems. They have to meet stricter constraints in terms of speed, disk size and memory while maintaining the same accuracy. Often they have to serve several ap- plications with different distributions at once, such as communicating with a virtual assistant and speech-to-text. The simplest solution to serve multiple applications is to build application-specific (language) models, but this leads to an increase in memory. Therefore, we explore different data- and architecture-driven language modeling approaches to build a single application-agnostic model. We propose two novel feed-forward architectures that find an optimal trade off between different on-device constraints. In comparison to the application-specific solution, one of our novel approaches reduces the disk size by half, while maintaining speed and accuracy of the original model.","anthology_url":"https://aclanthology.org/2023.acl-industry.25","authors":["Markus Nussbaum-thom","Lyan Verwimp","Youssef Oualil"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I69","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.25.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Application-Agnostic Language Modeling for On-Device ASR","tldr":"On-device automatic speech recognition systems face several challenges compared to server-based systems. They have to meet stricter constraints in terms of speed, disk size and memory while maintaining the same accuracy. Often they have to serve several ap- plications with different distributions at...","track":"Industry","underline_id":79791,"underline_url":"https://underline.io/events/395/posters/15254/poster/79791-application-agnostic-language-modeling-for-on-device-asr","video_url":null},{"abstract":"Automatic Speech Recognition (ASR) is essential for any voice-based application. The streaming capability of ASR becomes necessary to provide immediate feedback to the user in applications like Voice Search. LSTM/RNN and CTC based ASR systems are very simple to train and deploy for low latency streaming applications but have lower accuracy when compared to the state-of-the-art models. In this work, we build accurate LSTM, attention and CTC based streaming ASR models for large-scale Hinglish (blend of Hindi and English) Voice Search. We evaluate how various modifications in vanilla LSTM training improve the system's accuracy while preserving the streaming capabilities. We also discuss a simple integration of end-of-speech (EOS) detection with CTC models, which helps reduce the overall search latency. Our model achieves a word error rate (WER) of 3.69\\% without EOS and 4.78\\% with EOS, with \\textasciitilde{}1300 ms (\\textasciitilde{}46.64\\%) reduction in latency.","anthology_url":"https://aclanthology.org/2023.acl-industry.26","authors":["Abhinav Goyal","Nikesh Garera"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I75","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.26.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79813/poster_document/e7b75f1aff45d56f2df4cd44a81b4636.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Building Accurate Low Latency ASR for Streaming Voice Search in E-commerce","tldr":"Automatic Speech Recognition (ASR) is essential for any voice-based application. The streaming capability of ASR becomes necessary to provide immediate feedback to the user in applications like Voice Search. LSTM/RNN and CTC based ASR systems are very simple to train and deploy for low latency strea...","track":"Industry","underline_id":79813,"underline_url":"https://underline.io/events/395/posters/15240/poster/79813-building-accurate-low-latency-asr-for-streaming-voice-search-in-e-commerce","video_url":null},{"abstract":"Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52,898 items collected from 6,694 pages and 156,014 attributes, making it the first large-scale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively.","anthology_url":"https://aclanthology.org/2023.acl-industry.27","authors":["Aidan San","Yuan Zhuang","Jan Bakus","Colin Lockard","David Ciemiewicz","Sandeep Atluri","Kevin Small","Yangfeng Ji","Heba Elfardy"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I77","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.27.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] PLAtE: A Large-scale Dataset for List Page Web Extraction","tldr":"Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of L...","track":"Industry","underline_id":79786,"underline_url":"https://underline.io/events/395/posters/15254/poster/79786-plate-a-large-scale-dataset-for-list-page-web-extraction","video_url":null},{"abstract":"Text-to-Image Synthesis (TIS) aims to generate images based on textual inputs. Recently, several large pre-trained diffusion models have been released to create high-quality images with pre-trained text encoders and diffusion-based image synthesizers. However, popular diffusion-based models from the open-source community cannot support industrial domain-specific applications due to the lack of entity knowledge and low inference speed.\nIn this paper, we propose Rapid Diffusion, a novel framework for training and deploying super-resolution, text-to-image latent diffusion models with rich entity knowledge injected and optimized networks.\nFurthermore, we employ BladeDISC, an end-to-end Artificial Intelligence (AI) compiler, and FlashAttention techniques to optimize computational graphs of the generated models for online deployment. Experiments verify the effectiveness of our approach in terms of image quality and inference speed. In addition, we present industrial use cases and integrate Rapid Diffusion to an AI platform to show its practical values.","anthology_url":"https://aclanthology.org/2023.acl-industry.28","authors":["Bingyan Liu","Weifeng Lin","Zhongjie Duan","Chengyu Wang","Wu Ziheng","Zhang Zipeng","Kui Jia","Lianwen Jin","Cen Chen","Jun Huang"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I78","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.28.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Rapid Diffusion: Building Domain-Specific Text-to-Image Synthesizers with Fast Inference Speed","tldr":"Text-to-Image Synthesis (TIS) aims to generate images based on textual inputs. Recently, several large pre-trained diffusion models have been released to create high-quality images with pre-trained text encoders and diffusion-based image synthesizers. However, popular diffusion-based models from the...","track":"Industry","underline_id":79804,"underline_url":"https://underline.io/events/395/posters/15240/poster/79804-rapid-diffusion-building-domain-specific-text-to-image-synthesizers-with-fast-inference-speed","video_url":null},{"abstract":"In conventional radiology practice, the radiologist dictates the diagnosis to the transcriptionist, who then prepares a preliminary formatted report referring to the notes, after which the radiologist reviews the report, corrects the errors, and signs off. This workflow is prone to delay and error. In this paper, we report our work on automatic radiology report generation from radiologists' dictation, which is in collaboration with a startup about to become Unicorn. A major contribution of our work is the set of knowledge graphs (KGs) of ten abdominal organs- Liver, Kidney, Gallbladder, Uterus, Urinary bladder, Ovary, Pancreas, Prostate, Biliary Tree, and Bowel. Our method for constructing these KGs relies on extracting entity1-relation-entity2 triplets from a large collection (about 10,000) of free-text radiology reports. The quality and coverage of the KGs are verified by two experienced radiologists (practicing for the last 30 years and 8 years, respectively). The dictation of the radiologist is automatically converted to what is called a pathological description which is the clinical description of the findings of the radiologist during ultrasonography (USG).\u00a0\nOur knowledge-enhanced deep learning model improves the reported BLEU-3, ROUGE-L, METEOR, and CIDEr scores of the pathological description generation by 2\\%, 4\\%, 2\\% and 2\\% respectively. To the best of our knowledge, this is the first attempt at representing the abdominal organs in the form of knowledge graphs and utilising these graphs for the automatic generation of USG reports. A Minimum Viable Product (MVP) has been made available to the beta users, i.e., radiologists of reputed hospitals, for testing and evaluation. Our solution guarantees report generation within 30 seconds of running a scan.","anthology_url":"https://aclanthology.org/2023.acl-industry.2","authors":["Kaveri Kale","Pushpak Bhattacharyya","Aditya Shetty","Milind Gune","Kush Shrivastava","Rustom Lawyer","Spriha Biswas"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I8","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.2.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79823/poster_document/881eb9492728179747e752bc3beb1984.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79823/slideshow/59c42574fd13f7bfa5d35134602e639d.pptx","title":"[Industry] \"Knowledge is Power\": Constructing Knowledge Graph of Abdominal Organs and Using Them for Automatic Radiology Report Generation","tldr":"In conventional radiology practice, the radiologist dictates the diagnosis to the transcriptionist, who then prepares a preliminary formatted report referring to the notes, after which the radiologist reviews the report, corrects the errors, and signs off. This workflow is prone to delay and error. ...","track":"Industry","underline_id":79823,"underline_url":"https://underline.io/events/395/posters/15240/poster/79823-industry-knowledge-is-power-constructing-knowledge-graph-of-abdominal-organs-and-using-them-for-automatic-radiology-report-generation","video_url":null},{"abstract":"E-commerce websites (e.g. Amazon, Alibaba) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often don't label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute values from an eCommerce product page that contains both text and images is a challenging task, especially when the attribute value is not explicitly mentioned in the catalog. In this paper, we present a scalable solution for this problem where we pose attribute extraction problem as a question-answering task, which we solve using MXT, that consists of three key components: (i) MAG (Multimodal Adaptation Gate), (ii) Xception network, and (iii) T5 encoder-decoder. Our system consists of a generative model that generates attribute-values for a given product by using both textual and visual characteristics (e.g. images) of the product. We show that our system is capable of handling zero-shot attribute prediction (when attribute value is not seen in training data) and value-absent prediction (when attribute value is not mentioned in the text) which are missing in traditional classification-based and NER-based models respectively. We have trained our models using distant supervision, removing dependency on human labeling, thus making them practical for real-world applications. With this framework, we are able to train a single model for 1000s of (product-type, attribute) pairs, thus reducing the overhead of training and maintaining separate models. Extensive experiments on two real world datasets (total 57 attributes) show that our framework improves the absolute recall@90P by 10.16\\% and 6.9\\textbackslash{} from the existing state of the art models. In a popular e-commerce store, we have productionized our models that cater to \\textgreater{}12K (product-type, attribute) pairs, and have extracted \\textgreater{}150MM attribute values.","anthology_url":"https://aclanthology.org/2023.acl-industry.29","authors":["Anant Khandelwal","Happy Mittal","Shreyas Kulkarni","Deepak Gupta"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-3_-industry-(oral)"],"id":"I81","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.29.pdf","paper_type":"industry","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/79762/poster/e8177be4443196b6eadc3b2b5a7ea6a4.jpg","program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes","tldr":"E-commerce websites (e.g. Amazon, Alibaba) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often don't label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute v...","track":"Industry","underline_id":79762,"underline_url":"https://underline.io/events/395/sessions/15229/lecture/79762-industry-large-scale-generative-multimodal-attribute-extraction-for-e-commerce-attributes","video_url":null},{"abstract":"The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications.\nAt its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.\n\nTo address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. We use a semi-supervised approach for data augmentation and presents two different methods for utilizing unlabeled samples. One method relies directly on existing catalogs, while the other uses a generative model. We compare the pros and cons of each approach and present our experimental results.","anthology_url":"https://aclanthology.org/2023.acl-industry.30","authors":["Noa Avigdor","Guy Horowitz","Ariel Raviv","Stav Yanovsky Daye"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I83","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.30.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79783/poster_document/61c72c3ecffb7cffb46ece9393476001.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79783/slideshow/7b6cc043f9e2d33dbf970a7840e3c454.pdf","title":"[Industry] Consistent Text Categorization using Data Augmentation in e-Commerce","tldr":"The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications.\nAt its core, the product ca...","track":"Industry","underline_id":79783,"underline_url":"https://underline.io/events/395/posters/15240/poster/79783-industry-consistent-text-categorization-using-data-augmentation-in-e-commerce","video_url":null},{"abstract":"We present an efficient and reliable approach to Natural Language Querying (NLQ) on databases (DB) which is not based on text-to-SQL type semantic parsing. Our approach simplifies the NLQ on structured data problem to the following \"bread and butter\" NLP tasks: (a) Domain classification, for choosing which DB table to query, whether the question is out-of-scope (b) Multi-head slot/entity extraction (SE) to extract the field criteria and other attributes such as its role (filter, sort etc) from the raw text and (c) Slot value disambiguation (SVD) to resolve/normalize raw spans from SE to format suitable to query a DB. This is a general purpose, DB language agnostic approach and the output can be used to query any DB and return results to the user. Also each of these tasks is extremely well studied, mature, easier to collect data for and enables better error analysis by tracing problems to specific components when something goes wrong.","anthology_url":"https://aclanthology.org/2023.acl-industry.31","authors":["Hanoz Bhathena","Aviral Joshi","Prateek Singh"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I90","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.31.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79775/poster_document/4146a5f8b1d4323f3b3ecffa31bb26a4.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] An efficient method for Natural Language Querying on Structured Data","tldr":"We present an efficient and reliable approach to Natural Language Querying (NLQ) on databases (DB) which is not based on text-to-SQL type semantic parsing. Our approach simplifies the NLQ on structured data problem to the following \"bread and butter\" NLP tasks: (a) Domain classification, for choosin...","track":"Industry","underline_id":79775,"underline_url":"https://underline.io/events/395/posters/15254/poster/79775-an-efficient-method-for-natural-language-querying-on-structured-data","video_url":null},{"abstract":"Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients' clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases.","anthology_url":"https://aclanthology.org/2023.acl-industry.32","authors":["Zekai Chen","Mariann Micsinai Balan","Kevin Brown"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I92","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.32.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79822/poster_document/890cf431b628e1dccbfe339755d293c3.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/79822/slideshow/b9ce2b04446734ebbacd3dfa536ab86d.pdf","title":"[Industry] Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy","tldr":"Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immu...","track":"Industry","underline_id":79822,"underline_url":"https://underline.io/events/395/posters/15240/poster/79822-boosting-transformers-and-language-models-for-clinical-prediction-in-immunotherapy","video_url":null},{"abstract":"This work proposes a method named EvolveMT for the efficient combination of multiple machine translation (MT) engines. The method selects the output from one engine for each segment, using online learning techniques to predict the most appropriate system for each translation request. A neural quality estimation metric supervises the method without requiring reference translations. The method's online learning capability enables it to adapt to changes in the domain or MT engines dynamically, eliminating the requirement for retraining. The method selects a subset of translation engines to be called based on the source sentence features. The degree of exploration is configurable according to the desired quality-cost trade-off. Results from custom datasets demonstrate that EvolveMT achieves similar translation accuracy at a lower cost than selecting the best translation of each segment from all translations using an MT quality estimator. To the best of our knowledge, EvolveMT is the first MT system that adapts itself after deployment to incoming translation requests from the production environment without needing costly retraining on human feedback.","anthology_url":"https://aclanthology.org/2023.acl-industry.33","authors":["Kamer Y\u00fcksel","Ahmet Gunduz","Mohamed Al-badrashiny","Hassan Sawaf"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I93","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.33.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] EvolveMT: an Ensemble MT Engine Improving Itself with Usage Only","tldr":"This work proposes a method named EvolveMT for the efficient combination of multiple machine translation (MT) engines. The method selects the output from one engine for each segment, using online learning techniques to predict the most appropriate system for each translation request. A neural qualit...","track":"Industry","underline_id":79808,"underline_url":"https://underline.io/events/395/posters/15254/poster/79808-evolvemt-an-ensemble-mt-engine-improving-itself-with-usage-only","video_url":null},{"abstract":"Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the other hand, static analysis tools such as linters, which can detect errors without running the program, haven't been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models.\nThrough extensive studies, we also show the impact of sampling temperature, model size, and context on static errors in code completions.","anthology_url":"https://aclanthology.org/2023.acl-industry.34","authors":["Hantian Ding","Varun Kumar","Yuchen Tian","Zijian Wang","Rob Kwiatkowski","Xiaopeng Li","Murali Krishna Ramanathan","Baishakhi Ray","Parminder Bhatia","Sudipta Sengupta"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-5_-industry-(poster)"],"id":"I94","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.34.pdf","paper_type":"industry","poster_pdf":"https://assets.underline.io/lecture/79773/poster_document/ab504be29b12554b5dd1edef4b4b9bbf.pdf","preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] A Static Evaluation of Code Completion by Large Language Models","tldr":"Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform ...","track":"Industry","underline_id":79773,"underline_url":"https://underline.io/events/395/posters/15254/poster/79773-a-static-evaluation-of-code-completion-by-large-language-models","video_url":null},{"abstract":"Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable long-term policy improvements.","anthology_url":"https://aclanthology.org/2023.acl-industry.35","authors":["Sarthak Ahuja","Mohammad Kachuee","Fatemeh Sheikholeslami","Weiqing Liu","Jaeyoung Do"],"category":"Industry","demo_url":null,"display_track":"Industry","event_ids":["session-4_-industry-(virtual-poster)"],"id":"I96","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-industry.35.pdf","paper_type":"industry","poster_pdf":null,"preview_image":null,"program":"Industry","similar_paper_ids":[],"slides_pdf":null,"title":"[Industry] Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems","tldr":"Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance betw...","track":"Industry","underline_id":79805,"underline_url":"https://underline.io/events/395/posters/15240/poster/79805-scalable-and-safe-remediation-of-defective-actions-in-self-learning-conversational-systems","video_url":null}]
