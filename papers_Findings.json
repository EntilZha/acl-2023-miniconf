[{"abstract":"Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.","anthology_url":"https://aclanthology.org/2023.findings-acl.281","authors":["Minbeom Kim","Hwanhee Lee","Kang Min Yoo","Joonsuk Park","Hwaran Lee","Kyomin Jung"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-1_-generation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1018","is_paper":true,"keywords":["inference methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.281.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77394/poster_document/1d6b88cc8c54269cd28c6c9cd39d3f54.pdf","preview_image":"https://assets.underline.io/lecture/77394/poster/54e9b070b0f90ccb76ca3e76d853d61a.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77394/slideshow/68471414e4c7da83c1a107815800a518.pdf","title":"Critic-Guided Decoding for Controlled Text Generation","tldr":"Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality ...","track":"Generation","underline_id":77394,"underline_url":"https://underline.io/events/395/posters/15200/poster/77394-lida-a-tool-for-automatic-generation-of-grammar-agnostic-visualizations-and-infographics-using-large-language-models","video_url":null},{"abstract":"Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases. Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in real-world applications that require real-time or on-device processing capabilities. In this paper, we perform a focused study on the feasibility of applying recent model compression techniques to sketch-based and sequence-to-sequence Text-to-SQL models. Our results reveal that sketch-based Text-to-SQL models generally have higher inference efficiency and respond better to model compression than sequence-to-sequence models, making them ideal for real-world deployments, especially in use cases with simple SQL statements.","anthology_url":"https://aclanthology.org/2023.findings-acl.740","authors":["Shuo Sun","Yuze Gao","Yuchen Zhang","Jian Su","Bin Chen","Yingzhan Lin","Shuqi Sun"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-1_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1040","is_paper":true,"keywords":["lessons from deployment"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.740.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"An Exploratory Study on Model Compression for Text-to-SQL","tldr":"Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases. Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in real-world applications that require ...","track":"Theme: Reality Check","underline_id":77399,"underline_url":"https://underline.io/events/395/posters/15200/poster/77399-an-exploratory-study-on-model-compression-for-text-to-sql","video_url":null},{"abstract":"The prevalence of short video platforms has spawned a lot of fake news videos, which have stronger propagation ability than textual fake news. Thus, automatically detecting fake news videos has been an important countermeasure in practice. Previous works commonly verify each news video individually with multimodal information. Nevertheless, news videos from different perspectives regarding the same event are commonly posted together, which contain complementary or contradictory information and thus can be used to evaluate each other mutually. To this end, we introduce a new and practical paradigm, i.e., cross-sample fake news video detection, and propose a novel framework, Neighbor-Enhanced fakE news video Detection (NEED), which integrates the neighborhood relationship of new videos belonging to the same event. NEED can be readily combined with existing single-sample detectors and further enhance their performances with the proposed graph aggregation (GA) and debunking rectification (DR) modules. Specifically, given the feature representations obtained from single-sample detectors, GA aggregates the neighborhood information with the dynamic graph to enrich the features of independent samples. After that, DR explicitly leverages the relationship between debunking videos and fake news videos to refute the candidate videos via textual and visual consistency. Extensive experiments on the public benchmark demonstrate that NEED greatly improves the performance of both single-modal (up to 8.34\\% in accuracy) and multimodal (up to 4.97\\% in accuracy) base detectors.","anthology_url":"https://aclanthology.org/2023.findings-acl.756","authors":["Peng Qi","Yuyang Zhao","Yufeng Shen","Wei Ji","Juan Cao","Tat-Seng Chua"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)"],"id":"P1047","is_paper":true,"keywords":["misinformation detection and analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.756.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77401/poster_document/83336aa8ad7777df39d6eb784b2bff68.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Two Heads Are Better Than One: Improving Fake News Video Detection by Correlating with Neighbors","tldr":"The prevalence of short video platforms has spawned a lot of fake news videos, which have stronger propagation ability than textual fake news. Thus, automatically detecting fake news videos has been an important countermeasure in practice. Previous works commonly verify each news video individually ...","track":"Computational Social Science and Cultural Analytics","underline_id":77401,"underline_url":"https://underline.io/events/395/posters/15279/poster/77401-two-heads-are-better-than-one-improving-fake-news-video-detection-by-correlating-with-neighbors","video_url":null},{"abstract":"Annotating long-document question answering (long-document QA) pairs is time-consuming and expensive. To alleviate the problem, it might be possible to generate long-document QA pairs via unsupervised question answering (UQA) methods. However, existing UQA tasks are based on short documents, and can hardly incorporate long-range information. To tackle the problem, we propose a new task, named unsupervised long-document question answering (ULQA), aiming to generate high-quality long-document QA instances in an unsupervised manner. Besides, we propose AttenWalker, a novel unsupervised method to aggregate and generate answers with long-range dependency so as to construct long-document QA pairs. Specifically, AttenWalker is composed of three modules, i.e. span collector, span linker and answer aggregator. Firstly, the span collector takes advantage of constituent parsing and reconstruction loss to select informative candidate spans for constructing answers. Secondly, with the help of the attention graph of a pre-trained long-document model, potentially interrelated text spans (that might be far apart) could be linked together via an attention-walking algorithm. Thirdly, in the answer aggregator, linked spans are aggregated into the final answer via the mask-filling ability of a pre-trained model. Extensive experiments show that AttenWalker outperforms previous methods on NarrativeQA and Qasper. In addition, AttenWalker also shows strong performance in the few-shot learning setting.","anthology_url":"https://aclanthology.org/2023.findings-acl.862","authors":["Yuxiang Nie","Heyan Huang","Wei Wei","Xian-Ling Mao"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-7_-question-answering-(virtual-poster)"],"id":"P1056","is_paper":true,"keywords":["few-shot qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.862.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77402/poster_document/aa711b498244436ae10cfd9c4eb83007.pdf","preview_image":"https://assets.underline.io/lecture/77402/poster/8d5cbc773b46c97f5a1e5ed35e93dea4.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"AttenWalker: Unsupervised Long-Document Question Answering via Attention-based Graph Walking","tldr":"Annotating long-document question answering (long-document QA) pairs is time-consuming and expensive. To alleviate the problem, it might be possible to generate long-document QA pairs via unsupervised question answering (UQA) methods. However, existing UQA tasks are based on short documents, and can...","track":"Question Answering","underline_id":77402,"underline_url":"https://underline.io/events/395/posters/15279/poster/77402-attenwalker-unsupervised-long-document-question-answering-via-attention-based-graph-walking","video_url":null},{"abstract":"Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (\\emph{i.e.,} textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of contribution to emotion recognition. More seriously, there might be inconsistencies between the emotion of individual modality and the video. The challenges mentioned above are caused by the inherent uncertainty of emotion. Inspired by the recent advances of quantum theory in modeling uncertainty, we make an initial attempt to design a quantum-inspired adaptive-priority-learning model (QAP) to address the challenges. Specifically, the quantum state is introduced to model modal features, which allows each modality to retain all emotional tendencies until the final classification. Additionally, we design Q-attention to orderly integrate three modalities, and then QAP learns modal priority adaptively so that modalities can provide different amounts of information based on priority. Experimental results on the IEMOCAP and MOSEI datasets show that QAP establishes new state-of-the-art results.","anthology_url":"https://aclanthology.org/2023.findings-acl.772","authors":["Ziming Li","Yan Zhou","Yaxin Liu","Fuqing Zhu","Chuanpeng Yang","Songlin Hu"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)"],"id":"P106","is_paper":true,"keywords":["style analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.772.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77264/poster_document/b06b9473ecfc45646630814c29ec80e1.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"QAP: A Quantum-Inspired Adaptive-Priority-Learning Model for Multimodal Emotion Recognition","tldr":"Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (\\emph{i.e.,} textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77264,"underline_url":"https://underline.io/events/395/posters/15240/poster/77264-qap-a-quantum-inspired-adaptive-priority-learning-model-for-multimodal-emotion-recognition","video_url":null},{"abstract":"Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models.\nIn this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. \nThis is in contrast to the common belief that LayerNorm's only role is to normalize the activations during the forward pass, and their gradients during the backward pass.\n\nWe consider a geometric interpretation of LayerNorm and show that it consists of two components: \n(a) projection of the input vectors to a d-1 space that is orthogonal to the [1,1,...,1] vector, and\n(b) scaling of all vectors to the same norm of \\sqrt{d}. \nWe show that each of these components is important for the attention layer that follows it in Transformers:\n(a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation in the attention; and\n(b) scaling allows each key to potentially receive the highest attention, and prevents keys from being \"un-select-able''.\nWe show empirically that Transformers do indeed benefit from these properties of LayeNorm in general language modeling and even in computing simple functions such as \"majority''. Our code is available at https://github.com/tech-srl/layer\\_norm\\_expressivity\\_role .","anthology_url":"https://aclanthology.org/2023.findings-acl.895","authors":["Shaked Brody","Uri Alon","Eran Yahav"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1065","is_paper":true,"keywords":["generalization"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.895.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77407/poster_document/3ef6abb27f8ce5bfd562135f7a368179.pdf","preview_image":"https://assets.underline.io/lecture/77407/poster/9799352a4185fdf9a5c6dfdd13bc6100.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77407/slideshow/0f3cbe59439c48490dbdcd3c2509605d.pdf","title":"On the Expressivity Role of LayerNorm in Transformers' Attention","tldr":"Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models.\nIn this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. \nThis is in contrast to the common belief that LayerNorm's only role is to normalize the a...","track":"Machine Learning for NLP","underline_id":77407,"underline_url":"https://underline.io/events/395/posters/15279/poster/77407-on-the-expressivity-role-of-layernorm-in-transformers-attention","video_url":null},{"abstract":"The open-ended Visual Question Answering (VQA) task requires AI models to jointly reason over visual and natural language inputs using world knowledge. Recently, pre-trained Language Models (PLM) such as GPT-3 have been applied to the task and shown to be powerful world knowledge sources. However, these methods suffer from low knowledge coverage caused by PLM bias -- the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality -- only models using GPT-3 can achieve the best result. \n\nTo address the aforementioned challenges, we propose RASO: a new VQA pipeline that deploys a generate-then-select strategy guided by world knowledge for the first time. Rather than following the de facto standard to train a multi-modal model that directly generates the VQA answer, \\{pasted macro `MODEL'\\}name first adopts PLM to generate all the possible answers, and then trains a lightweight answer selection model for the correct answer. As proved in our analysis, RASO expands the knowledge coverage from in-domain training data by a large margin. We provide extensive experimentation and show the effectiveness of our pipeline by advancing the state-of-the-art by 4.1\\% on OK-VQA, without additional computation cost.","anthology_url":"https://aclanthology.org/2023.findings-acl.147","authors":["Xingyu Fu","Sheng Zhang","Gukyeong Kwon","Pramuditha Perera","Henghui Zhu","Yuhao Zhang","Alexander Hanbo Li","William Yang Wang","Zhiguo Wang","Vittorio Castelli","Patrick Ng","Dan Roth","Bing Xiang"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1067","is_paper":true,"keywords":["vision question answering"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.147.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77408/poster_document/3621ca1f66a6bf2b6f670d9eb15e1de6.pdf","preview_image":"https://assets.underline.io/lecture/77408/poster/89e46fab33a9e70e7dd917cb915df391.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77408/slideshow/f62dff9c63fbea88821e759ac1d5058a.pdf","title":"Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge","tldr":"The open-ended Visual Question Answering (VQA) task requires AI models to jointly reason over visual and natural language inputs using world knowledge. Recently, pre-trained Language Models (PLM) such as GPT-3 have been applied to the task and shown to be powerful world knowledge sources. However, t...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77408,"underline_url":"https://underline.io/events/395/posters/15240/poster/77408-indicmt-eval-a-dataset-to-meta-evaluate-machine-translation-metrics-for-indian-languages","video_url":null},{"abstract":"Few-shot cross-lingual transfer, fine-tuning Multilingual Masked Language Model (MMLM) with source language labeled data and a small amount of target language labeled data, provides excellent  performance in the target language.\nHowever, if no labeled data in the target language are available, they need to be created through human annotations.\nIn this study, we devise a metric to select annotation candidates from an unlabeled data pool that efficiently enhance accuracy for few-shot cross-lingual transfer.\nIt is known that training a model with hard examples is important to improve the model's performance.\nTherefore, we first identify examples that MMLM cannot solve in a zero-shot cross-lingual transfer setting and demonstrate that it is hard to predict peculiar examples in the target language, i.e., the examples distant from the source language examples in cross-lingual semantic space of the MMLM.\nWe then choose high peculiarity examples as annotation candidates and perform few-shot cross-lingual transfer.\nIn comprehensive experiments with 20 languages and 6 tasks, we demonstrate that the high peculiarity examples improve the target language accuracy compared to other candidate selection methods proposed in previous studies.","anthology_url":"https://aclanthology.org/2023.findings-acl.47","authors":["Hwichan Kim","Mamoru Komachi"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)"],"id":"P1075","is_paper":true,"keywords":["cross-lingual transfer"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.47.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77409/poster_document/85f1d6316ddb7031a780dcc84f43203a.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Enhancing Few-shot Cross-lingual Transfer with Target Language Peculiar Examples","tldr":"Few-shot cross-lingual transfer, fine-tuning Multilingual Masked Language Model (MMLM) with source language labeled data and a small amount of target language labeled data, provides excellent  performance in the target language.\nHowever, if no labeled data in the target language are available, they ...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77409,"underline_url":"https://underline.io/events/395/posters/15279/poster/77409-enhancing-few-shot-cross-lingual-transfer-with-target-language-peculiar-examples","video_url":null},{"abstract":"Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training. In pilot studies, we find that after continual pre-training, the upgraded PLM remains compatible with the outdated adapted weights to some extent. Motivated by this finding, we analyze the connection between continually pre-trained PLMs from two novel aspects, i.e., mode connectivity, and functional similarity. Based on the corresponding findings, we propose both an initialization-based method and a distillation-based method for our task. We demonstrate their feasibility in improving the convergence and performance for tuning the upgraded PLM. We also show that both methods can be combined to achieve better performance.","anthology_url":"https://aclanthology.org/2023.findings-acl.723","authors":["Yujia Qin","Cheng Qian","Xu Han","Yankai Lin","Huadong Wang","Ruobing Xie","Zhiyuan Liu","Maosong Sun","Jie Zhou"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-1_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1084","is_paper":true,"keywords":["continual learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.723.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77410/poster_document/c4ef5eee9fb2d123e0b0105d0a38d537.pdf","preview_image":"https://assets.underline.io/lecture/77410/poster/45d3e720762aba034738de1bdcdf91cc.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77410/slideshow/19fcf4201201873ced30ea7cff722f5d.pdf","title":"Recyclable Tuning for Continual Pre-training","tldr":"Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tun...","track":"Large Language Models","underline_id":77410,"underline_url":"https://underline.io/events/395/posters/15200/poster/77410-recyclable-tuning-for-continual-pre-training","video_url":null},{"abstract":"Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. \"supervised pre-training\") showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model's capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tuning on relatively small PLMs. Extensive experiments have demonstrated the effectiveness and generality of our MVP model in a number of NLG tasks, which achieves state-of-the-art performance on 13 out of 17 datasets, outperforming BART by 9.3\\% and Flan-T5 by 5.8\\%.","anthology_url":"https://aclanthology.org/2023.findings-acl.558","authors":["Tianyi Tang","Junyi Li","Wayne Xin Zhao","Ji-Rong Wen"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-7_-generation-(virtual-poster)"],"id":"P110","is_paper":true,"keywords":["text-to-text generation","model architectures"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.558.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77265/poster_document/8f8cff628a409b202c7ea9163c3b1ec9.pdf","preview_image":"https://assets.underline.io/lecture/77265/poster/f1e604f45bac1df589b05c9a133e0dad.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77265/slideshow/846c31c843142d32746e7deee54b0d7b.pdf","title":"MVP: Multi-task Supervised Pre-training for Natural Language Generation","tldr":"Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled ...","track":"Generation","underline_id":77265,"underline_url":"https://underline.io/events/395/posters/15279/poster/77265-mvp-multi-task-supervised-pre-training-for-natural-language-generation","video_url":null},{"abstract":"Over the last few years, Masked Language Modeling (MLM) pre-training has resulted in remarkable advancements in many Natural Language Understanding (NLU) tasks, which sparked an interest in researching alternatives and extensions to the MLM objective. In this paper, we tackle the absence of explicit semantic grounding in MLM and propose Descriptive Masked Language Modeling (DMLM), a knowledge-enhanced reading comprehension objective, where the model is required to predict the most likely word in a context, being provided with the word's definition. For instance, given the sentence \"I was going to the \\_'', if we provided as definition \"financial institution'', the model would have to predict the word \"bank''; if, instead, we provided \"sandy seashore'', the model should predict \"beach''. Our evaluation highlights the effectiveness of DMLM in comparison with standard MLM, showing improvements on a number of well-established NLU benchmarks, as well as other semantics-focused tasks, e.g., Semantic Role Labeling. Furthermore, we demonstrate how it is possible to take full advantage of DMLM to embed explicit semantics in downstream tasks, explore several properties of DMLM-based contextual representations and suggest a number of future directions to investigate.","anthology_url":"https://aclanthology.org/2023.findings-acl.808","authors":["Edoardo Barba","Niccol\u00f2 Campolungo","Roberto Navigli"],"category":"Findings","demo_url":null,"display_track":"Semantics: Lexical","event_ids":["session-7_-semantics_-lexical-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1142","is_paper":true,"keywords":["polysemy","word embeddings"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.808.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77420/poster_document/e8c5693df2d730de094fe1a2dce532d3.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77420/slideshow/4f4c7f120fd743abaa0fc952e81cc7d5.pdf","title":"DMLM: Descriptive Masked Language Modeling","tldr":"Over the last few years, Masked Language Modeling (MLM) pre-training has resulted in remarkable advancements in many Natural Language Understanding (NLU) tasks, which sparked an interest in researching alternatives and extensions to the MLM objective. In this paper, we tackle the absence of explicit...","track":"Semantics: Lexical","underline_id":77420,"underline_url":"https://underline.io/events/395/posters/15279/poster/77420-dmlm-descriptive-masked-language-modeling","video_url":null},{"abstract":"Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a data augmentation framework using knowledge distillation. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to augment training data, which is then used for the next step. We incorporate this framework into the state-of-the-art  language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs' performance in sequence-generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.","anthology_url":"https://aclanthology.org/2023.findings-acl.823","authors":["Hung Quoc To","Nghi D. Q. Bui","Jin L.C. Guo","Tien N Nguyen"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)"],"id":"P1154","is_paper":true,"keywords":["code generation and understanding"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.823.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77421/poster_document/5150c4a9928ac4789de14deb7a705d6e.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77421/slideshow/870a81a1ef30c1118779e2b2a96fef1d.pdf","title":"Better Language Models of Code through Self-Improvement","tldr":"Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve thi...","track":"NLP Applications","underline_id":77421,"underline_url":"https://underline.io/events/395/posters/15279/poster/77421-better-language-models-of-code-through-self-improvement","video_url":null},{"abstract":"Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide future designs of sentence representation learning methods by taking a closer look at contrastive SRL through the lens of isotropy, contextualization and learning dynamics. We interpret its successes through the geometry of the representation shifts and show that contrastive learning brings isotropy, and drives high intra-sentence similarity: when in the same sentence, tokens converge to similar positions in the semantic space. We also find that what we formalize as \"spurious contextualization\" is mitigated for semantically meaningful tokens, while augmented for functional ones. We find that the embedding space is directed towards the origin during training, with more areas now better defined. We ablate these findings by observing the learning dynamics with different training temperatures, batch sizes and pooling methods.","anthology_url":"https://aclanthology.org/2023.findings-acl.778","authors":["Chenghao Xiao","Yang Long","Noura Al Moubayed"],"category":"Findings","demo_url":null,"display_track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","event_ids":["session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1168","is_paper":true,"keywords":["phrase/sentence embedding"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.778.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77423/poster_document/f6bb8ffec07c33905e87e4e94f35d4dc.pdf","preview_image":"https://assets.underline.io/lecture/77423/poster/669860ea77b6569293703d7039f32cc8.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77423/slideshow/7ead86acd03db3d2258da8177c0967ce.pdf","title":"On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning","tldr":"Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide ...","track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","underline_id":77423,"underline_url":"https://underline.io/events/395/posters/15200/poster/77423-on-isotropy-contextualization-and-learning-dynamics-of-contrastive-based-sentence-representation-learning","video_url":null},{"abstract":"In this paper, we aim to adapt the idea of retrieval-based neural approaches \nto the Aspect Sentiment Triplet Extraction (ASTE) task. Different from previous studies retrieving semantic similar neighbors, the ASTE task has its specialized challenges when adapting, i.e., the purpose includes predicting the sentiment polarity and it is usually aspect-dependent. Semantic similar neighbors with different polarities will be infeasible even counterproductive. To tackle this issue, we propose a  retrieval-based neural ASTE approach, named RLI (Retrieval-based Aspect Sentiment Triplet Extraction via Label Interpolation), which exploits the label information of neighbors. Given an aspect-opinion term pair, we retrieve semantic similar triplets from the training corpus and interpolate their label information into the augmented representation of the target pair. The retriever is jointly trained with the whole ASTE framework, and neighbors with both similar semantics and sentiments can be recalled with the aid of this distant supervision. In addition, we design a simple yet effective pre-train method for the retriever that implicitly encodes the label similarities. Extensive experiments and analysis on two widely-used benchmarks show that the proposed model establishes a new state-of-the-art on ASTE.","anthology_url":"https://aclanthology.org/2023.findings-acl.303","authors":["Guoxin Yu","Lemao Liu","Haiyun Jiang","Shuming Shi","Xiang Ao"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)"],"id":"P1190","is_paper":true,"keywords":["argument mining"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.303.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77425/poster_document/5269d25a78796e1773162e7a71291efd.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77425/slideshow/232ffb460c6c68d071825b2aa4fa02ee.pdf","title":"Making Better Use of Training Corpus: Retrieval-based Aspect Sentiment Triplet Extraction via Label Interpolation","tldr":"In this paper, we aim to adapt the idea of retrieval-based neural approaches \nto the Aspect Sentiment Triplet Extraction (ASTE) task. Different from previous studies retrieving semantic similar neighbors, the ASTE task has its specialized challenges when adapting, i.e., the purpose includes predicti...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77425,"underline_url":"https://underline.io/events/395/posters/15240/poster/77425-making-better-use-of-training-corpus-retrieval-based-aspect-sentiment-triplet-extraction-via-label-interpolation","video_url":null},{"abstract":"Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can proactively lead the conversation based on some key topics of the news. In addition, both information-seeking and chit-chat scenarios are included realistically, where the user may ask a series of questions about the news details or express their opinions and be eager to chat. To further develop this novel task, we collect a human-to-human Chinese dialogue dataset NewsDialogues, which includes 1K conversations with a total of 14.6K utterances and detailed annotations for target topics and knowledge spans. Furthermore, we propose a method named Predict-Generate-Rank, consisting of a \ngenerator for grounded knowledge prediction and response generation, and a ranker for the ranking of multiple responses to alleviate the exposure bias. We conduct comprehensive experiments to demonstrate the effectiveness of the proposed method and further present several key findings and challenges to prompt future research.","anthology_url":"https://aclanthology.org/2023.findings-acl.224","authors":["Siheng Li","Yichun Yin","Cheng Yang","Wangjie Jiang","Yiwei Li","Zesen Cheng","Lifeng Shang","Xin Jiang","Qun Liu","Yujiu Yang"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P1212","is_paper":true,"keywords":["knowledge augmented","grounded dialog"],"languages":["chinese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.224.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77427/poster_document/9fea4b8ade0af790b73a9aef85d00827.pdf","preview_image":"https://assets.underline.io/lecture/77427/poster/5cafdc4a460d917645862872d3de2053.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77427/slideshow/97bc4967ee30e1276e1a874fe5c1ef79.pdf","title":"NewsDialogues: Towards Proactive News Grounded Conversation","tldr":"Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can proa...","track":"Dialogue and Interactive Systems","underline_id":77427,"underline_url":"https://underline.io/events/395/posters/15200/poster/77427-newsdialogues-towards-proactive-news-grounded-conversation","video_url":null},{"abstract":"Out-of-Domain (OOD) Intent Classification and New Intent Discovering are two basic and critical tasks in the Task-Oriented Dialogue System, which are typically treated two independent tasks. Classification focuses on identifying intents beyond the predefined set of the dialog system, but it will not further differentiate detected OOD intents in fine granularity. Discovering focuses on how to cluster unlabeled samples according to their semantic representation, which relies heavily on prior knowledge and can not provide label information for the formed clusters. To be closer to the real user-facing scenarios, we introduce a task paradigm to extend Classification with Discovering referred as Open Environment Intent Prediction, which is to make a further fine-grained discovery of OOD based on OOD Intent Classification. Using various widely-used generative models as an archetype, we propose a general scheme for Open Environment Intent Prediction. In a nutshell, we first perform intent detection to identify the In-domain (IND) samples and then generate labels for those identified as OOD. With these generated labels, we can discover new general intents and provide label information for them. We develop a suite of benchmarks on the existing intent datasets and present a simple yet effective implementation. Extensive experiments demonstrate that our method establishes substantial improvement compared to the baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.140","authors":["Yunhua Zhou","Jiawei Hong","Xipeng Qiu"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P1223","is_paper":true,"keywords":["task-oriented","factuality","applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.140.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77430/poster_document/de0b841c92acb5d4070fba18e6a34be2.pdf","preview_image":"https://assets.underline.io/lecture/77430/poster/82e52b17611665291c4b1c44ba0ef351.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77430/slideshow/4c8d7593a50df62ba7c88f5666939e67.pptx","title":"Towards Open Environment Intent Prediction","tldr":"Out-of-Domain (OOD) Intent Classification and New Intent Discovering are two basic and critical tasks in the Task-Oriented Dialogue System, which are typically treated two independent tasks. Classification focuses on identifying intents beyond the predefined set of the dialog system, but it will not...","track":"Dialogue and Interactive Systems","underline_id":77430,"underline_url":"https://underline.io/events/395/posters/15200/poster/77430-towards-open-environment-intent-prediction","video_url":null},{"abstract":"To date, most work on text simplification has focused on sentence-level inputs. Early attempts at document simplification merely applied these approaches iteratively over the sentences of a document. However, this fails to coherently preserve the discourse structure, leading to suboptimal output quality. Recently, strategies from controllable simplification have been leveraged to achieve state-of-the-art results on document simplification by first generating a document-level plan (a sequence of sentence-level simplification operations) and using this plan to guide sentence-level simplification downstream. However, this is still limited in that the simplification model has no direct access to the local inter-sentence document context, likely having a negative impact on surface realisation. We explore various systems that use document context within the simplification process itself, either by iterating over larger text units or by extending the system architecture to attend over a high-level representation of document context. In doing so, we achieve state-of-the-art performance on the document simplification task, even when not relying on plan-guidance. Further, we investigate the performance and efficiency tradeoffs of system variants and make suggestions of when each should be preferred.","anthology_url":"https://aclanthology.org/2023.findings-acl.834","authors":["Liam Cripwell","Jo\u00ebl Legrand","Claire Gardent"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-4_-generation-(virtual-poster)"],"id":"P1228","is_paper":true,"keywords":["text-to-text generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.834.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77432/poster_document/7ae844174e21e7933b07ba16d04b5572.pdf","preview_image":"https://assets.underline.io/lecture/77432/poster/d9b55b964cc44bc7a810bd19f5ddf49d.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Context-Aware Document Simplification","tldr":"To date, most work on text simplification has focused on sentence-level inputs. Early attempts at document simplification merely applied these approaches iteratively over the sentences of a document. However, this fails to coherently preserve the discourse structure, leading to suboptimal output qua...","track":"Generation","underline_id":77432,"underline_url":"https://underline.io/events/395/posters/15240/poster/77432-a-synthetic-data-generation-framework-for-grounded-dialogues","video_url":null},{"abstract":"The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate a compact embedding without requiring any changes in its architecture or training procedure. Specifically, we use a correlation penalty added to the weighted reconstruction loss that better captures the informative features in the text embeddings, which improves the efficiency of the language models. We evaluated EmbedTextNet on three different downstream tasks: text similarity, language modelling, and text retrieval. Empirical results on diverse benchmark datasets demonstrate the effectiveness and superiority of EmbedTextNet compared to state-of-art methodologies in recent works, especially in extremely low dimensional embedding sizes. The developed code for reproducibility is included in the supplementary material.","anthology_url":"https://aclanthology.org/2023.findings-acl.625","authors":["Dae Yon Hwang","Bilal Taha","Yaroslav Nechaev"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1258","is_paper":true,"keywords":["representation learning","model compression methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.625.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77434/poster_document/280fbeb68b32fcf5c1475fea7712157c.pdf","preview_image":"https://assets.underline.io/lecture/77434/poster/968641458a34e92b5096e7593aefb875.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77434/slideshow/606333221cf40a7592eb718693d11af0.pdf","title":"EmbedTextNet: Dimension Reduction with Weighted Reconstruction and Correlation Losses for Efficient Text Embedding","tldr":"The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate ...","track":"Machine Learning for NLP","underline_id":77434,"underline_url":"https://underline.io/events/395/posters/15200/poster/77434-propsegment-a-large-scale-corpus-for-proposition-level-segmentation-and-entailment-recognition","video_url":null},{"abstract":"Existing supervised sign language recognition systems rely on an abundance of well-annotated data. Instead, an unsupervised speech-to-sign language recognition (SSR-U) system learns to translate between spoken and sign languages by observing only non-parallel speech and sign-language corpora. We propose speech2sign-U, a neural network-based approach capable of both character-level and word-level SSR-U. Our approach significantly outperforms baselines directly adapted from unsupervised speech recognition (ASR-U) models by as much as 50\\% recall@10 on several challenging American sign language corpora with various levels of sample sizes, vocabulary sizes, and audio and visual variability. The code is available at {https://github.com/cactuswiththoughts/UnsupSpeech2Sign.git}{cactuswiththoughts/UnsupSpeech2Sign.git}.","anthology_url":"https://aclanthology.org/2023.findings-acl.424","authors":["Liming Wang","Junrui Ni","Heting Gao","Jialu Li","Kai Chieh Chang","Xulin Fan","Junkai Wu","Mark Hasegawa-Johnson","Chang D. Yoo"],"category":"Findings","demo_url":null,"display_track":"Speech and Multimodality","event_ids":["session-7_-speech-and-multimodality-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1270","is_paper":true,"keywords":["speech and vision","speech technologies","multimodality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.424.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77437/poster_document/9168bfd85dae94702d6477fdc9b56e46.pdf","preview_image":"https://assets.underline.io/lecture/77437/poster/82af5b6eb511616c4796e49c036b5f63.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77437/slideshow/65b1ce823a196ccd0ff0c25fe2d5598d.pdf","title":"Listen, Decipher and Sign: Toward Unsupervised Speech-to-Sign Language Recognition","tldr":"Existing supervised sign language recognition systems rely on an abundance of well-annotated data. Instead, an unsupervised speech-to-sign language recognition (SSR-U) system learns to translate between spoken and sign languages by observing only non-parallel speech and sign-language corpora. We pro...","track":"Speech and Multimodality","underline_id":77437,"underline_url":"https://underline.io/events/395/posters/15279/poster/77437-listen-decipher-and-sign-toward-unsupervised-speech-to-sign-language-recognition","video_url":null},{"abstract":"Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-sequence textual (NarrativeQA) and video (ActivityNet-QA) question answering datasets, where it achieves substantial improvements over previous memory network approaches. Furthermore, our ablation study confirms the proposed mechanisms' importance for memory models.","anthology_url":"https://aclanthology.org/2023.findings-acl.830","authors":["Vladimir Araujo","Alvaro M Soto","Marie-Francine Moens"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1277","is_paper":true,"keywords":["self-supervised learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.830.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77439/slideshow/03d1e72457b0a1defff3fec76bf302d5.pdf","title":"A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information","tldr":"Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity me...","track":"Machine Learning for NLP","underline_id":77439,"underline_url":"https://underline.io/events/395/posters/15200/poster/77439-a-memory-model-for-question-answering-from-streaming-data-supported-by-rehearsal-and-anticipation-of-coreference-information","video_url":null},{"abstract":"Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming.\nTypical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.","anthology_url":"https://aclanthology.org/2023.findings-acl.19","authors":["Antonio Valerio Miceli Barone","Fazl Barez","Shay B. Cohen","Ioannis Konstas"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-4_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1278","is_paper":true,"keywords":["scaling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.19.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77440/poster_document/4b40c88c89c8f17b7a85b113c28aaf2a.pdf","preview_image":"https://assets.underline.io/lecture/77440/poster/4a094764843a5b2507d29aa838ea5d24.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77440/slideshow/91600b73a216de457928d5883999fd68.pdf","title":"The Larger they are, the Harder they Fail: Language Models do not Recognize Identifier Swaps in Python","tldr":"Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming.\nTypical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, s...","track":"Large Language Models","underline_id":77440,"underline_url":"https://underline.io/events/395/posters/15240/poster/77440-the-larger-they-are-the-harder-they-fail-language-models-do-not-recognize-identifier-swaps-in-python","video_url":null},{"abstract":"Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two types of disagreement: 1) how tasks are conceptualized and 2) how measurements of model performance are operationalized. To provide evidence for our taxonomy, we conduct a meta-analysis of relevant literature to understand how NLP tasks are conceptualized, as well as a survey of practitioners about their impressions of different factors that affect benchmark validity. Our meta-analysis and survey across eight tasks, ranging from coreference resolution to question answering, uncover that tasks are generally not clearly and consistently conceptualized and benchmarks suffer from operationalization disagreements. These findings support our proposed taxonomy of disagreement. Finally, based on our taxonomy, we present a framework for constructing benchmarks and documenting their limitations.","anthology_url":"https://aclanthology.org/2023.findings-acl.202","authors":["Arjun Subramonian","Xingdi Yuan","Hal Daum\u00e9 III","Su Lin Blodgett"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-4_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1291","is_paper":true,"keywords":["evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.202.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77441/poster_document/e497a137f96f0a7ccf2ce9afb9e80eca.pdf","preview_image":"https://assets.underline.io/lecture/77441/poster/ea097c0389640d7c983d1c91289c1da1.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77441/slideshow/47f6a315f8ad000b454128218ba95835.pdf","title":"It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance","tldr":"Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two...","track":"Theme: Reality Check","underline_id":77441,"underline_url":"https://underline.io/events/395/posters/15240/poster/77441-it-takes-two-to-tango-navigating-conceptualizations-of-nlp-tasks-and-measurements-of-performance","video_url":null},{"abstract":"Supervised models based on Transformers have been shown to achieve impressive performances in many natural language processing tasks. However, besides requiring a large amount of costly manually annotated data, supervised models tend to adapt to the characteristics of the training dataset, which are usually created ad-hoc and whose data distribution often differs from the one in real applications, showing significant performance degradation in real-world scenarios. We perform an extensive assessment of the out-of-distribution performances of supervised models for classification in the emotion and hate-speech detection tasks and show that NLI-based zero-shot models often outperform them, making task-specific annotation useless when the characteristics of final-user data are not known in advance. To benefit from both supervised and zero-shot approaches, we propose to fine-tune an NLI-based model on the task-specific dataset. The resulting model often outperforms all available supervised models both in distribution and out of distribution, with only a few thousand training samples.","anthology_url":"https://aclanthology.org/2023.findings-acl.524","authors":["Luana Bulla","Aldo Gangemi","Misael Mongiovi'"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)"],"id":"P1297","is_paper":true,"keywords":["transfer learning / domain adaptation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.524.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77442/poster_document/911c65a1155a191cf87fd1680c412881.pdf","preview_image":"https://assets.underline.io/lecture/77442/poster/071d52858cbee9a3eedf0b6a823aeb7b.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77442/slideshow/5ae5ff02cce16e6d98d1a1c8438a3239.pdf","title":"Towards Distribution-shift Robust Text Classification of Emotional Content","tldr":"Supervised models based on Transformers have been shown to achieve impressive performances in many natural language processing tasks. However, besides requiring a large amount of costly manually annotated data, supervised models tend to adapt to the characteristics of the training dataset, which are...","track":"Machine Learning for NLP","underline_id":77442,"underline_url":"https://underline.io/events/395/posters/15200/poster/77442-leveraging-explicit-procedural-instructions-for-data-efficient-action-prediction","video_url":null},{"abstract":"Task-oriented dialogue research has mainly focused on a few popular languages like English and Chinese, due to the high dataset creation cost for a new language. \nTo reduce the cost, we apply manual editing to automatically translated data. We create a new multilingual benchmark, X-RiSAWOZ, by translating the Chinese RiSAWOZ to 4 languages: English, French, Hindi, Korean; and a code-mixed English-Hindi language.\nX-RiSAWOZ has more than 18,000 human-verified dialogue utterances for each language, and unlike most multilingual prior work, is an end-to-end dataset for building fully-functioning agents. \n\nThe many difficulties we encountered in creating X-RiSAWOZ led us to develop a toolset to accelerate the post-editing of a new language dataset after translation. This toolset improves machine translation with a hybrid entity alignment technique that combines neural with dictionary-based methods, along with many automated and semi-automated validation checks. \n\nWe establish strong baselines for X-RiSAWOZ by training dialogue agents in the zero- and few-shot settings where limited gold data is available in the target language. Our results suggest that our translation and post-editing methodology and toolset can be used to create new high-quality multilingual dialogue agents cost-effectively. Our dataset, code, and toolkit are released open-source.","anthology_url":"https://aclanthology.org/2023.findings-acl.174","authors":["Mehrad Moradshahi","Tianhao Shen","Kalika Bali","Monojit Choudhury","Gael de Chalendar","Anmol Goel","Sungkyun Kim","Prashant Kodali","Ponnurangam Kumaraguru","Nasredine Semmar","Sina Semnani","Jiwon Seo","Vivek Seshadri","Manish Shrivastava","Michael Sun","Aditya Yadavalli","Chaobin You","Deyi Xiong","Monica S Lam"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1303","is_paper":true,"keywords":["code-switching","mixed language","multilingualism","cross-lingual transfer","multilingual pre-training","multilingual benchmarks","multilingual evaluation"],"languages":["chinese","french","hindi"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.174.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77443/poster_document/55ed3b6827fba1d72753d135a0276776.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"X-RiSAWOZ: High-Quality End-to-End Multilingual Dialogue Datasets and Few-shot Agents","tldr":"Task-oriented dialogue research has mainly focused on a few popular languages like English and Chinese, due to the high dataset creation cost for a new language. \nTo reduce the cost, we apply manual editing to automatically translated data. We create a new multilingual benchmark, X-RiSAWOZ, by trans...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77443,"underline_url":"https://underline.io/events/395/posters/15240/poster/77443-x-risawoz-high-quality-end-to-end-multilingual-dialogue-datasets-and-few-shot-agents","video_url":null},{"abstract":"We have investigated methods utilizing hierarchical structure information representation in the semantic parsing task and have devised a method that reinforces the semantic awareness of a pre-trained language model via a two-step fine-tuning mechanism:   hierarchical structure information strengthening and a final specific task. \nThe model used is better than existing ones at learning the contextual representations of utterances embedded within its hierarchical semantic structure and thereby improves system performance. In addition, we created a mechanism using inductive grammar to dynamically prune the unpromising directions in the semantic structure parsing process. \nFinally, through experiments{Our code will be published when this paper is accepted.} on the TOP and TOPv2 (low-resource setting) datasets, we achieved state-of-the-art (SOTA)  performance, confirming the effectiveness of our proposed model.","anthology_url":"https://aclanthology.org/2023.findings-acl.648","authors":["Truong Dinh Do","Phuong Minh Nguyen","Minh Le Nguyen"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-4_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P1306","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.648.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77444/poster_document/ab2163b24bc4e66af4f92aa7bd60448b.pdf","preview_image":"https://assets.underline.io/lecture/77444/poster/370f94b94b4534d95c2ee3ab8be63a36.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77444/slideshow/dedc8ce522a80f3ad1ece1107df25bf5.pdf","title":"StructSP: Efficient Fine-tuning of Task-Oriented Dialog System by Using Structure-aware Boosting and Grammar Constraints","tldr":"We have investigated methods utilizing hierarchical structure information representation in the semantic parsing task and have devised a method that reinforces the semantic awareness of a pre-trained language model via a two-step fine-tuning mechanism:   hierarchical structure information strengthen...","track":"Dialogue and Interactive Systems","underline_id":77444,"underline_url":"https://underline.io/events/395/posters/15240/poster/77444-end-to-end-knowledge-retrieval-with-multi-modal-queries","video_url":null},{"abstract":"Is the output softmax layer, which is adopted by most language models (LMs), always the best way to compute the next word probability? Given so many attention layers in a modern transformer-based LM, are the pointer networks redundant nowadays? In this study, we discover that the answers to both questions are no. This is because the softmax bottleneck sometimes prevents the LMs from predicting the desired distribution and the pointer networks can be used to break the bottleneck efficiently. Based on the finding, we propose several softmax alternatives by simplifying the pointer networks and accelerating the word-by-word rerankers. In GPT-2, our proposals are significantly better and more efficient than mixture of softmax, a state-of-the-art softmax alternative. In summarization experiments, without very significantly decreasing its training/testing speed, our best method based on T5-Small improves factCC score by 2 points in CNN/DM and XSUM dataset, and improves MAUVE scores by 30\\% in BookSum paragraph-level dataset.","anthology_url":"https://aclanthology.org/2023.findings-acl.805","authors":["Haw-Shiuan Chang","Zonghai Yao","Alolika Gon","hong yu","Andrew McCallum"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-4_-generation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1312","is_paper":true,"keywords":["model architectures"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.805.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77445/poster_document/ca3c3eba1d2c3c8811f773babf0287f8.pdf","preview_image":"https://assets.underline.io/lecture/77445/poster/f693371637f01badd41e0752674cdafc.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77445/slideshow/7fdf0937ea83c77c8139d9d5e73ebee7.pdf","title":"Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond","tldr":"Is the output softmax layer, which is adopted by most language models (LMs), always the best way to compute the next word probability? Given so many attention layers in a modern transformer-based LM, are the pointer networks redundant nowadays? In this study, we discover that the answers to both que...","track":"Generation","underline_id":77445,"underline_url":"https://underline.io/events/395/posters/15240/poster/77445-revisiting-the-architectures-like-pointer-networks-to-efficiently-improve-the-next-word-distribution-summarization-factuality-and-beyond","video_url":null},{"abstract":"There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022).\nHowever, existing methods typically encode task information with a simple dataset name as a prefix to the encoder.\nThis not only limits the effectiveness of multi-task learning, but also hinders the model's ability to generalize to new domains or tasks that were not seen during training, which is crucial for real-world applications.\nIn this paper, we propose compositional task configurations, a set of prompts prepended to the encoder to improve cross-task generalization of unified models.\nWe design the task configurations to explicitly specify the task type, as well as its input and output types.\nWe show that this not only allows the model to better learn shared knowledge across different tasks at training, but also allows us to control the model by composing new configurations that apply novel input-output combinations in a zero-shot manner.\nWe demonstrate via experiments over ten table-to-text tasks that our method outperforms the UnifiedSKG baseline by noticeable margins in both in-domain and zero-shot settings, with average improvements of +0.5 and +12.6 from using a T5-large backbone, respectively.","anthology_url":"https://aclanthology.org/2023.findings-acl.341","authors":["Jifan Chen","Yuhao Zhang","Lan Liu","Rui Dong","Xinchi Chen","Patrick Ng","William Yang Wang","zhiheng huang"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1325","is_paper":true,"keywords":["generalization","few-shot qa","table qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.341.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77447/poster_document/73f9cf3df21fb941f56230f09fea5250.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77447/slideshow/af5fbd986d047381ab302bd95a1f6876.pdf","title":"Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations","tldr":"There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022).\nHowever, existing methods typically encode task information with a simple dataset name as a prefix to the encoder.\nThis not only limits the e...","track":"Question Answering","underline_id":77447,"underline_url":"https://underline.io/events/395/posters/15240/poster/77447-improving-cross-task-generalization-of-unified-table-to-text-models-with-compositional-task-configurations","video_url":null},{"abstract":"Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore,  these methods hurt the PLMs' performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks' datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.","anthology_url":"https://aclanthology.org/2023.findings-acl.336","authors":["Somayeh Ghanbarzadeh","Yan Huang","Hamid Palangi","Radames Saul Cruz Moreno","Hamed Khanpour"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)"],"id":"P1333","is_paper":true,"keywords":["model bias/unfairness mitigation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.336.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77448/poster_document/28e1264f034916d23782954078565334.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models","tldr":"Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore,  t...","track":"Ethics and NLP","underline_id":77448,"underline_url":"https://underline.io/events/395/posters/15279/poster/77448-gender-tuning-empowering-fine-tuning-for-debiasing-pre-trained-language-models","video_url":null},{"abstract":"Inquiry conversation is a common form of conversation that aims to complete the investigation (e.g., court hearing, medical consultation and police interrogation) during which a series of focus shifts occurs. While many models have been proposed to generate a smooth response to a given conversation history, neglecting the focus can limit performance in inquiry conversation where the order of the focuses plays there a key role. In this paper, we investigate the problem of response generation in inquiry conversation by taking the focus into consideration. We propose a novel Focus-aware Response Generation (FRG) method by jointly optimizing a multi-level encoder and a set of focal decoders to generate several candidate responses that correspond to different focuses. Additionally, a focus ranking module is proposed to predict the next focus and rank the candidate responses. Experiments on two orthogonal inquiry conversation datasets (judicial, medical domain) demonstrate that our method generates results significantly better in automatic metrics and human evaluation compared to the state-of-the-art approaches.","anthology_url":"https://aclanthology.org/2023.findings-acl.797","authors":["Yiquan Wu","Weiming Lu","Yating Zhang","Adam Jatowt","Jun Feng","Changlong Sun","Fei Wu","Kun Kuang"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-4_-generation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1342","is_paper":true,"keywords":["text-to-text generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.797.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77449/poster_document/c8f257b2a23af2b8ac4483198d3c2c14.pdf","preview_image":"https://assets.underline.io/lecture/77449/poster/0082d921a94079d8d39ee6585c839d15.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77449/slideshow/b7e60a6af37a984ff3b26ac3dd7b40f4.pdf","title":"Focus-aware Response Generation in Inquiry Conversation","tldr":"Inquiry conversation is a common form of conversation that aims to complete the investigation (e.g., court hearing, medical consultation and police interrogation) during which a series of focus shifts occurs. While many models have been proposed to generate a smooth response to a given conversation ...","track":"Generation","underline_id":77449,"underline_url":"https://underline.io/events/395/posters/15240/poster/77449-focus-aware-response-generation-in-inquiry-conversation","video_url":null},{"abstract":"Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method.\nBPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down.\nWe formalize BPE as a combinatorial optimization problem.\nVia submodular functions, we prove that the iterative greedy version is a 1/sigma*(1-e\\^(-sigma))-approximation of an optimal merge sequence, where sigma is the total backward curvature with respect to the optimal merge sequence.\nEmpirically the lower bound of the approximation is approx0.37.\n\nWe provide a faster implementation of BPE which improves the runtime complexity from O(NM) to O(N log M), where N is the sequence length and M is the merge count.\nFinally, we optimize the brute-force algorithm for optimal BPE using memoization.","anthology_url":"https://aclanthology.org/2023.findings-acl.38","authors":["Vil\u00e9m Zouhar","Clara Meister","Juan Luis Gastaldi","Li Du","Tim Vieira","Mrinmaya Sachan","Ryan Cotterell"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-4_-machine-translation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1348","is_paper":true,"keywords":["vocabulary learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.38.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77450/poster_document/08e73809b853942ce77257a52be27bbf.pdf","preview_image":"https://assets.underline.io/lecture/77450/poster/63c9bc7e69f245c449e5107f8539bede.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77450/slideshow/d04e0ce8204ec1af37cbd73160344b2d.pdf","title":"A Formal Perspective on Byte-Pair Encoding","tldr":"Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method.\nBPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down.\nWe formalize BPE a...","track":"Machine Translation","underline_id":77450,"underline_url":"https://underline.io/events/395/posters/15240/poster/77450-a-formal-perspective-on-byte-pair-encoding","video_url":null},{"abstract":"Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired distributions. However, many important distributions, such as personal preferences, are unquantified. In this work, we tackle the problem of generating text following arbitrary distributions (quantified and unquantified) by proposing NANO, a few-shot human-in-the-loop training algorithm that continuously learns from human feedback. NANO achieves state-of-the-art results on single topic/attribute as well as quantified distribution control compared to previous works. We also show that NANO is able to learn unquantified distributions, achieves personalization, and captures differences between different individuals' personal preferences with high sample efficiency.","anthology_url":"https://aclanthology.org/2023.findings-acl.758","authors":["Xiang Fan","Yiwei Lyu","Paul Pu Liang","Ruslan Salakhutdinov","Louis-Philippe Morency"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-7_-generation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1355","is_paper":true,"keywords":["few-shot generation","interactive and collaborative generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.758.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77451/poster_document/0b40e9723cb7b18696cad9e27f235936.pdf","preview_image":"https://assets.underline.io/lecture/77451/poster/d54db28460aa04ea65c992b4913560be.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77451/slideshow/b1dd32cd7e8c8d9c6aa965cf20237708.pdf","title":"Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control","tldr":"Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the dis...","track":"Generation","underline_id":77451,"underline_url":"https://underline.io/events/395/posters/15279/poster/77451-nano-nested-human-in-the-loop-reward-learning-for-few-shot-language-model-control","video_url":null},{"abstract":"Recent developments of dense retrieval rely on quality representations of queries and contexts from pre-trained query and context encoders. In this paper, we introduce TOUR (Test-Time Optimization of Query Representations), which further optimizes instance-level query representations guided by signals from test-time retrieval results. We leverage a cross-encoder re-ranker to provide fine-grained pseudo labels over retrieval results and iteratively optimize query representations with gradient descent. Our theoretical analysis reveals that TOUR can be viewed as a generalization of the classical Rocchio algorithm for pseudo relevance feedback, and we present two variants that leverage pseudo-labels as hard binary or soft continuous labels. We first apply TOUR on phrase retrieval with our proposed phrase re-ranker, and also evaluate its effectiveness on passage retrieval with an off-the-shelf re-ranker. TOUR greatly improves end-to-end open-domain question answering accuracy, as well as passage retrieval performance. TOUR also consistently improves direct re-ranking by up to 2.0\\% while running 1.3\u20132.4x faster with an efficient implementation.","anthology_url":"https://aclanthology.org/2023.findings-acl.354","authors":["Mujeen Sung","Jungsoo Park","Jaewoo Kang","Danqi Chen","Jinhyuk Lee"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1362","is_paper":true,"keywords":["open-domain qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.354.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77452/poster_document/ece65ef96f4330317ede438c04668a26.pdf","preview_image":"https://assets.underline.io/lecture/77452/poster/e4a1905ddc987330bd67d6943323d770.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Optimizing Test-Time Query Representations for Dense Retrieval","tldr":"Recent developments of dense retrieval rely on quality representations of queries and contexts from pre-trained query and context encoders. In this paper, we introduce TOUR (Test-Time Optimization of Query Representations), which further optimizes instance-level query representations guided by signa...","track":"Question Answering","underline_id":77452,"underline_url":"https://underline.io/events/395/posters/15240/poster/77452-optimizing-test-time-query-representations-for-dense-retrieval","video_url":null},{"abstract":"We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics. For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata. This dataset adopts morphological feature schema from CITATION and CITATION for the Korean language as we extract inflected verb forms from the Sejong morphologically analyzed corpus that is one of the largest annotated corpora for Korean. During the data creation, our methodology also includes investigating the correctness of the conversion from the Sejong corpus. Furthermore, we carry out the inflection task using three different Korean word forms: letters, syllables and morphemes. Finally, we discuss and describe future perspectives on Korean morphological paradigms and the dataset.","anthology_url":"https://aclanthology.org/2023.findings-acl.414","authors":["Eunkyul Leah Jo","Kim Kyuwon","Xihan Wu","KyungTae Lim","Jungyeul Park","Chulwoo Park"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)"],"id":"P1378","is_paper":true,"keywords":["corpus creation"],"languages":["korean"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.414.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77454/poster_document/aa501cd84c59a94c9c2cc79abeb325e0.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"K-UniMorph: Korean Universal Morphology and its Feature Schema","tldr":"We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that ...","track":"Resources and Evaluation","underline_id":77454,"underline_url":"https://underline.io/events/395/posters/15279/poster/77454-k-unimorph-korean-universal-morphology-and-its-feature-schema","video_url":null},{"abstract":"We present a reality check on large language models and inspect the promise of retrieval-augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the parametric nature of vanilla large language models. We give initial experimental findings that semi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.382","authors":["Wang-Chiew Tan","Yuliang Li","Pedro Rodriguez","Richard James","Xi Victoria Lin","Alon Halevy","Wen-tau Yih"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-4_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1391","is_paper":true,"keywords":["methodology"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.382.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77458/poster_document/4ac0bb9c320f68d92f1ce0bbeb02b185.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Reimagining Retrieval Augmented Language Models for Answering Queries","tldr":"We present a reality check on large language models and inspect the promise of retrieval-augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the pa...","track":"Theme: Reality Check","underline_id":77458,"underline_url":"https://underline.io/events/395/posters/15240/poster/77458-reimagining-retrieval-augmented-language-models-for-answering-queries","video_url":null},{"abstract":"Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical resource when they try to provide advice in context.  However, obtaining answers to questions over lifelogs is beyond the current state of the art of question answering techniques for a variety of reasons, the most pronounced of which is that lifelogs combine free text with some degree of structure such as temporal and geographical information.  \n\nWe create and publicly release TimelineQA, a benchmark for accelerating progress on querying lifelogs. TimelineQA generates lifelogs of imaginary people. The episodes in the lifelog range from major life episodes such as high school graduation to those that occur on a daily basis such as going for a run. We describe a set of experiments on TimelineQA with several state-of-the-art QA models. Our experiments reveal that for atomic queries, an extractive QA system significantly out-performs a state-of-the-art retrieval-augmented QA system. For multi-hop queries involving aggregates, we show that the best result is obtained with a state-of-the-art table QA technique, assuming the ground truth set of episodes for deriving the answer is available.","anthology_url":"https://aclanthology.org/2023.findings-acl.6","authors":["Wang-Chiew Tan","Jane Dwivedi-Yu","Yuliang Li","Lambert Mathias","Marzieh Saeidi","Jing Nathan Yan","Alon Y. Halevy"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-7_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1398","is_paper":true,"keywords":["commonsense qa","multihop qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.6.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"TimelineQA: A Benchmark for Question Answering over Timelines","tldr":"Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical re...","track":"Question Answering","underline_id":78155,"underline_url":"https://underline.io/events/395/posters/15279/poster/78155-timelineqa-a-benchmark-for-question-answering-over-timelines","video_url":null},{"abstract":"While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of \\{pasted macro `BENCHMARK'\\}, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries.","anthology_url":"https://aclanthology.org/2023.findings-acl.322","authors":["Derek Tam","Anisha Mascarenhas","Shiyue Zhang","Sarah Kwan","Mohit Bansal","Colin Raffel"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-4_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1402","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.322.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77460/poster_document/1940608b78cfcaf7455cb928d44725a9.pdf","preview_image":"https://assets.underline.io/lecture/77460/poster/44e7086176cd43d083187f9eb2faf945.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77460/slideshow/fd403f4c7f6f85da1f11e2dfe41c1579.pdf","title":"Evaluating the Factual Consistency of Large Language Models Through News Summarization","tldr":"While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that ...","track":"Large Language Models","underline_id":77460,"underline_url":"https://underline.io/events/395/posters/15240/poster/77460-diffusiondb-a-large-scale-prompt-gallery-dataset-for-text-to-image-generative-models","video_url":null},{"abstract":"Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures. ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tailed label distribution. Recent automated ICD coding efforts improve performance by encoding medical notes and codes with additional data and knowledge bases. However, most of them do not reflect how human coders generate the code: first, the coders select general code categories and then look for specific subcategories that are relevant to a patient's condition. Inspired by this, we propose a two-stage decoding mechanism to predict ICD codes. Our model uses the hierarchical properties of the codes to split the prediction into two steps: At first, we predict the parent code and then predict the child code based on the previous prediction. Experiments on the public MIMIC-III data set have shown that our model performs well in single-model settings without external data or knowledge.","anthology_url":"https://aclanthology.org/2023.findings-acl.285","authors":["Thanh-Tung Nguyen","Viktor Schlegel","Abhinav Ramesh Kashyap","Stefan Winkler"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-1_-nlp-applications-(virtual-poster)"],"id":"P1411","is_paper":true,"keywords":["healthcare applications, clincial nlp"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.285.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77463/poster_document/6f055ce7bc934d57a13ca067ebb143b2.pdf","preview_image":"https://assets.underline.io/lecture/77463/poster/507f5b7b44176d2e5827c6b6b55da3af.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77463/slideshow/30fdc6a942a1bd757ae35a554b4eea0c.pdf","title":"A Two-Stage Decoder for Efficient ICD Coding","tldr":"Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures. ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tail...","track":"NLP Applications","underline_id":77463,"underline_url":"https://underline.io/events/395/posters/15200/poster/77463-a-two-stage-decoder-for-efficient-icd-coding","video_url":null},{"abstract":"Word sense disambiguation (WSD), which aims to determine an appropriate sense for a target word given its context, is crucial for natural language understanding. Existing supervised methods treat WSD as a classification task and have achieved remarkable performance. However, they ignore uncertainty estimation (UE) in the real-world setting, where the data is always noisy and out of distribution. This paper extensively studies UE on the benchmark designed for WSD. Specifically, we first compare four uncertainty scores for a state-of-the-art WSD model and verify that the conventional predictive probabilities obtained at the end of the model are inadequate to quantify uncertainty. Then, we examine the capability of capturing data and model uncertainties by the model with the selected UE score on well-designed test scenarios and discover that the model reflects data uncertainty satisfactorily but underestimates model uncertainty. Furthermore, we explore numerous lexical properties that intrinsically affect data uncertainty and provide a detailed analysis of four critical aspects: the syntactic category, morphology, sense granularity, and semantic relations.","anthology_url":"https://aclanthology.org/2023.findings-acl.245","authors":["Zhu Liu","Ying Liu"],"category":"Findings","demo_url":null,"display_track":"Semantics: Lexical","event_ids":["session-1_-semantics_-lexical-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1436","is_paper":true,"keywords":["polysemy"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.245.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Ambiguity Meets Uncertainty: Investigating Uncertainty Estimation for Word Sense Disambiguation","tldr":"Word sense disambiguation (WSD), which aims to determine an appropriate sense for a target word given its context, is crucial for natural language understanding. Existing supervised methods treat WSD as a classification task and have achieved remarkable performance. However, they ignore uncertainty ...","track":"Semantics: Lexical","underline_id":77467,"underline_url":"https://underline.io/events/395/posters/15200/poster/77467-ambiguity-meets-uncertainty-investigating-uncertainty-estimation-for-word-sense-disambiguation","video_url":null},{"abstract":"Lexically constrained neural machine translation (LCNMT), which controls the translation generation with pre-specified constraints, is important in many practical applications. Current approaches to LCNMT typically assume that the pre-specified lexicon constraints are contextually appropriate. This assumption limits their application to real-world scenarios where a source lexicon may have multiple target constraints, and disambiguation is needed to select the most suitable one. In this paper, we propose disambiguated LCNMT (D-LCNMT) to solve the problem. D-LCNMT is a robust and effective two-stage framework that disambiguates the constraints based on contexts at first, then integrates the disambiguated constraints into LCNMT. Experimental results show that our approach outperforms strong baselines including existing data argumentation based approaches on benchmark datasets, and comprehensive experiments in scenarios where a source lexicon corresponds to multiple target constraints demonstrate the constraint disambiguation superiority of our approach.","anthology_url":"https://aclanthology.org/2023.findings-acl.673","authors":["Jinpeng Zhang","Nini Xiao","Ke Wang","Chuanqi Dong","Xiangyu Duan","Yuqi Zhang","Min Zhang"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-7_-machine-translation-(virtual-poster)"],"id":"P1463","is_paper":true,"keywords":["switch-code translation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.673.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77473/poster_document/fb4b1c1bd9de67953276b3a4febb771d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Disambiguated Lexically Constrained Neural Machine Translation","tldr":"Lexically constrained neural machine translation (LCNMT), which controls the translation generation with pre-specified constraints, is important in many practical applications. Current approaches to LCNMT typically assume that the pre-specified lexicon constraints are contextually appropriate. This ...","track":"Machine Translation","underline_id":77473,"underline_url":"https://underline.io/events/395/posters/15279/poster/77473-disambiguated-lexically-constrained-neural-machine-translation","video_url":null},{"abstract":"Recent advancements in multimodal foundation models (e.g., CLIP) have excelled in zero-shot generalization. Prompt tuning involved in the knowledge transfer from foundation models to downstream tasks has gained significant attention recently. Existing prompt-tuning methods in cross-modal learning, however, either solely focus on language branch, or learn vision-language interaction in a shallow mechanism. In this context, we propose a Deeply coupled Cross-modal Prompt learning (DCP) method based on CLIP. DCP flexibly accommodates the interplay between vision and language with a Cross-Modal Prompt Attention (CMPA) mechanism, which enables the mutual exchange of respective representation through a well-connected multi-head attention progressively and strongly. We then conduct comprehensive few-shot learning experiments on 11 image classification datasets and analyze the robustness to domain shift as well. Thorough experimental analysis evidently demonstrates the superb few-shot generalization and compelling domain adaption capacity of a well-executed DCP.","anthology_url":"https://aclanthology.org/2023.findings-acl.504","authors":["Xuejing Liu","Wei Tang","Jinghui Lu","Rui Zhao","Zhaojun Guo","Fei Tan"],"category":"Findings","demo_url":null,"display_track":"Speech and Multimodality","event_ids":["session-1_-speech-and-multimodality-(virtual-poster)"],"id":"P1464","is_paper":true,"keywords":["multimodality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.504.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77474/poster_document/f58ad0d9efec7295a3621e26748f70e3.pdf","preview_image":"https://assets.underline.io/lecture/77474/poster/06ca68dbecb79c70162bfb6ae8139b6d.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77474/slideshow/76e56204ea828744b3507e287ef470dc.pptx","title":"Deeply Coupled Cross-Modal Prompt Learning","tldr":"Recent advancements in multimodal foundation models (e.g., CLIP) have excelled in zero-shot generalization. Prompt tuning involved in the knowledge transfer from foundation models to downstream tasks has gained significant attention recently. Existing prompt-tuning methods in cross-modal learning, h...","track":"Speech and Multimodality","underline_id":77474,"underline_url":"https://underline.io/events/395/posters/15200/poster/77474-deeply-coupled-cross-modal-prompt-learning","video_url":null},{"abstract":"Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26,647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset can be found at https://persuasion-deductiongame. socialai-data.org. The codes and models are available at https://github.com/ SALT-NLP/PersuationGames.","anthology_url":"https://aclanthology.org/2023.findings-acl.411","authors":["Bolin Lai","Hongxin Zhang","Miao Liu","Aryan J Pariani","Fiona Ryan","Wenqi Jia","Shirley Anugrah Hayati","James M Rehg","Diyi Yang"],"category":"Findings","demo_url":null,"display_track":"Speech and Multimodality","event_ids":["session-1_-speech-and-multimodality-(virtual-poster)"],"id":"P1465","is_paper":true,"keywords":["multimodality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.411.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games","tldr":"Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multim...","track":"Speech and Multimodality","underline_id":77475,"underline_url":"https://underline.io/events/395/posters/15200/poster/77475-werewolf-among-us-multimodal-resources-for-modeling-persuasion-behaviors-in-social-deduction-games","video_url":null},{"abstract":"Dialogue-level dependency parsing has received insufficient attention, especially for Chinese. To this end, we draw on ideas from syntactic dependency and rhetorical structure theory (RST), developing a high-quality human-annotated corpus, which contains 850 dialogues and 199,803 dependencies. Considering that such tasks suffer from high annotation costs, we investigate zero-shot and few-shot scenarios. Based on an existing syntactic treebank, we adopt a signal-based method to transform seen syntactic dependencies into unseen ones between elementary discourse units (EDUs), where the signals are detected by masked language modeling. Besides, we apply single-view and multi-view data selection to access reliable pseudo-labeled instances. Experimental results show the effectiveness of these baselines. Moreover, we discuss several crucial points about our dataset and approach.","anthology_url":"https://aclanthology.org/2023.findings-acl.607","authors":["Gongyao Jiang","Shuang Liu","Meishan Zhang","Min Zhang"],"category":"Findings","demo_url":null,"display_track":"Syntax: Tagging, Chunking, and Parsing","event_ids":["session-7_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)"],"id":"P1498","is_paper":true,"keywords":["dependency parsing"],"languages":["chinese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.607.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77480/poster_document/f43ed1f947363d9ff71d2119c810efac.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"A Pilot Study on Dialogue-Level Dependency Parsing for Chinese","tldr":"Dialogue-level dependency parsing has received insufficient attention, especially for Chinese. To this end, we draw on ideas from syntactic dependency and rhetorical structure theory (RST), developing a high-quality human-annotated corpus, which contains 850 dialogues and 199,803 dependencies. Consi...","track":"Syntax: Tagging, Chunking, and Parsing","underline_id":77480,"underline_url":"https://underline.io/events/395/posters/15279/poster/77480-a-pilot-study-on-dialogue-level-dependency-parsing-for-chinese","video_url":null},{"abstract":"Large Language Models (LLMs) have in recent years demonstrated impressive prowess in natural language generation. A common practice to improve generation diversity is to sample multiple outputs from the model. However, partly due to the inaccessibility of LLMs, there lacks a simple and robust way of selecting the best output from these stochastic samples. As a case study framed in the context of question generation, we propose two prompt-based approaches, namely round-trip and prompt-based score, to selecting high-quality questions from a set of LLM-generated candidates. Our method works without the need to modify the underlying model,  nor does it rely on human-annotated references --- both of which are realistic constraints for real-world deployment of LLMs. With automatic as well as human evaluations, we empirically demonstrate that our approach can effectively select questions of higher qualities than greedy generation.","anthology_url":"https://aclanthology.org/2023.findings-acl.820","authors":["Xingdi Yuan","Tong Wang","Yen-Hsiang Wang","Emery Fine","Rania Abdelghani","H\u00e9l\u00e8ne Sauz\u00e9on","Pierre-Yves Oudeyer"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)"],"id":"P1500","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.820.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77481/poster_document/ec5346bde36a033a793256fa873a9880.pdf","preview_image":"https://assets.underline.io/lecture/77481/poster/844bbda65770611bec001be461d21233.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77481/slideshow/2ed632a068029e987781d9ab1f4530cf.pdf","title":"Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation","tldr":"Large Language Models (LLMs) have in recent years demonstrated impressive prowess in natural language generation. A common practice to improve generation diversity is to sample multiple outputs from the model. However, partly due to the inaccessibility of LLMs, there lacks a simple and robust way of...","track":"NLP Applications","underline_id":77481,"underline_url":"https://underline.io/events/395/posters/15240/poster/77481-selecting-better-samples-from-pre-trained-llms-a-case-study-on-question-generation","video_url":null},{"abstract":"Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the conversational system. However, they share the same parameters to train tasks of the dialogue system (NLU, DST, NLG), so debugging each task is challenging. Also, they require a lot of effort to fine-tune large parameters to create a task-oriented chatbot, making it difficult for non-experts to handle. Therefore, we intend to train relatively lightweight and fast models compared to PLM. In this paper, we propose an End-to-end TOD system with Task-Optimized Adapters which learn independently per task, adding only small number of parameters after fixed layers of pre-trained network. We also enhance the performance of the DST and NLG modules through reinforcement learning, overcoming the learning curve that has lacked at the adapter learning and enabling the natural and consistent response generation that is appropriate for the goal. Our method is a model-agnostic approach and does not require prompt-tuning as only input data without a prompt. As results of the experiment, our method shows competitive performance on the MultiWOZ benchmark compared to the existing end-to-end models. In particular, we attain state-of-the-art performance on the DST task of 2.2 dataset.","anthology_url":"https://aclanthology.org/2023.findings-acl.464","authors":["Namo Bang","Jeehyun Lee","Myoung-Wan Koo"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-4_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P1513","is_paper":true,"keywords":["task-oriented"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.464.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77483/poster/9854eba389cfb341aab3ff10f13f4d99.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77483/slideshow/054c5367cd9c6919c5086f4cb3cfd448.pdf","title":"Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System","tldr":"Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the convers...","track":"Dialogue and Interactive Systems","underline_id":77483,"underline_url":"https://underline.io/events/395/posters/15240/poster/77483-detoxifying-text-with-marco-controllable-revision-with-experts-and-anti-experts","video_url":null},{"abstract":"This study presents a dynamic structured neural topic model, which can handle the time-series development of topics while capturing their dependencies.\nOur model captures the topic branching and merging processes by modeling topic dependencies based on a self-attention mechanism.\nAdditionally, we introduce citation regularization, which induces attention weights to represent citation relations by modeling text and citations jointly.\nOur model outperforms a prior dynamic embedded topic model regarding perplexity and coherence, while maintaining sufficient diversity across topics.\nFurthermore, we confirm that our model can potentially predict emerging topics from academic literature.","anthology_url":"https://aclanthology.org/2023.findings-acl.366","authors":["Nozomu Miyamoto","Masaru Isonuma","Sho Takase","Junichiro Mori","Ichiro Sakata"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-1_-information-retrieval-and-text-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1520","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.366.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77484/poster_document/3fccb1d729afcfc619645f1f1fdb5546.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77484/slideshow/893a66f11f83065eb006145ade9f936d.pdf","title":"Dynamic Structured Neural Topic Model with Self-Attention Mechanism","tldr":"This study presents a dynamic structured neural topic model, which can handle the time-series development of topics while capturing their dependencies.\nOur model captures the topic branching and merging processes by modeling topic dependencies based on a self-attention mechanism.\nAdditionally, we in...","track":"Information Retrieval and Text Mining","underline_id":77484,"underline_url":"https://underline.io/events/395/posters/15200/poster/77484-dynamic-structured-neural-topic-model-with-self-attention-mechanism","video_url":null},{"abstract":"Open-vocabulary state tracking is a more practical version of state tracking that aims to track state changes of entities throughout a process without restricting the state space and entity space. OpenPI (Tandon et al., 2020) is to date the only dataset annotated for open-vocabulary state tracking. However, we identify issues with the dataset quality and evaluation metric. For the dataset, we categorize 3 types of problems on the procedure level, step level and state change level respectively, and build a clean dataset OpenPI-C using multiple rounds of human judgment. For the evaluation metric, we propose a cluster-based metric to fix the original metric's preference for repetition.\n\nModel-wise, we enhance the seq2seq generation baseline by reinstating two key properties for state tracking: temporal dependency and entity awareness. The state of the world after an action is inherently dependent on the previous state. We model this dependency through a dynamic memory bank and allow the model to attend to the memory slots during decoding. On the other hand, the state of the world is naturally a union of the states of involved entities. Since the entities are unknown in the open-vocabulary setting, we propose a two-stage model that refines the state change prediction conditioned on entities predicted from the first stage. Empirical results show the effectiveness of our proposed model, especially on the cleaned dataset and the cluster-based metric. The code and data are released at https://github.com/shirley-wu/openpi-c","anthology_url":"https://aclanthology.org/2023.findings-acl.452","authors":["Xueqing Wu","Sha Li","Heng Ji"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-1_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1530","is_paper":true,"keywords":["benchmarking","nlp datasets","metrics"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.452.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77487/poster_document/230bfa3a997136d079ef4916c99c4bb0.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77487/slideshow/8c2f48c2823d0424b87e05ab7005c40d.pptx","title":"OpenPI-C: A Better Benchmark and Stronger Baseline for Open-Vocabulary State Tracking","tldr":"Open-vocabulary state tracking is a more practical version of state tracking that aims to track state changes of entities throughout a process without restricting the state space and entity space. OpenPI (Tandon et al., 2020) is to date the only dataset annotated for open-vocabulary state tracking. ...","track":"Resources and Evaluation","underline_id":77487,"underline_url":"https://underline.io/events/395/posters/15200/poster/77487-openpi-c-a-better-benchmark-and-stronger-baseline-for-open-vocabulary-state-tracking","video_url":null},{"abstract":"Instruction tuning has been attracting much attention to achieve generalization ability across a wide variety of tasks.\nAlthough various types of instructions have been manually created for instruction tuning, it is still unclear what kind of instruction is optimal to obtain cross-task generalization ability.\nThis work presents instruction optimization, which optimizes training instructions with respect to generalization ability.\nRather than manually tuning instructions, we introduce learnable instructions and optimize them with gradient descent by leveraging bilevel optimization.\nExperimental results show that the learned instruction enhances the diversity of instructions and improves the generalization ability compared to using only manually created instructions.","anthology_url":"https://aclanthology.org/2023.findings-acl.667","authors":["Masaru Isonuma","Junichiro Mori","Ichiro Sakata"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-7_-generation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1538","is_paper":true,"keywords":["few-shot generation","text-to-text generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.667.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77489/poster_document/13c608f7fcfad8ae1ed4d6d13305d602.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77489/slideshow/fb066108892654c9c2f5893954faa53b.pdf","title":"Differentiable Instruction Optimization for Cross-Task Generalization","tldr":"Instruction tuning has been attracting much attention to achieve generalization ability across a wide variety of tasks.\nAlthough various types of instructions have been manually created for instruction tuning, it is still unclear what kind of instruction is optimal to obtain cross-task generalizatio...","track":"Generation","underline_id":77489,"underline_url":"https://underline.io/events/395/posters/15279/poster/77489-differentiable-instruction-optimization-for-cross-task-generalization","video_url":null},{"abstract":"We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying  pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using Real Toxicity Prompts. Our experiments show that CFL  achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.","anthology_url":"https://aclanthology.org/2023.findings-acl.720","authors":["Rahul Madhavan","Rishabh Garg","Kahini Wadhawan","Sameep Mehta"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1545","is_paper":true,"keywords":["causality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.720.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77490/poster_document/786b111a42b231256062294baf2220f4.pdf","preview_image":"https://assets.underline.io/lecture/77490/poster/7a66ab7105aaa84932cd799ceb35bf99.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77490/slideshow/64ec7705301aa8c5fa316eadf31faaab.pdf","title":"CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation","tldr":"We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architectu...","track":"Machine Learning for NLP","underline_id":77490,"underline_url":"https://underline.io/events/395/posters/15279/poster/77490-seti-systematicity-evaluation-of-textual-inference","video_url":null},{"abstract":"Prompt-based methods have shown their efficacy in transferring general knowledge within pre-trained language models (PLMs) for low-resource scenarios.\nTypically, prompt-based methods convert downstream tasks to cloze-style problems and map all labels to verbalizers.\nHowever, when applied to zero-shot entity and relation extraction, vanilla prompt-based methods may struggle with the limited coverage of verbalizers to labels and the slow inference speed.\nIn this work, we propose a novel Discriminate Soft Prompts (DSP) approach to take advantage of the prompt-based methods to strengthen the transmission of general knowledge.\nSpecifically, we develop a discriminative prompt method, which reformulates zero-shot tasks into token discrimination tasks without having to construct verbalizers.\nFurthermore, to improve the inference speed of the prompt-based methods, we design a soft prompt co-reference strategy, which leverages soft prompts to approximately refer to the vector representation of text tokens.\nThe experimental results show that, our model outperforms baselines on two zero-shot entity recognition datasets with higher inference speed, and obtains a 7.5\\% average relation F1-score improvement over previous state-of-the-art models on Wiki-ZSL and FewRel.","anthology_url":"https://aclanthology.org/2023.findings-acl.339","authors":["Bo Lv","Xin Liu","Shaojie Dai","Nayu Liu","Fan Yang","Ping Luo","Yue Yu"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-4_-information-extraction-(virtual-poster)"],"id":"P1550","is_paper":true,"keywords":["zero/few-shot extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.339.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"DSP: Discriminative Soft Prompts for Zero-Shot Entity and Relation Extraction","tldr":"Prompt-based methods have shown their efficacy in transferring general knowledge within pre-trained language models (PLMs) for low-resource scenarios.\nTypically, prompt-based methods convert downstream tasks to cloze-style problems and map all labels to verbalizers.\nHowever, when applied to zero-sho...","track":"Information Extraction","underline_id":77493,"underline_url":"https://underline.io/events/395/posters/15240/poster/77493-dsp-discriminative-soft-prompts-for-zero-shot-entity-and-relation-extraction","video_url":null},{"abstract":"Learning multiscale Transformer models has been evidenced as a viable approach to  augmenting machine translation systems. Prior research has primarily focused on treating subwords as basic units in developing such systems. However, the incorporation of fine-grained character-level features into multiscale Transformer has not yet been explored. In this work, we present a \\textbf{S}low-\\textbf{F}ast two-stream learning model, referred to as Tran\\textbf{SF}ormer, which utilizes a ``slow'' branch to deal with subword sequences and a ``fast'' branch to deal with longer character sequences. This model is efficient since the fast branch is very lightweight by reducing the model width, and yet provides useful fine-grained features for the slow branch. Our TranSFormer shows consistent BLEU improvements (larger than 1 BLEU point) on several machine translation benchmarks.","anthology_url":"https://aclanthology.org/2023.findings-acl.430","authors":["Bei Li","Yi Jing","Xu Tan","Zhen Xing","Tong Xiao","Jingbo Zhu"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-4_-machine-translation-(virtual-poster)"],"id":"P157","is_paper":true,"keywords":["modelling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.430.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77270/poster_document/278c03b28eb18e217292f89797ea6884.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"TranSFormer: Slow-Fast Transformer for Machine Translation","tldr":"Learning multiscale Transformer models has been evidenced as a viable approach to  augmenting machine translation systems. Prior research has primarily focused on treating subwords as basic units in developing such systems. However, the incorporation of fine-grained character-level features into mul...","track":"Machine Translation","underline_id":77270,"underline_url":"https://underline.io/events/395/posters/15240/poster/77270-transformer-slow-fast-transformer-for-machine-translation","video_url":null},{"abstract":"Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models' (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the context limit, which has already been heavily investigated, we explore an orthogonal direction: making LMs divide a problem into multiple contexts. We propose a new inference framework, called Recursion of Thought (RoT), which introduces several special tokens that the models can output to trigger context-related operations. Extensive experiments with multiple architectures including GPT-3 show that RoT dramatically improves LMs' inference capability to solve problems, whose solution consists of hundreds of thousands of tokens.","anthology_url":"https://aclanthology.org/2023.findings-acl.40","authors":["Soochan Lee","Gunhee Kim"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-1_-large-language-models-(virtual-poster)"],"id":"P1583","is_paper":true,"keywords":["fine-tuning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.40.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77499/poster_document/03150f22f29e4c53b86ea9b9401fb969.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77499/slideshow/d10988e36abfe28293b26b459d5da051.pdf","title":"Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models","tldr":"Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models' (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the co...","track":"Large Language Models","underline_id":77499,"underline_url":"https://underline.io/events/395/posters/15200/poster/77499-recursion-of-thought-a-divide-and-conquer-approach-to-multi-context-reasoning-with-language-models","video_url":null},{"abstract":"This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition. Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task. Without pretraining on large-scale annotated or synthetic text recognition data, OFA-OCR outperforms the baselines and achieves state-of-the-art performance in the Chinese text recognition benchmark. Additionally, we construct an OCR pipeline with OFA-OCR, and we demonstrate that it can achieve competitive performance with the product-level API.","anthology_url":"https://aclanthology.org/2023.findings-acl.37","authors":["Junyang Lin","Xuancheng Ren","Yichang Zhang","Gao Liu","Peng Wang","An Yang","Chang Zhou"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)"],"id":"P1593","is_paper":true,"keywords":["cross-modal content generation","cross-modal application"],"languages":["chinese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.37.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77502/poster_document/31f252e4abe05e0e7d963d8204e610e4.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Transferring General Multimodal Pretrained Models to Text Recognition","tldr":"This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition. Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task. Without pretraining on large-scale annotated or synt...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77502,"underline_url":"https://underline.io/events/395/posters/15240/poster/77502-transferring-general-multimodal-pretrained-models-to-text-recognition","video_url":null},{"abstract":"Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm, called subword segmental machine translation (SSMT). SSMT unifies subword segmentation and MT in a single trainable model. It learns to segment target sentence words while jointly learning to generate target sentences. To use SSMT during inference we propose  dynamic decoding, a text generation algorithm that adapts segmentations as it generates translations. Experiments across 6 translation directions show that SSMT improves chrF scores for morphologically rich agglutinative languages. Gains are strongest in the very low-resource scenario. SSMT also learns subwords that are closer to morphemes compared to baselines and proves more robust on a test set constructed for evaluating morphological compositional generalisation.","anthology_url":"https://aclanthology.org/2023.findings-acl.175","authors":["Francois Meyer","Jan Buys"],"category":"Findings","demo_url":null,"display_track":"Phonology, Morphology, and Word Segmentation","event_ids":["session-7_-phonology,-morphology,-and-word-segmentation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1621","is_paper":true,"keywords":["morphological segmentation","subword representations","morphological analysis"],"languages":["xhosa","zulu","finnish","swati","tswana","afrikaans"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.175.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77506/poster_document/f9ff196bad288ed541143f1a6900cee0.pdf","preview_image":"https://assets.underline.io/lecture/77506/poster/2ae63cbbe1ebe70e0c3dfab104e17188.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77506/slideshow/9b1742e5aceaa8cca7d33e95a51a3f06.pdf","title":"Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation","tldr":"Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm...","track":"Phonology, Morphology, and Word Segmentation","underline_id":77506,"underline_url":"https://underline.io/events/395/posters/15279/poster/77506-language-model-acceptability-judgements-are-not-always-robust-to-context","video_url":null},{"abstract":"Multilingual Knowledge Graph Completion (mKGC) aim at solving queries  in different languages by reasoning a tail entity thus improving multilingual knowledge graphs. Previous studies leverage multilingual pretrained language models (PLMs) and the generative paradigm to achieve mKGC. Although multilingual pretrained language models contain extensive knowledge of different languages, its pretraining tasks cannot be directly aligned with the mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit a pronounced English-centric bias. This makes it difficult for mKGC to achieve good results, particularly in the context of low-resource languages. To overcome previous problems, this paper introduces global and local knowledge constraints for mKGC. The former is used to constrain the reasoning of answer entities , while the latter is used to enhance the representation of query contexts. The proposed method makes the pretrained model better adapt to the mKGC task. Experimental results on public datasets  demonstrate that our method outperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32\\% and 16.03\\%, which indicates that our proposed method has significant enhancement on mKGC.","anthology_url":"https://aclanthology.org/2023.findings-acl.488","authors":["Ran Song","Shizhu He","Shengxiang Gao","Li Cai","Kang Liu","Zhengtao YU","Jun Zhao"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)"],"id":"P1623","is_paper":true,"keywords":["knowledge graphs"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.488.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77507/poster_document/5a6b8d374c891e256bc27cd7f4f2a655.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints","tldr":"Multilingual Knowledge Graph Completion (mKGC) aim at solving queries  in different languages by reasoning a tail entity thus improving multilingual knowledge graphs. Previous studies leverage multilingual pretrained language models (PLMs) and the generative paradigm to achieve mKGC. Although multil...","track":"NLP Applications","underline_id":77507,"underline_url":"https://underline.io/events/395/posters/15240/poster/77507-multilingual-knowledge-graph-completion-from-pretrained-language-models-with-knowledge-constraints","video_url":null},{"abstract":"Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Thanks to the superior generative capacity of large pretrained language models (PLM), recent work built on prompt engineering enables explanations generated without specific training. However, explanations generated through single-pass prompting often lack sufficiency and conciseness, due to the prompt complexity and hallucination issues. To discard the dross and take the essence of current PLM's results, we propose to produce sufficient and concise explanations via the information bottleneck (EIB) theory. EIB regenerates explanations by polishing the single-pass output of PLM but retaining the information that supports the contents being explained by balancing two information bottleneck objectives. Experiments on two different tasks verify the effectiveness of EIB through automatic evaluation and thoroughly-conducted human evaluation.","anthology_url":"https://aclanthology.org/2023.findings-acl.765","authors":["Qintong Li","Zhiyong Wu","Lingpeng Kong","Wei Bi"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1636","is_paper":true,"keywords":["free-text/natural language explanations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.765.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77509/poster_document/2aa23fbbb71fb76d98ac1e6f91dc604d.pdf","preview_image":"https://assets.underline.io/lecture/77509/poster/16fa0f2831ee2560ae217d21496d1b9b.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77509/slideshow/8ea96781a9f576f5a034a5afaf846c83.pptx","title":"Explanation Regeneration via Information Bottleneck","tldr":"Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Thanks to the superio...","track":"Interpretability and Analysis of Models for NLP","underline_id":77509,"underline_url":"https://underline.io/events/395/posters/15200/poster/77509-explanation-regeneration-via-information-bottleneck","video_url":null},{"abstract":"Twitter user profile inference utilizes information from Twitter to predict user attributes (e.g., occupation, location), which is controversial because of its usefulness for downstream applications and its potential to reveal users' privacy. Therefore, it is important for researchers to determine the extent of profiling in a safe environment to facilitate proper use and make the public aware of the potential risks. Contrary to existing approaches on limited attributes, we explore open-domain Twitter user profile inference. We conduct a case study where we collect publicly available WikiData public figure profiles and use diverse WikiData predicates for profile inference. After removing sensitive attributes, our data contains over 150K public figure profiles from WikiData, over 50 different attribute predicates, and over 700K attribute values. We further propose a prompt-based generation method, which can infer values that are implicitly mentioned in the Twitter information. Experimental results show that the generation-based approach can infer more comprehensive user profiles than baseline extraction-based methods, but limitations still remain to be applied for real-world use. We also enclose a detailed ethical statement for our data, potential benefits and risks from this work, and our efforts to mitigate the risks.","anthology_url":"https://aclanthology.org/2023.findings-acl.198","authors":["Haoyang Wen","Zhenxin Xiao","Eduard H Hovy","Alexander Hauptmann"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)"],"id":"P1637","is_paper":true,"keywords":["psycho-demographic trait prediction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.198.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77510/poster_document/1a8423978023a2cf7d1f122393cbd73c.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Towards Open-Domain Twitter User Profile Inference","tldr":"Twitter user profile inference utilizes information from Twitter to predict user attributes (e.g., occupation, location), which is controversial because of its usefulness for downstream applications and its potential to reveal users' privacy. Therefore, it is important for researchers to determine t...","track":"Computational Social Science and Cultural Analytics","underline_id":77510,"underline_url":"https://underline.io/events/395/posters/15279/poster/77510-towards-open-domain-twitter-user-profile-inference","video_url":null},{"abstract":"In this work, we enhance higher-order graph-based approaches for span-based semantic role labeling (SRL) by means of structured modeling.  To decrease the complexity of higher-order modeling, we decompose the edge from predicate word to argument span into three different edges, predicate-to-head (P2H), predicate-to-tail (P2T), and head-to-tail (H2T), where head/tail means the first/last word of the semantic argument span. As such, we use a CRF-based higher-order dependency parser and leverage Mean-Field Variational Inference (MFVI) for higher-order inference. Moreover, since semantic arguments of predicates are often constituents within a constituency parse tree, we can leverage such nice structural property by defining a TreeCRF distribution over all H2T edges, using the idea of partial marginalization to define structural training loss. We further leverage structured MFVI to enhance inference. We experiment on span-based SRL benchmarks, showing the effectiveness of both higher-order and structured  modeling and the combination thereof.  In addition, we show superior performance of structured MFVI against vanilla MFVI.","anthology_url":"https://aclanthology.org/2023.findings-acl.58","authors":["Wei Liu","Songlin Yang","Kewei Tu"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)"],"id":"P1645","is_paper":true,"keywords":["structured prediction","graphical models"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.58.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77512/poster_document/8e8831a274f0ae36b1bf18e4386f0eed.pdf","preview_image":"https://assets.underline.io/lecture/77512/poster/1889bef041ad9cf527719f9beecc306c.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77512/slideshow/4bb793460b58786c5dfb0b075df8e495.pdf","title":"Structured Mean-Field Variational Inference for Higher-Order Span-Based Semantic Role","tldr":"In this work, we enhance higher-order graph-based approaches for span-based semantic role labeling (SRL) by means of structured modeling.  To decrease the complexity of higher-order modeling, we decompose the edge from predicate word to argument span into three different edges, predicate-to-head (P2...","track":"Machine Learning for NLP","underline_id":77512,"underline_url":"https://underline.io/events/395/posters/15200/poster/77512-structured-mean-field-variational-inference-for-higher-order-span-based-semantic-role","video_url":null},{"abstract":"We investigate the representation of pretrained language models and humans, using the idea of word definition modeling\u2013how well a word is represented by its definition, and vice versa. Our analysis shows that a word representation in pretrained language models does not successfully map its human-written definition and its usage in example sentences. We then present a simple method DefBERT that integrates pretrained models with word semantics in dictionaries. We show its benefits on newly-proposed tasks of definition ranking and definition sense disambiguation. Furthermore, we present the results on standard word similarity tasks and short text classification tasks where models are required to encode semantics with only a few words. The results demonstrate the effectiveness of integrating word definitions and pretrained language models.","anthology_url":"https://aclanthology.org/2023.findings-acl.2","authors":["Hwiyeol Jo"],"category":"Findings","demo_url":null,"display_track":"Semantics: Lexical","event_ids":["session-7_-semantics_-lexical-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1662","is_paper":true,"keywords":["lexical relationships","compositionality","interpretability"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.2.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77514/poster_document/e50465d1b7050663a82d10e1f539b33d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"A Self-Supervised Integration Method of Pretrained Language Models and Word Definitions","tldr":"We investigate the representation of pretrained language models and humans, using the idea of word definition modeling\u2013how well a word is represented by its definition, and vice versa. Our analysis shows that a word representation in pretrained language models does not successfully map its human-wri...","track":"Semantics: Lexical","underline_id":77514,"underline_url":"https://underline.io/events/395/posters/15279/poster/77514-towards-generative-event-factuality-prediction","video_url":null},{"abstract":"As a critical step to achieve human-like chatbots, empathetic response generation has attained increasing interests. Previous attempts are incomplete and not sufficient enough to elicit empathy because they only stay on the initial stage of empathy to automatically sense and simulate the feelings and thoughts of others via other-awareness. However, they ignore to include self-awareness to consider the own views of the self in their responses, which is a crucial process to achieve the empathy. To this end, we propose to generate Empathetic response with explicit Self-Other Awareness (EmpSOA). Specifically, three stages, self-other differentiation, self-other modulation and self-other generation, are devised to clearly maintain, regulate and inject the self-other aware information into the process of empathetic response generation. Both automatic and human evaluations on the benchmark dataset demonstrate the superiority of EmpSOA to generate more empathetic responses. {Our source code will be publicly available.}","anthology_url":"https://aclanthology.org/2023.findings-acl.843","authors":["Weixiang Zhao","Yanyan Zhao","Xin Lu","Bing Qin"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1670","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.843.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77515/poster_document/e31615f4e9240cfe8df946d773d254bb.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77515/slideshow/b8aa58dcb16460b3b7701f8f734ab387.pdf","title":"Don't Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness","tldr":"As a critical step to achieve human-like chatbots, empathetic response generation has attained increasing interests. Previous attempts are incomplete and not sufficient enough to elicit empathy because they only stay on the initial stage of empathy to automatically sense and simulate the feelings an...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77515,"underline_url":"https://underline.io/events/395/posters/15279/poster/77515-don-t-lose-yourself-empathetic-response-generation-via-explicit-self-other-awareness","video_url":null},{"abstract":"Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. \nExisting monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models.\nIn this paper, we revisit the classic multi-way structures and develop a detachable model by assigning each language (or group of languages) to an individual branch that supports plug-and-play training and inference. To address the needs of learning representations for all languages in a unified space, we propose a novel efficient training recipe, upon which we build an effective detachable model, Lego-MT.\nFor a fair comparison, we collect data from OPUS and build a translation benchmark covering 433 languages and 1.3B parallel data. \nExperiments show that Lego-MT with 1.2B parameters brings an average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters. \nThe proposed training recipe brings a 28.2$\\times$ speedup over the conventional multi-way training method.{code and data repo: {https://github.com/CONE-MT/Lego-MT.git}.}","anthology_url":"https://aclanthology.org/2023.findings-acl.731","authors":["Fei Yuan","Yinquan Lu","Wenhao Zhu","Lingpeng Kong","Lei Li","Yu Qiao","Jingjing Xu"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-7_-machine-translation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1676","is_paper":true,"keywords":["multilingual mt"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.731.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77517/poster_document/28314a5c8c83d44d8c725a5ca4cc0179.pdf","preview_image":"https://assets.underline.io/lecture/77517/poster/3a47b5fe9fa34085b231ee03b0addd1e.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77517/slideshow/ba266585c205787d6a496bd6de6eceb6.pptx","title":"Lego-MT: Learning Detachable Models for Massively Multilingual Machine Translation","tldr":"Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. \nExisting monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models.\nIn this paper, we revisit the classic multi-way...","track":"Machine Translation","underline_id":77517,"underline_url":"https://underline.io/events/395/posters/15279/poster/77517-lego-mt-learning-detachable-models-for-massively-multilingual-machine-translation","video_url":null},{"abstract":"Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less sensitive to the prompt design. Our code and pre-trained models are publicly available at https://github.com/DAMO-NLP-SG/SSTuning.","anthology_url":"https://aclanthology.org/2023.findings-acl.110","authors":["Chaoqun Liu","Wenxuan Zhang","Guizhen Chen","Xiaobao Wu","Anh Tuan Luu","Chip Hong Chang","Lidong Bing"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)"],"id":"P1678","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.110.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77518/poster_document/32a2af3910272fb2de69a644abebeeb1.pdf","preview_image":"https://assets.underline.io/lecture/77518/poster/02b481ef7ca44df0e446b4734e86af91.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77518/slideshow/fd9cb61110e72321a648b6136461b7d0.pptx","title":"Zero-Shot Text Classification via Self-Supervised Tuning","tldr":"Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised lea...","track":"NLP Applications","underline_id":77518,"underline_url":"https://underline.io/events/395/posters/15279/poster/77518-zero-shot-text-classification-via-self-supervised-tuning","video_url":null},{"abstract":"Aspect sentiment quad prediction (ASQP) is a challenging yet significant subtask in aspectbased sentiment analysis as it provides a complete aspect-level sentiment structure. However, existing ASQP datasets are usually small and low-density, hindering technical advancement. To expand the capacity, in this paper, we release two new datasets for ASQP, which contain the following characteristics: larger size, more words per sample, and higher density. With such datasets, we unveil the shortcomings of existing strong ASQP baselines and therefore propose a unified one-step solution for ASQP, namely One-ASQP, to detect the aspect categories and to identify the aspectopinion-sentiment (AOS) triplets simultaneously. Our One-ASQP holds several unique advantages: (1) by separating ASQP into two subtasks and solving them independently and simultaneously, we can avoid error propagation in pipeline-based methods and overcome slow training and inference in generation-based methods; (2) by introducing sentiment-specific horns tagging schema in a token-pair-based two-dimensional matrix, we can exploit deeper interactions between sentiment elements and efficiently decode the AOS triplets; (3) we design \"[NULL]\u201d token can help us effectively identify the implicit aspects or opinions. Experiments on two benchmark datasets and our released two datasets demonstrate the advantages of our One-ASQP. The two new datasets are publicly released at https://www.github.com/Datastory-CN/ASQP-Datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.777","authors":["Junxian Zhou","Haiqin Yang","Yuxuan He","Hao Mou","JunBo Yang"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1691","is_paper":true,"keywords":["argument mining"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.777.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77521/poster_document/0c52a7a44349b41ee34735db833ca53c.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"A Unified One-Step Solution for Aspect Sentiment Quad Prediction","tldr":"Aspect sentiment quad prediction (ASQP) is a challenging yet significant subtask in aspectbased sentiment analysis as it provides a complete aspect-level sentiment structure. However, existing ASQP datasets are usually small and low-density, hindering technical advancement. To expand the capacity, i...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77521,"underline_url":"https://underline.io/events/395/posters/15240/poster/77521-a-unified-one-step-solution-for-aspect-sentiment-quad-prediction","video_url":null},{"abstract":"We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data distribution. To capture the specific prompt for each aspect term in a few-shot scenario, we propose a novel Generative Multimodal Prompt (GMP) model for MABSA, which includes the Multimodal Encoder module and the N-Stream Decoders module. We further introduce a subtask to predict the number of aspect terms in each instance to construct the multimodal prompt.\nExtensive experiments on two datasets demonstrate that our approach outperforms strong baselines on two MABSA-related tasks in the few-shot setting.","anthology_url":"https://aclanthology.org/2023.findings-acl.735","authors":["Xiaocui Yang","Shi Feng","Daling Wang","Qi Sun","Wenfang Wu","Yifei Zhang","Pengfei Hong","Soujanya Poria"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1694","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.735.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77522/poster_document/5b29e454a0fde66f7da6ed2840288ce1.pdf","preview_image":"https://assets.underline.io/lecture/77522/poster/278049e7085738eb869b02e86d1104f6.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative Multimodal Prompt","tldr":"We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MAB...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77522,"underline_url":"https://underline.io/events/395/posters/15240/poster/77522-contrastive-training-improves-zero-shot-classification-of-semi-structured-documents","video_url":null},{"abstract":"This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore to find a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding function. Finally, we study how modularity emerges during pre-training, and find that the modular structure is stabilized at the early stage, which is faster than neuron stabilization. It suggests that Transformer first constructs the modular structure and then learns fine-grained neuron functions. Our code and data are available at https://github.com/THUNLP/modularity-analysis.","anthology_url":"https://aclanthology.org/2023.findings-acl.250","authors":["Zhengyan Zhang","Zhiyuan Zeng","Yankai Lin","Chaojun Xiao","Xiaozhi Wang","Xu Han","Zhiyuan Liu","Ruobing Xie","Maosong Sun","Jie Zhou"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P1715","is_paper":true,"keywords":["probing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.250.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77524/poster_document/a83edcba52389e209f0566364d7c5b60.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Emergent Modularity in Pre-trained Transformers","tldr":"This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate ...","track":"Interpretability and Analysis of Models for NLP","underline_id":77524,"underline_url":"https://underline.io/events/395/posters/15279/poster/77524-emergent-modularity-in-pre-trained-transformers","video_url":null},{"abstract":"Distant supervision reduces the reliance on human annotation in the named entity recognition tasks. The class-level imbalanced distant annotation is a realistic and unexplored problem, and the popular method of self-training can not handle class-level imbalanced learning. More importantly, self-training is dominated by the high-performance class in selecting candidates, and deteriorates the low-performance class with the bias of generated pseudo label. To address the class-level imbalance performance, we propose a class-rebalancing self-training framework for improving the distantly-supervised named entity recognition. In candidate selection, a class-wise flexible threshold is designed to fully explore other classes besides the high-performance class. In label generation, injecting the distant label, a hybrid pseudo label is adopted to provide straight semantic information for the low-performance class. Experiments on five flat and two nested datasets show that our model achieves state-of-the-art results. We also conduct extensive research to analyze the effectiveness of the flexible threshold and the hybrid pseudo label.","anthology_url":"https://aclanthology.org/2023.findings-acl.703","authors":["Qi Li","Tingyu Xie","Peng Peng","Hongwei Wang","Gaoang Wang"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-1_-information-extraction-(virtual-poster)"],"id":"P176","is_paper":true,"keywords":["named entity recognition and relation extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.703.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77277/poster_document/7fbcd5298d01f55dde4dfe3bf6f62fe0.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"A Class-Rebalancing Self-Training Framework for Distantly-Supervised Named Entity Recognition","tldr":"Distant supervision reduces the reliance on human annotation in the named entity recognition tasks. The class-level imbalanced distant annotation is a realistic and unexplored problem, and the popular method of self-training can not handle class-level imbalanced learning. More importantly, self-trai...","track":"Information Extraction","underline_id":77277,"underline_url":"https://underline.io/events/395/posters/15200/poster/77277-a-class-rebalancing-self-training-framework-for-distantly-supervised-named-entity-recognition","video_url":null},{"abstract":"With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models while only one candidate is kept as the summary output. In this paper, we propose to re-rank summary candidates in an unsupervised manner, aiming to close the performance gap between unsupervised and supervised models. Our approach improves the unsupervised PEGASUS by up to 7.27\\% and ChatGPT by up to 6.86\\% relative mean ROUGE across four widely-adopted summarization benchmarks ; and achieves relative gains of 7.51\\% (up to 23.73\\% from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on a dataset, evaluating on another).","anthology_url":"https://aclanthology.org/2023.findings-acl.529","authors":["Mathieu Ravaut","Shafiq Joty","Nancy Chen"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-1_-summarization-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1772","is_paper":true,"keywords":["abstractive summarisation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.529.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77528/poster_document/654fb999a205d7ad4f3f5cbb5037e528.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Unsupervised Summarization Re-ranking","tldr":"With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly...","track":"Summarization","underline_id":77528,"underline_url":"https://underline.io/events/395/posters/15200/poster/77528-unsupervised-summarization-re-ranking","video_url":null},{"abstract":"Most existing event causality identification (ECI) methods rarely consider the event causal label information and the interaction information between event pairs. In this paper, we propose a framework to enrich the representation of event pairs by introducing the event causal label information and the event pair interaction information. In particular, 1) we design an event-causal-label-aware module to model the event causal label information, in which we design the event causal label prediction task as an auxiliary task of ECI, aiming to predict which events are involved in the causal relationship (we call them causality-related events) by mining the dependencies between events. 2) We further design an event pair interaction graph module to model the interaction information between event pairs, in which we construct the interaction graph with event pairs as nodes and leverage graph attention mechanism to model the degree of dependency between event pairs. The experimental results show that our approach outperforms previous state-of-the-art methods on two benchmark datasets EventStoryLine and Causal-TimeBank.","anthology_url":"https://aclanthology.org/2023.findings-acl.655","authors":["Ruili Pu","Yang Li","Suge Wang","Deyu Li","Jianxing Zheng","Jian Liao"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-1_-information-extraction-(virtual-poster)"],"id":"P1780","is_paper":true,"keywords":["event extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.655.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77529/poster_document/4712a3f48b0352848490a45f6a172acd.pdf","preview_image":"https://assets.underline.io/lecture/77529/poster/681d1d270eec804665dd32988ab40bf3.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Enhancing Event Causality Identification with Event Causal Label and Event Pair Interaction Graph","tldr":"Most existing event causality identification (ECI) methods rarely consider the event causal label information and the interaction information between event pairs. In this paper, we propose a framework to enrich the representation of event pairs by introducing the event causal label information and t...","track":"Information Extraction","underline_id":77529,"underline_url":"https://underline.io/events/395/posters/15200/poster/77529-enhancing-event-causality-identification-with-event-causal-label-and-event-pair-interaction-graph","video_url":null},{"abstract":"Building robust deep neural networks (DNNs) against adversarial attacks is an important but challenging task. Previous defense approaches mainly focus on developing new model structures or training algorithms, but they do little to tap the potential of training instances, especially instances with robust patterns carring innate robustness. In this paper, we show that robust and non-robust instances in the training dataset, though are both important for test performance, have contrary impacts on robustness, which makes it possible to build a highly robust model by leveraging the training dataset in a more effective way. We propose a new method that can distinguish between robust instances from non-robust ones according to the model's sensitivity to perturbations on individual instances during training. Surprisingly, we find that the model under standard training easily overfits the robust instances by relying on their simple patterns before the model completely learns their robust features. Finally, we propose a new mitigation algorithm to further release the potential of robust instances. Experimental results show that proper use of robust instances in the original dataset is a new line to achieve highly robust models.","anthology_url":"https://aclanthology.org/2023.findings-acl.146","authors":["Rui Zheng","Zhiheng Xi","Qin Liu","Wenbin Lai","Tao Gui","Qi Zhang","Xuanjing Huang","Jin Ma","Ying Shan","Weifeng Ge"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1786","is_paper":true,"keywords":["adversarial attacks/examples/training","robustness"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.146.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77530/poster_document/8eec3cd1560ba50c17b5c0779cfc85dc.pdf","preview_image":"https://assets.underline.io/lecture/77530/poster/365e1f510ffa29ad7391d2dd604cdbdc.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Characterizing the Impacts of Instances on Robustness","tldr":"Building robust deep neural networks (DNNs) against adversarial attacks is an important but challenging task. Previous defense approaches mainly focus on developing new model structures or training algorithms, but they do little to tap the potential of training instances, especially instances with r...","track":"Interpretability and Analysis of Models for NLP","underline_id":77530,"underline_url":"https://underline.io/events/395/posters/15200/poster/77530-characterizing-the-impacts-of-instances-on-robustness","video_url":null},{"abstract":"Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works highlight that adversarial samples are even more vulnerable than normal ones. In this work, this phenomenon is illustrated frWe first show that adversarial samples locate in steep and narrow local minima of the loss landscape (high sharpness) while normal samples, which differs distinctly from adversarial ones, reside in the loss surface that is more flatter (low sharpness).om the perspective of sharpness via visualizing the input loss landscape of models.  Based on this, we propose a simple and effective sharpness-based detector to distinct adversarial samples by maximizing the loss increment within the region where the inference sample is located. Considering that the notion of sharpness of a loss landscape is relative, we further propose an adaptive optimization strategy in an attempt to fairly compare the relative sharpness among different samples. Experimental results show that our approach can outperform previous detection methods by large margins (average +6.6 F1 score) for four advanced attack strategies considered in this paper across three text classification tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.717","authors":["Rui Zheng","Shihan Dou","Yuhao Zhou","Qin Liu","Tao Gui","Qi Zhang","Zhongyu Wei","Xuanjing Huang","Menghan Zhang"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1798","is_paper":true,"keywords":["security/privacy"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.717.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77533/poster_document/daae424463975e4a7bbb2fb15fed35cb.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Detecting Adversarial Samples through Sharpness of Loss Landscape","tldr":"Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works highlight that adversarial samples are even more vulnerable than normal ones. In this work, this phenomenon is illustrated frWe first show that adversarial samples locate in steep ...","track":"NLP Applications","underline_id":77533,"underline_url":"https://underline.io/events/395/posters/15240/poster/77533-detecting-adversarial-samples-through-sharpness-of-loss-landscape","video_url":null},{"abstract":"Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further limits their adoption among non-expert users. In this paper, we propose a simple prompting strategy Help Me Think where we encourage large\nlanguage models (such as GPT3 and ChatGPT) to help non-expert users by asking a set of relevant questions and leveraging user answers to execute the task. We demonstrate the efficacy of our technique Help Me Think on a variety of tasks. Specifically, we focus on tasks that are hard for average humans and require significant thinking to perform. We hope our work will encourage the development of unconventional ways to harness the power of large language models.","anthology_url":"https://aclanthology.org/2023.findings-acl.751","authors":["Swaroop Mishra","Elnaz Nouri"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1801","is_paper":true,"keywords":["prompting"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.751.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77534/poster_document/8eb7d388f8fda1958ad3bd3161f79410.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models","tldr":"Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable m...","track":"Spotlight - Metropolitan Centre","underline_id":77534,"underline_url":null,"video_url":null},{"abstract":"Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news article and multiple target headlines. Toward this, we propose a novel model called DIVHSK. It has two components:\nKEYSELECT for selecting the important keywords, and SEQGEN, for finally generating the multiple diverse headlines. In KEYSELECT, we cluster the self-attention heads of the last layer of the pre-trained encoder and select the most-attentive theme and general keywords from the source article. Then, cluster-specific keyword sets guide the SEQGEN, a pre-trained encoder-decoder model, to generate diverse yet semantically similar headlines. The proposed model consistently outperformed existing literature and our strong baselines and emerged as a state-of-the-art model. We have also created a high-quality multi-reference headline dataset from news articles.","anthology_url":"https://aclanthology.org/2023.findings-acl.118","authors":["Venkatesh E","Kaushal Kumar Maurya","Deepak Kumar","Maunendra Sankar Desarkar"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-4_-generation-(virtual-poster)"],"id":"P1810","is_paper":true,"keywords":["text-to-text generation","model architectures"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.118.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77535/poster_document/d669d5d3e8d46610a7b8362681e6a3e8.pdf","preview_image":"https://assets.underline.io/lecture/77535/poster/d00a2759b25c7424c398d4bd1590e279.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77535/slideshow/0e83c24602059b41d789ac62c1826896.pdf","title":"DivHSK: Diverse Headline Generation using Self-Attention based Keyword Selection","tldr":"Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news...","track":"Generation","underline_id":77535,"underline_url":"https://underline.io/events/395/posters/15240/poster/77535-divhsk-diverse-headline-generation-using-self-attention-based-keyword-selection","video_url":null},{"abstract":"User and product information associated with a review is useful for sentiment polarity prediction. Typical approaches incorporating such information focus on modeling users and products as implicitly learned representation vectors. Most do not exploit the potential of historical reviews, or those that currently do require unnecessary modifications to model architecture\nor do not make full use of user/product associations. The contribution of this work is twofold: i) a method to explicitly employ historical reviews belonging to the same user/product in initializing representations, and ii) efficient incorporation of textual associations between users and products via a user-product cross-context module. Experiments on the IMDb, Yelp-2013 and Yelp-2014 English benchmarks with BERT, SpanBERT and Longformer pretrained language models show that our approach substantially outperforms previous state-of-the-art.","anthology_url":"https://aclanthology.org/2023.findings-acl.92","authors":["Chenyang Lyu","Linyi Yang","Yue Zhang","Yvette Graham","Jennifer Foster"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)"],"id":"P1816","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.92.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77536/poster_document/20aa84fbb4270e9ac191f245dfaae7b1.pdf","preview_image":"https://assets.underline.io/lecture/77536/poster/ccc2abd9a0d52d4763b4b179e3b19c12.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77536/slideshow/0f2dc38acbe7d644ba08941c5c6acaec.pdf","title":"Exploiting Rich Textual User-Product Context for Improving Personalized Sentiment Analysis","tldr":"User and product information associated with a review is useful for sentiment polarity prediction. Typical approaches incorporating such information focus on modeling users and products as implicitly learned representation vectors. Most do not exploit the potential of historical reviews, or those th...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77536,"underline_url":"https://underline.io/events/395/posters/15279/poster/77536-stop-pre-training-adapt-visual-language-models-to-unseen-languages","video_url":null},{"abstract":"We identify the robust overfitting issue for pre-trained language models by showing that the robust test loss increases as the epoch grows. Through comprehensive exploration of the robust loss on the training set, we attribute robust overfitting to the model's memorization of the adversarial training data. \nWe attempt to mitigate robust overfitting by combining regularization methods with adversarial training. \nFollowing the philosophy that prevents the model from memorizing the adversarial data, we find that flooding, a regularization method with loss scaling, can mitigate robust overfitting for pre-trained language models. Eventually, we investigate the effect of flooding levels and evaluate the models' adversarial robustness under textual attacks. Extensive experiments demonstrate that our methods can mitigate robust overfitting upon three top adversarial training methods and further promote adversarial robustness.","anthology_url":"https://aclanthology.org/2023.findings-acl.340","authors":["Bin Zhu","Yanghui Rao"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)"],"id":"P1823","is_paper":true,"keywords":["adversarial training"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.340.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77537/poster_document/e30287312bcfb66d4284a9a5cd03a5d3.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Exploring Robust Overfitting for Pre-trained Language Models","tldr":"We identify the robust overfitting issue for pre-trained language models by showing that the robust test loss increases as the epoch grows. Through comprehensive exploration of the robust loss on the training set, we attribute robust overfitting to the model's memorization of the adversarial trainin...","track":"Machine Learning for NLP","underline_id":77537,"underline_url":"https://underline.io/events/395/posters/15279/poster/77537-exploring-robust-overfitting-for-pre-trained-language-models","video_url":null},{"abstract":"Developing monolingual large Pre-trained Language Models (PLMs) is shown to be very successful in handling different tasks in Natural Language Processing (NLP). In this work, we present AraMUS, the  largest Arabic PLM with 11B parameters trained on 529GB of high-quality Arabic textual data. AraMUS achieves state-of-the-art performances on a diverse set of Arabic classification and generative tasks. Moreover, AraMUS shows impressive few-shot learning abilities compared with the best existing Arabic PLMs.","anthology_url":"https://aclanthology.org/2023.findings-acl.181","authors":["Asaad Alghamdi","Xinyu Duan","Wei Jiang","Zhenhai Wang","Yimeng Wu","Qingrong Xia","Zhefeng Wang","Yi ZHENG","Mehdi Rezagholizadeh","baoxing Huai","Peilun Cheng","Abbas Ghaddar"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-4_-large-language-models-(virtual-poster)"],"id":"P1834","is_paper":true,"keywords":["pre-training"],"languages":["arabic"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.181.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77539/poster/02a2a48e449772ca21c4bf9b64290d09.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing","tldr":"Developing monolingual large Pre-trained Language Models (PLMs) is shown to be very successful in handling different tasks in Natural Language Processing (NLP). In this work, we present AraMUS, the  largest Arabic PLM with 11B parameters trained on 529GB of high-quality Arabic textual data. AraMUS a...","track":"Large Language Models","underline_id":77539,"underline_url":"https://underline.io/events/395/posters/15240/poster/77539-aramus-pushing-the-limits-of-data-and-model-scale-for-arabic-natural-language-processing","video_url":null},{"abstract":"Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the errors propagated from the retriever, but also demands additional effort to develop both the retriever and the reader, which further makes it slower since they are not runnable in parallel. In this work, we propose a method to directly predict answers with a phrase retrieval scheme for a sequence of words, reducing the conventional two distinct subtasks into a single one. Also, for the first time, we study its capability for ODConvQA tasks. However, simply adopting it is largely problematic, due to the dependencies between previous and current turns in a conversation. To address this problem, we further introduce a novel contrastive learning strategy, making sure to reflect previous turns when retrieving the phrase for the current context, by maximizing representational similarities of consecutive turns in a conversation while minimizing irrelevant conversational contexts. We validate our model on two ODConvQA datasets, whose experimental results show that it substantially outperforms the relevant baselines with the retriever-reader. Code is available at: https://github.com/starsuzi/PRO-ConvQA.","anthology_url":"https://aclanthology.org/2023.findings-acl.374","authors":["Soyeong Jeong","Jinheon Baek","Sung Ju Hwang","Jong Park"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1847","is_paper":true,"keywords":["conversational qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.374.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77541/poster_document/d88574516190a0831db5676b6e2ddcf8.pdf","preview_image":"https://assets.underline.io/lecture/77541/poster/3bab5300735b1ee2068e755d487f153f.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77541/slideshow/5cbfae63300854745f44a2b14ff2f8d0.pdf","title":"Phrase Retrieval for Open Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning","tldr":"Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the err...","track":"Question Answering","underline_id":77541,"underline_url":"https://underline.io/events/395/posters/15240/poster/77541-phrase-retrieval-for-open-domain-conversational-question-answering-with-conversational-dependency-modeling-via-contrastive-learning","video_url":null},{"abstract":"Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods.","anthology_url":"https://aclanthology.org/2023.findings-acl.61","authors":["Sukmin Cho","Soyeong Jeong","Jeong yeon Seo","Jong Park"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-1_-information-retrieval-and-text-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1857","is_paper":true,"keywords":["re-ranking"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.61.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77543/poster_document/7f2fd02ce2b8c188a8e97199e065ec17.pdf","preview_image":"https://assets.underline.io/lecture/77543/poster/186dcdb613494bd1cb4f2676f5c5d67f.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77543/slideshow/b49d2d0b3aed3aba4d04e787edca9a13.pdf","title":"Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker","tldr":"Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker w...","track":"Information Retrieval and Text Mining","underline_id":77543,"underline_url":"https://underline.io/events/395/posters/15200/poster/77543-discrete-prompt-optimization-via-constrained-generation-for-zero-shot-re-ranker","video_url":null},{"abstract":"In the deployment of real-world text classification models, label scarcity is a common problem and as the number of classes increases, this problem becomes even more complex. An approach to addressing this problem is by applying text augmentation methods.\n\nOne of the more prominent methods involves using the text-generation capabilities of language models. In this paper, we propose Text AUgmentation by Dataset Reconstruction (TAU-DR), a novel method of data augmentation for text classification. We conduct experiments on several multi-class datasets, showing that our approach improves the current state-of-the-art techniques for data augmentation.","anthology_url":"https://aclanthology.org/2023.findings-acl.466","authors":["Adir Rahamim","Guy Uziel","Esther Goldbraich","Ateret Anaby Tavor"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P187","is_paper":true,"keywords":["data augmentation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.466.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77279/poster_document/22893ee657052df8afe9acd8a09378a0.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Text Augmentation Using Dataset Reconstruction for Low-Resource Classification","tldr":"In the deployment of real-world text classification models, label scarcity is a common problem and as the number of classes increases, this problem becomes even more complex. An approach to addressing this problem is by applying text augmentation methods.\n\nOne of the more prominent methods involves ...","track":"Machine Learning for NLP","underline_id":77279,"underline_url":"https://underline.io/events/395/posters/15240/poster/77279-text-augmentation-using-dataset-reconstruction-for-low-resource-classification","video_url":null},{"abstract":"We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEval 2022 idiomatic semantic text similarity task, and show that it outperforms all unsupervised systems and rivals supervised systems.","anthology_url":"https://aclanthology.org/2023.findings-acl.290","authors":["Takashi Wada","Yuji Matsumoto","Timothy Baldwin","Jey Han Lau"],"category":"Findings","demo_url":null,"display_track":"Semantics: Lexical","event_ids":["session-1_-semantics_-lexical-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1876","is_paper":true,"keywords":["multi-word expressions","paraphrasing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.290.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77547/poster/97fd2956b74493245204221efa00fd63.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77547/slideshow/6fc6620b9da744a0143e39e7e987c824.pdf","title":"Unsupervised Paraphrasing of Multiword Expressions","tldr":"We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEv...","track":"Semantics: Lexical","underline_id":77547,"underline_url":"https://underline.io/events/395/posters/15200/poster/77547-few-shot-reranking-for-multi-hop-qa-via-language-model-prompting","video_url":null},{"abstract":"When scoring argumentative essays in an educational context, not only the presence or absence of certain argumentative elements but also their quality is important. \nOn the recently published student essay dataset PERSUADE, we first show that the automatic scoring of argument quality benefits from additional information about context, writing prompt and argument type. We then explore the different combinations of three tasks: automated span detection, type and quality prediction. Results show that a multi-task learning approach combining the three tasks outperforms sequential approaches that first learn to segment and then predict the quality/type of a segment.","anthology_url":"https://aclanthology.org/2023.findings-acl.825","authors":["Yuning Ding","Marie Bexte","Andrea Horbach"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-1_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P1890","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.825.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77552/poster_document/6531d288fafcc52ed482151b8b05539a.pdf","preview_image":"https://assets.underline.io/lecture/77552/poster/ca323a0b8cdf5f6c26ae508a6f43f319.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77552/slideshow/f11daad3970c451d5669c4f9833630c6.pdf","title":"Score It All Together: A Multi-Task Learning Study on Automatic Scoring of Argumentative Essays","tldr":"When scoring argumentative essays in an educational context, not only the presence or absence of certain argumentative elements but also their quality is important. \nOn the recently published student essay dataset PERSUADE, we first show that the automatic scoring of argument quality benefits from a...","track":"NLP Applications","underline_id":77552,"underline_url":"https://underline.io/events/395/posters/15200/poster/77552-transfer-and-active-learning-for-dissonance-detection-addressing-the-rare-class-challenge","video_url":null},{"abstract":"Well pre-trained contextualized representations from pre-trained language model (PLM) have been shown helpful for enhancing various natural language processing tasks, surely including neural machine translation (NMT). However, existing methods either consider encoder-only enhancement or rely on specific multilingual PLMs, which leads to a much larger model or give up potentially helpful knowledge from target PLMs.  In this paper, we propose a new monolingual PLM-sponsored NMT model to let both encoder and decoder enjoy PLM enhancement to alleviate such obvious inconvenience. Especially, incorporating a newly proposed frequency-weighted embedding transformation algorithm, PLM embeddings can be effectively exploited in terms of the representations of the NMT decoder. We evaluate our model on IWSLT14 En-De, De-En, WMT14 En-De, and En-Fr tasks, and the results show that our proposed PLM enhancement gives significant improvement and even helps achieve new state-of-the-art.","anthology_url":"https://aclanthology.org/2023.findings-acl.222","authors":["Sufeng Duan","Hai Zhao"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-4_-machine-translation-(virtual-poster)"],"id":"P1922","is_paper":true,"keywords":["pre-training for mt"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.222.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77556/poster_document/5942552ab298ddf178224cb6d3674a1d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Encoder and Decoder, Not One Less for Pre-trained Language Model Sponsored NMT","tldr":"Well pre-trained contextualized representations from pre-trained language model (PLM) have been shown helpful for enhancing various natural language processing tasks, surely including neural machine translation (NMT). However, existing methods either consider encoder-only enhancement or rely on spec...","track":"Machine Translation","underline_id":77556,"underline_url":"https://underline.io/events/395/posters/15240/poster/77556-continual-knowledge-distillation-for-neural-machine-translation","video_url":null},{"abstract":"kNN-MT presents a new paradigm for domain adaptation by building an external datastore, which usually saves all target language token occurrences in the parallel corpus. As a result, the constructed datastore is usually large and possibly redundant. In this paper, we investigate the interpretability issue of this approach: what knowledge does the NMT model need? We propose the notion of local correctness (LAC) as a new angle, which describes the potential translation correctness for a single entry and for a given neighborhood. Empirical study shows that our investigation successfully finds the conditions where the NMT model could easily fail and need related knowledge. Experiments on six diverse target domains and two language-pairs show that pruning according to local correctness brings a light and more explainable memory for kNN-MT domain adaptation.","anthology_url":"https://aclanthology.org/2023.findings-acl.177","authors":["Wenhao Zhu","Shujian Huang","Yunzhe Lv","Xin Zheng","Jiajun CHEN"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-4_-machine-translation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1934","is_paper":true,"keywords":["domain adaptation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.177.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77557/poster_document/3e2b97f9ad7469b04c14fe69e361db31.pdf","preview_image":"https://assets.underline.io/lecture/77557/poster/616b73c5fc75a2304ea41f9640378d06.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77557/slideshow/c3e667cf0c1f4ad76fc6f319382ee356.pdf","title":"What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation","tldr":"kNN-MT presents a new paradigm for domain adaptation by building an external datastore, which usually saves all target language token occurrences in the parallel corpus. As a result, the constructed datastore is usually large and possibly redundant. In this paper, we investigate the interpretability...","track":"Machine Translation","underline_id":77557,"underline_url":"https://underline.io/events/395/posters/15240/poster/77557-what-knowledge-is-neededquestion-towards-explainable-memory-for-knn-mt-domain-adaptation","video_url":null},{"abstract":"Pretrained multilingual Transformers have achieved great success in cross-lingual transfer learning. Current methods typically activate the cross-lingual transferability of multilingual Transformers by fine-tuning them on end-task data. However, the methods cannot perform cross-lingual transfer when end-task data are unavailable. In this work, we explore whether the cross-lingual transferability can be activated without end-task data. We propose a cross-lingual transfer method, named PlugIn-X. PlugIn-X disassembles monolingual and multilingual Transformers into sub-modules, and reassembles them to be the multilingual end-task model. After representation adaptation, PlugIn-X finally performs cross-lingual transfer in a plug-and-play style. Experimental results show that PlugIn-X successfully activates the cross-lingual transferability of multilingual Transformers without accessing end-task data. Moreover, we analyze how the cross-model representation alignment affects the cross-lingual transferability.","anthology_url":"https://aclanthology.org/2023.findings-acl.796","authors":["Zewen Chi","Heyan Huang","Xian-Ling Mao"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1936","is_paper":true,"keywords":["cross-lingual transfer"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.796.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77558/poster_document/cc4e9b7630f63dde63fa4b6c252fb48a.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Can Cross-Lingual Transferability of Multilingual Transformers Be Activated Without End-Task Data?","tldr":"Pretrained multilingual Transformers have achieved great success in cross-lingual transfer learning. Current methods typically activate the cross-lingual transferability of multilingual Transformers by fine-tuning them on end-task data. However, the methods cannot perform cross-lingual transfer when...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77558,"underline_url":"https://underline.io/events/395/posters/15279/poster/77558-can-cross-lingual-transferability-of-multilingual-transformers-be-activated-without-end-task-dataquestion","video_url":null},{"abstract":"Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as task embeddings, resulting in substantial computational complexity and storage requirements. In this work, we draw inspiration from the operating mechanism of deep neural networks (DNNs) and biological brains, where neuronal activations are sparse and task-specific, and we use the connectivity patterns of neurons as a unique identifier associated with the task. The proposed method learns to assign importance masks for sub-structures of DNNs, and accordingly indicate the task-specific connectivity patterns. In addition to the storage advantages brought by the binary masking mechanism and structured sparsity, the early-bird nature of the sparse optimization process can deliver an efficient computation advantage. Experiments show that our method consistently outperforms other baselines in predicting inter-task transferability across data regimes and transfer settings, while keeping high efficiency in computation and storage.","anthology_url":"https://aclanthology.org/2023.findings-acl.759","authors":["Zhiheng Xi","Rui Zheng","Yuansen Zhang","Xuanjing Huang","Zhongyu Wei","Minlong Peng","Mingming Sun","Qi Zhang","Tao Gui"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)"],"id":"P1937","is_paper":true,"keywords":["transfer learning / domain adaptation","model compression methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.759.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77559/poster_document/fe891ec87291d4bd9b71820965eec64e.pdf","preview_image":"https://assets.underline.io/lecture/77559/poster/c678e8f0754e78562517590772fcd123.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Connectivity Patterns are Task Embeddings","tldr":"Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as task...","track":"Machine Learning for NLP","underline_id":77559,"underline_url":"https://underline.io/events/395/posters/15240/poster/77559-reinforced-active-learning-for-low-resource-domain-specific-multi-label-text-classification","video_url":null},{"abstract":"Compositional generalization is a basic mechanism in human language\n  learning, which current neural networks struggle with.  A recently\n  proposed Disentangled sequence-to-sequence model\n  (Dangle) shows promising generalization capability by learning\n  specialized encodings for each decoding step. We introduce two key\n  modifications to this model which encourage more disentangled\n  representations and improve its compute and memory efficiency,\n  allowing us to tackle compositional generalization in a more\n  realistic setting.  Specifically, instead of adaptively re-encoding\n  source keys and values at each time step, we disentangle their\n  representations and only re-encode keys periodically, at some\n  interval.  Our new architecture leads to better generalization\n  performance across existing tasks and datasets, and a new machine\n  translation benchmark which we create by\n  detecting naturally occurring compositional patterns in\n  relation to a training set. We show this methodology  better emulates real-world\n  requirements than artificial challenges.","anthology_url":"https://aclanthology.org/2023.findings-acl.108","authors":["Hao Zheng","Mirella Lapata"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)"],"id":"P1939","is_paper":true,"keywords":["generalization"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.108.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning","tldr":"Compositional generalization is a basic mechanism in human language\n  learning, which current neural networks struggle with.  A recently\n  proposed Disentangled sequence-to-sequence model\n  (Dangle) shows promising generalization capability by learning\n  specialized encodings for each decoding step....","track":"Machine Learning for NLP","underline_id":77560,"underline_url":"https://underline.io/events/395/posters/15279/poster/77560-real-world-compositional-generalization-with-disentangled-sequence-to-sequence-learning","video_url":null},{"abstract":"Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models' cross-lingual transfer ability. We design a semantic contrastive loss to align representations of parallel sentences that share the same semantics in different languages, and a new language contrastive loss to leverage parallel sentence pairs to remove language-specific information in sentence representations from non-parallel corpora. When trained on English IR data with these losses and evaluated zero-shot on non-English data, our model demonstrates significant improvement to prior work on retrieval performance, while it requires much less computational effort. We also demonstrate the value of our model for a practical setting when a parallel corpus is only available for a few languages, but a lack of parallel corpora resources persists for many other low-resource languages. Our model can work well even with a small number of parallel sentences, and be used as an add-on module to any backbones and other tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.581","authors":["Xiyang Hu","Xinchi Chen","Peng Qi","Deguang Kong","Kunlun Liu","William Yang Wang","zhiheng huang"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P194","is_paper":true,"keywords":["cross-lingual transfer"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.581.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77280/poster_document/96c978214151cc094553cd0ce063642c.pdf","preview_image":"https://assets.underline.io/lecture/77280/poster/9c4dbdc1f24f6aa112c8c881be8d2891.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77280/slideshow/105818c8dbc324bbe77ac61d17aa839a.pdf","title":"Language Agnostic Multilingual Information Retrieval with Contrastive Learning","tldr":"Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. ...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77280,"underline_url":"https://underline.io/events/395/posters/15279/poster/77280-language-agnostic-multilingual-information-retrieval-with-contrastive-learning","video_url":null},{"abstract":"The machine reading comprehension (MRC) of user manuals has huge potential in customer service. However, current methods have trouble answering complex questions. Therefore, we introduce the knowing-how \\& knowing-that task that requires the model to answer factoid-style, procedure-style, and inconsistent questions about user manuals. We resolve this task by jointly representing the sTeps and fActs in a gRAh (TARA), which supports a unified inference of various questions. Towards a systematical benchmarking study, we design a heuristic method to automatically parse user manuals into TARAs and build an annotated dataset to test the model's ability in answering real-world questions. Empirical results demonstrate that representing user manuals as TARAs is a desired solution for the MRC of user manuals. An in-depth investigation of TARA further sheds light on the issues and broader impacts of future representations of user manuals. We hope our work can move the MRC of user manuals to a more complex and realistic stage.","anthology_url":"https://aclanthology.org/2023.findings-acl.671","authors":["Hongru Liang","Jia Liu","Weihong Du","Dingnan Jin","Wenqiang Lei","Zujie Wen","Jiancheng Lv"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-7_-question-answering-(virtual-poster)"],"id":"P1952","is_paper":true,"keywords":["reading comprehension"],"languages":["chinese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.671.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77561/poster_document/b7acf84f49a2c5d2bbd106e07804ca7c.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Knowing-how & Knowing-that: A New Task for Machine Comprehension of User Manuals","tldr":"The machine reading comprehension (MRC) of user manuals has huge potential in customer service. However, current methods have trouble answering complex questions. Therefore, we introduce the knowing-how \\& knowing-that task that requires the model to answer factoid-style, procedure-style, and incons...","track":"Question Answering","underline_id":77561,"underline_url":"https://underline.io/events/395/posters/15279/poster/77561-knowing-how-and-knowing-that-a-new-task-for-machine-comprehension-of-user-manuals","video_url":null},{"abstract":"Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are better causal reasoners. We further intervene on the prompts from different aspects, and discover that the key point is the programming structure. Code and data are available at https://github.com/xxxiaol/magic-if.","anthology_url":"https://aclanthology.org/2023.findings-acl.574","authors":["Xiao Liu","Da Yin","Chen Zhang","Yansong Feng","Dongyan Zhao"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-1_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1959","is_paper":true,"keywords":["prompting","interpretability/analysis","applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.574.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77564/poster_document/878fba9be392e28437a99ba1913bbf76.pdf","preview_image":"https://assets.underline.io/lecture/77564/poster/2934dcaf67d052db241bb4d1f902974f.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77564/slideshow/8c1f1be92382b8baaf75a557049b9303.pdf","title":"The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code","tldr":"Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given th...","track":"Large Language Models","underline_id":77564,"underline_url":"https://underline.io/events/395/posters/15200/poster/77564-the-magic-of-if-investigating-causal-reasoning-abilities-in-large-language-models-of-code","video_url":null},{"abstract":"Knowledge-based referring expression comprehension (KB-REC) aims to identify visual objects referred to by expressions that incorporate knowledge. Existing methods employ sentence-level retrieval and fusion methods, which may lead to issues of similarity bias and interference from irrelevant information in unstructured knowledge sentences. To address these limitations, we propose a segment-level and category-oriented network (SLCO). Our approach includes a segment-level and prompt-based knowledge retrieval method to mitigate the similarity bias problem and a category-based grounding method to alleviate interference from irrelevant information in knowledge sentences. Experimental results show that our SLCO can eliminate interference and improve the overall performance of the KB-REC task.","anthology_url":"https://aclanthology.org/2023.findings-acl.557","authors":["Yuqi Bu","Xin Wu","Liuwu Li","Yi Cai","Qiong Liu","Qingbao Huang"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)"],"id":"P1977","is_paper":true,"keywords":["cross-modal application"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.557.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77567/poster_document/434b7b3fd192b4b108a6012384ca6ab4.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Segment-Level and Category-Oriented Network for Knowledge-Based Referring Expression Comprehension","tldr":"Knowledge-based referring expression comprehension (KB-REC) aims to identify visual objects referred to by expressions that incorporate knowledge. Existing methods employ sentence-level retrieval and fusion methods, which may lead to issues of similarity bias and interference from irrelevant informa...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77567,"underline_url":"https://underline.io/events/395/posters/15200/poster/77567-segment-level-and-category-oriented-network-for-knowledge-based-referring-expression-comprehension","video_url":null},{"abstract":"We present SkillQG: a question generation framework with controllable comprehension types for assessing and improving machine reading comprehension models. \nExisting question generation systems widely differentiate questions by literal information such as question words and answer types to generate semantically relevant questions for a given context.\nHowever, they rarely consider the comprehension nature of questions, i.e., the different comprehension capabilities embodied by different questions.\nIn comparison, our SkillQG is able to tailor a fine-grained assessment and improvement to the capabilities of questions answering models built on it.\nSpecifically, we first frame the comprehension type of questions based on a hierarchical skill-based schema.\nWe then formulate SkillQG as a skill-conditioned question generator.\nFurthermore, to improve the controllability of generation, we augment the input text with skill-specific question focus and knowledge, which are constructed by iteratively prompting the pre-trained language models.\nEmpirical results demonstrate that SkillQG outperforms baselines in terms of quality, relevance, and skill-controllability while showing a promising performance boost in downstream question answering task.","anthology_url":"https://aclanthology.org/2023.findings-acl.870","authors":["Xiaoqiang Wang","Bang Liu","Siliang Tang","Lingfei Wu"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)"],"id":"P1985","is_paper":true,"keywords":["reading comprehension","question generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.870.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77568/poster_document/6091d349526515c14ae119bb84bb350c.pdf","preview_image":"https://assets.underline.io/lecture/77568/poster/c9697e66ffbba459739320dc7c4fdccd.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77568/slideshow/d0c2ffd8b6f883f3d1abeb2bab7df791.pdf","title":"SkillQG: Learning to Generate Question for Reading Comprehension Assessment","tldr":"We present SkillQG: a question generation framework with controllable comprehension types for assessing and improving machine reading comprehension models. \nExisting question generation systems widely differentiate questions by literal information such as question words and answer types to generate ...","track":"Question Answering","underline_id":77568,"underline_url":"https://underline.io/events/395/posters/15240/poster/77568-using-neural-machine-translation-for-generating-diverse-challenging-exercises-for-language-learner","video_url":null},{"abstract":"In the wake of responsible AI, interpretability methods, which attempt to provide an explanation for the predictions of neural models have seen rapid progress. In this work, we are concerned with explanations that are applicable to natural language processing (NLP) models and tasks, and we focus specifically on the analysis of counterfactual, contrastive explanations. We note that while there have been several explainers proposed to produce counterfactual explanations, their behaviour can vary significantly and the lack of a universal ground truth for the counterfactual edits imposes an insuperable barrier on their evaluation. We propose a new back translation-inspired evaluation methodology that utilises earlier outputs of the explainer as ground truth proxies to investigate the consistency of explainers. We show that by iteratively feeding the counterfactual to the explainer we can obtain valuable insights into the behaviour of both the predictor and the explainer models, and infer patterns that would be otherwise obscured. Using this methodology, we conduct a thorough analysis and propose a novel metric to evaluate the consistency of counterfactual generation approaches with different characteristics across available performance indicators.","anthology_url":"https://aclanthology.org/2023.findings-acl.606","authors":["George Filandrianos","Edmund G Dervakos","Orfeas Menis Mastromichalakis","Chrysoula Zerva","Giorgos Stamou"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P1996","is_paper":true,"keywords":["counterfactual/contrastive explanations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.606.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77569/poster_document/1d198c30bec098b82eff058ff587f72e.pdf","preview_image":"https://assets.underline.io/lecture/77569/poster/a16eb5a206c0d69ff567b4613514b24e.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77569/slideshow/5767681be6ae2f9f1f28290021282b0b.key","title":"Counterfactuals of Counterfactuals: a back-translation-inspired approach to analyse counterfactual editors","tldr":"In the wake of responsible AI, interpretability methods, which attempt to provide an explanation for the predictions of neural models have seen rapid progress. In this work, we are concerned with explanations that are applicable to natural language processing (NLP) models and tasks, and we focus spe...","track":"Interpretability and Analysis of Models for NLP","underline_id":77569,"underline_url":"https://underline.io/events/395/posters/15279/poster/77569-counterfactuals-of-counterfactuals-a-back-translation-inspired-approach-to-analyse-counterfactual-editors","video_url":null},{"abstract":"We extend a non-parametric Bayesian model of (Titov and Klementiev, 2011) to deal with homonymy and polysemy by leveraging distributed contextual word and phrase representations pre-trained on a large collection of unlabelled texts. Then, unsupervised semantic parsing is performed by decomposing sentences into fragments, clustering the fragments to abstract away syntactic variations of the same meaning, and predicting predicate-argument relations between the fragments. To better model the statistical dependencies between predicates and their arguments, we further conduct a hierarchical Pitman-Yor process. An improved Metropolis-Hastings merge-split sampler is proposed to speed up the mixing and convergence of Markov chains by leveraging pre-trained distributed representations. The experimental results show that the models achieve better accuracy on both question-answering and relation extraction tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.726","authors":["Zixuan Ling","Xiaoqing Zheng","Jianhan Xu","Jinshu Lin","Kai-Wei Chang","Cho-Jui Hsieh","Xuanjing Huang"],"category":"Findings","demo_url":null,"display_track":"Syntax: Tagging, Chunking, and Parsing","event_ids":["session-7_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)"],"id":"P202","is_paper":true,"keywords":["semantic parsing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.726.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77281/poster_document/1aee14ab7bc597aaf8ad7ee15e9a5caf.pdf","preview_image":"https://assets.underline.io/lecture/77281/poster/3936c88b7e6b9a8e6d130250779338c5.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77281/slideshow/0632b33e8eb0eb069784d0bd2aeec157.pdf","title":"Enhancing Unsupervised Semantic Parsing with Distributed Contextual Representations","tldr":"We extend a non-parametric Bayesian model of (Titov and Klementiev, 2011) to deal with homonymy and polysemy by leveraging distributed contextual word and phrase representations pre-trained on a large collection of unlabelled texts. Then, unsupervised semantic parsing is performed by decomposing sen...","track":"Syntax: Tagging, Chunking, and Parsing","underline_id":77281,"underline_url":"https://underline.io/events/395/posters/15279/poster/77281-hucurl-human-induced-curriculum-discovery","video_url":null},{"abstract":"Languages are dynamic entities, where the meanings associated with words constantly change with time.\nDetecting the semantic variation of words is an important task for various NLP applications that must make time-sensitive predictions.\nExisting work on semantic variation prediction have predominantly focused on comparing some form of an averaged contextualised representation of a target word computed from a given corpus.\nHowever, some of the previously associated meanings of a target word can become obsolete over time (e.g. meaning of gay as happy), while novel usages of existing words are observed (e.g. meaning of cell as a mobile phone).\nWe argue that mean representations alone cannot accurately capture such semantic variations and propose a method that uses the entire cohort of the contextualised embeddings of the target word, which we refer to as the sibling distribution.\nExperimental results on SemEval-2020 Task 1 benchmark dataset for semantic variation prediction show that our method outperforms prior work that consider only the mean embeddings, and is comparable to the current state-of-the-art. \nMoreover, a qualitative analysis shows that our method detects important semantic changes in words that are not captured by the existing methods.","anthology_url":"https://aclanthology.org/2023.findings-acl.429","authors":["Taichi Aida","Danushka Bollegala"],"category":"Findings","demo_url":null,"display_track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","event_ids":["session-7_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2021","is_paper":true,"keywords":["linguistic theories"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.429.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77575/poster_document/fdd032a9bae73564f616937f8473f6ff.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77575/slideshow/36f9db5e41bf0fee5c88ca218e4c5c67.pdf","title":"Unsupervised Semantic Variation Prediction using the Distribution of Sibling Embeddings","tldr":"Languages are dynamic entities, where the meanings associated with words constantly change with time.\nDetecting the semantic variation of words is an important task for various NLP applications that must make time-sensitive predictions.\nExisting work on semantic variation prediction have predominant...","track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","underline_id":77575,"underline_url":"https://underline.io/events/395/posters/15279/poster/77575-unsupervised-semantic-variation-prediction-using-the-distribution-of-sibling-embeddings","video_url":null},{"abstract":"Annotator disagreement is common whenever human judgment is needed for supervised learning. It is conventional to assume that one label per item represents ground truth. However, this obscures minority opinions, if present. We regard \"ground truth'' as the distribution of all labels that a population of annotators could produce, if asked (and of which we only have a small sample). We next introduce DisCo (Distribution from Context), a simple neural model that learns to predict this distribution. The model takes annotator-item pairs, rather than items alone, as input, and performs inference by aggregating over all annotators. Despite its simplicity, our experiments show that, on six benchmark datasets, our model is competitive with, and frequently outperforms, other, more complex models that either do not model specific annotators or were not designed for label distribution learning.","anthology_url":"https://aclanthology.org/2023.findings-acl.287","authors":["Tharindu Cyril Weerasooriya","Alexander Ororbia","Raj B Bhensadadia","Ashiqur KhudaBukhsh","Christopher Homan"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-4_-ethics-and-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2024","is_paper":true,"keywords":["model bias/fairness evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.287.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Disagreement Matters: Preserving Label Diversity by Jointly Modeling Item and Annotator Label Distributions with DisCo","tldr":"Annotator disagreement is common whenever human judgment is needed for supervised learning. It is conventional to assume that one label per item represents ground truth. However, this obscures minority opinions, if present. We regard \"ground truth'' as the distribution of all labels that a populatio...","track":"Ethics and NLP","underline_id":77577,"underline_url":"https://underline.io/events/395/posters/15240/poster/77577-disagreement-matters-preserving-label-diversity-by-jointly-modeling-item-and-annotator-label-distributions-with-disco","video_url":null},{"abstract":"Sentiment analysis (SA) systems are used in many products and hundreds of languages. Gender and racial biases are well-studied in English SA systems, but understudied in other languages, with few resources for such studies. To remedy this, we build a counterfactual evaluation corpus for gender and racial/migrant bias in four languages. We demonstrate its usefulness by answering a simple but important question that an engineer might need to answer when deploying a system: What biases do systems import from pre-trained models when compared to a baseline with no pre-training? Our evaluation corpus, by virtue of being counterfactual, not only reveals which models have less bias, but also pinpoints changes in model bias behaviour, which enables more targeted mitigation strategies. We release our code and evaluation corpora to facilitate future research.","anthology_url":"https://aclanthology.org/2023.findings-acl.272","authors":["Seraphina Goldfarb-Tarrant","Adam Lopez","Roi Blanco","Diego Marcheggiani"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-1_-ethics-and-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2039","is_paper":true,"keywords":["model bias/fairness evaluation"],"languages":["spanish","german","japanese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.272.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77580/poster_document/968647466c6e382390f89b6ae328de35.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages","tldr":"Sentiment analysis (SA) systems are used in many products and hundreds of languages. Gender and racial biases are well-studied in English SA systems, but understudied in other languages, with few resources for such studies. To remedy this, we build a counterfactual evaluation corpus for gender and r...","track":"Ethics and NLP","underline_id":77580,"underline_url":"https://underline.io/events/395/posters/15200/poster/77580-bias-beyond-english-counterfactual-tests-for-bias-in-sentiment-analysis-in-four-languages","video_url":null},{"abstract":"Multi-Task Learning used with pre-trained models has been quite popular in the field of Natural Language Processing in recent years. This framework remains still challenging due to the complexity of the tasks and the challenges associated with fine-tuning large pre-trained models. In this paper, we propose a new approach for Multi-task learning which is based on stacking the weights of Neural Networks as a tensor. We show that low-rank updates in the canonical polyadic tensor decomposition of this tensor of weights lead to a simple, yet efficient algorithm, which without loss of performance allows to reduce considerably the model parameters. We investigate the interactions between tasks inside the model as well as the inclusion of sparsity to find the best tensor rank and to increase the compression rate. Our strategy is consistent with recent efforts that attempt to use constraints to fine-tune some model components. More precisely, we achieve equivalent performance as the state-of-the-art on the General Language Understanding Evaluation benchmark by training only 0.3 of the parameters per task while not modifying the baseline weights.","anthology_url":"https://aclanthology.org/2023.findings-acl.476","authors":["Alexandre Daniel AUDIBERT","Massih R Amini","Konstantin Usevich","Marianne Clausel"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2048","is_paper":true,"keywords":["multi-task learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.476.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77582/slideshow/b5b51c4a09d40ade4063521b70e1c256.pdf","title":"Low-Rank Updates of pre-trained Weights for Multi-Task Learning","tldr":"Multi-Task Learning used with pre-trained models has been quite popular in the field of Natural Language Processing in recent years. This framework remains still challenging due to the complexity of the tasks and the challenges associated with fine-tuning large pre-trained models. In this paper, we ...","track":"Machine Learning for NLP","underline_id":77582,"underline_url":"https://underline.io/events/395/posters/15240/poster/77582-low-rank-updates-of-pre-trained-weights-for-multi-task-learning","video_url":null},{"abstract":"Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored task. In this work, we propose a coherent dialogue planning approach that uses a stochastic process to model the temporal dynamics of dialogue paths. We define a latent space that captures the coherence of goal-directed behavior using a Brownian bridge process, which allows us to incorporate user feedback flexibly in dialogue planning. Based on the derived latent trajectories, we generate dialogue paths explicitly using pre-trained language models. We finally employ these paths as natural language prompts to guide dialogue generation. Our experiments show that our approach generates more coherent utterances and achieves the goal with a higher success rate.","anthology_url":"https://aclanthology.org/2023.findings-acl.25","authors":["Jian Wang","Dongding Lin","Wenjie Li"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-4_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2057","is_paper":true,"keywords":["applications","grounded dialog","conversational modeling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.25.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77584/poster_document/ff857de9f7f0b515a5e4e20bca0a2d96.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77584/slideshow/2b16f3b10d728aa235f340a8606b0165.pdf","title":"Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue","tldr":"Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored tas...","track":"Dialogue and Interactive Systems","underline_id":77584,"underline_url":"https://underline.io/events/395/posters/15240/poster/77584-dialogue-planning-via-brownian-bridge-stochastic-process-for-goal-directed-proactive-dialogue","video_url":null},{"abstract":"Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive learning, the encoding of long documents (Ks of words) is still challenging with respect to both efficiency and quality considerations. Therefore, we train Longfomer-based document encoders using a state-of-the-art unsupervised contrastive learning method (SimCSE). Further on, we complement the baseline method -siamese neural network- with additional convex neural networks based on functional Bregman divergence aiming to enhance the quality of the output document representations. We show that overall the combination of a self-contrastive siamese network and our proposed neural Bregman network outperforms the baselines in two linear classification settings on three long document topic classification tasks from the legal and biomedical domains.","anthology_url":"https://aclanthology.org/2023.findings-acl.771","authors":["Daniel Saggau","Mina Rezaei","Bernd Bischl","Ilias Chalkidis"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)"],"id":"P2080","is_paper":true,"keywords":["contrastive learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.771.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77585/poster_document/6a680dc93de324d29326055efda06714.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77585/slideshow/04bc550e7d3a54369ba8893dceb97987.pdf","title":"Efficient Document Embeddings via Self-Contrastive  Bregman Divergence Learning","tldr":"Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive le...","track":"Machine Learning for NLP","underline_id":77585,"underline_url":"https://underline.io/events/395/posters/15279/poster/77585-efficient-document-embeddings-via-self-contrastive-bregman-divergence-learning","video_url":null},{"abstract":"Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks.\nIn this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets---two in the legal domain and two in the biomedical domain, each with two levels of label granularity--- and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. \nUsing encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths.","anthology_url":"https://aclanthology.org/2023.findings-acl.360","authors":["Yova Kementchedjhieva","Ilias Chalkidis"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2092","is_paper":true,"keywords":["healthcare applications, clincial nlp"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.360.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77586/slideshow/9fe13e46e570cbeb8baf4136e59aaf0c.pdf","title":"An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text","tldr":"Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks.\nIn this study, we compare four methods for multi-label classification, two based on an encoder only,...","track":"NLP Applications","underline_id":77586,"underline_url":"https://underline.io/events/395/posters/15279/poster/77586-an-exploration-of-encoder-decoder-approaches-to-multi-label-classification-for-legal-and-biomedical-text","video_url":null},{"abstract":"Sense embedding learning methods learn multiple vectors for a given ambiguous word, corresponding to its different word senses.\nFor this purpose, different methods have been proposed in prior work on sense embedding learning that use different sense inventories, sense-tagged corpora and learning methods.\nHowever, not all existing sense embeddings cover all senses of ambiguous words equally well due to the discrepancies in their training resources.\nTo address this problem, we propose the first-ever meta-sense embedding method -- Neighbour Preserving Meta-Sense Embeddings, which learns meta-sense embeddings by combining multiple independently trained source sense embeddings such that the sense neighbourhoods computed from the source embeddings are preserved in the meta-embedding space.\nOur proposed method can combine source sense embeddings that cover different sets of word senses.\nExperimental results on Word Sense Disambiguation (WSD) and Word-in-Context (WiC) tasks show that the proposed meta-sense embedding method consistently outperforms several competitive baselines.{An anonymised version of the source code implementation for our proposed method is submitted to reviewing system. \nBoth source code and the learnt meta-sense embeddings will be publicly released upon paper acceptance.","anthology_url":"https://aclanthology.org/2023.findings-acl.165","authors":["Haochen Luo","Yi Zhou","Danushka Bollegala"],"category":"Findings","demo_url":null,"display_track":"Semantics: Lexical","event_ids":["session-7_-semantics_-lexical-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2095","is_paper":true,"keywords":["word embeddings"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.165.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77587/poster_document/a30fe25a8f7d0a5cbb655b01668c18a5.pdf","preview_image":"https://assets.underline.io/lecture/77587/poster/e5cfc92b2f2b895e58e0499eaf5fb61c.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77587/slideshow/32579d541d22de1a211d37cdbb761d8b.pdf","title":"Together We Make Sense--Learning Meta-Sense Embeddings","tldr":"Sense embedding learning methods learn multiple vectors for a given ambiguous word, corresponding to its different word senses.\nFor this purpose, different methods have been proposed in prior work on sense embedding learning that use different sense inventories, sense-tagged corpora and learning met...","track":"Semantics: Lexical","underline_id":77587,"underline_url":"https://underline.io/events/395/posters/15279/poster/77587-together-we-make-sense-learning-meta-sense-embeddings","video_url":null},{"abstract":"A hallmark of human intelligence is the ability to learn new concepts purely from language. Several recent approaches have explored training machine learning models via natural language supervision. However, these approaches fall short in leveraging linguistic quantifiers (such as \u2018always' or \u2018rarely') and mimicking humans in compositionally learning complex tasks. Here, we present LaSQuE, a method that can learn zero-shot classifiers from language explanations by using three new strategies - (1) modeling the semantics of linguistic quantifiers in explanations (including exploiting ordinal strength relationships, such as \u2018always' > \u2018likely'), (2) aggregating information from multiple explanations using an attention-based mechanism, and (3) model training via curriculum learning. With these strategies, LaSQuE outperforms prior work, showing an absolute gain of up to 7\\% in generalizing to unseen real-world classification tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.467","authors":["Sayan Ghosh","Rakesh R. Menon","Shashank Srivastava"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)"],"id":"P2098","is_paper":true,"keywords":["few-shot learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.467.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77589/poster_document/48af9e0bda4b81cee9bd96db6307c7fe.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"LaSQuE: Improved Zero-Shot Classification from Explanations Through Quantifier Modeling and Curriculum Learning","tldr":"A hallmark of human intelligence is the ability to learn new concepts purely from language. Several recent approaches have explored training machine learning models via natural language supervision. However, these approaches fall short in leveraging linguistic quantifiers (such as \u2018always' or \u2018rarel...","track":"Machine Learning for NLP","underline_id":77589,"underline_url":"https://underline.io/events/395/posters/15240/poster/77589-lasque-improved-zero-shot-classification-from-explanations-through-quantifier-modeling-and-curriculum-learning","video_url":null},{"abstract":"Dynamic topic models (DTMs) analyze text streams to capture the evolution of topics. Despite their popularity, existing DTMs are either fully supervised, requiring expensive human annotations, or fully unsupervised, producing topic evolutions that often do not cater to a user's needs. Further, the topic evolutions produced by DTMs tend to contain generic terms that are not indicative of their designated time steps. To address these issues, we propose the task of discriminative dynamic topic discovery. This task aims to discover topic evolutions from temporal corpora that distinctly align with a set of user-provided category names and uniquely capture topics at each time step. We solve this task by developing DynaMiTE, a framework that ensembles semantic similarity, category indicative, and time indicative scores to produce informative topic evolutions. Through experiments on three diverse datasets, including the use of a newly-designed human evaluation experiment, we demonstrate that DynaMiTE is a practical and efficient framework for helping users discover high-quality topic evolutions suited to their interests.","anthology_url":"https://aclanthology.org/2023.findings-acl.14","authors":["Nishant Balepur","Shivam Agarwal","Karthik Venkat Ramanan","Susik Yoon","Diyi Yang","Jiawei Han"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-7_-information-retrieval-and-text-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2105","is_paper":true,"keywords":["document representation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.14.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77590/poster_document/d319274e5062ba85c24657f40105667f.pdf","preview_image":"https://assets.underline.io/lecture/77590/poster/d66fdc8011ac4bbded488e7569a15f73.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77590/slideshow/90113e587afc420d5192257bc7a2c216.pdf","title":"DynaMiTE: Discovering Explosive Topic Evolutions with User Guidance","tldr":"Dynamic topic models (DTMs) analyze text streams to capture the evolution of topics. Despite their popularity, existing DTMs are either fully supervised, requiring expensive human annotations, or fully unsupervised, producing topic evolutions that often do not cater to a user's needs. Further, the t...","track":"Information Retrieval and Text Mining","underline_id":77590,"underline_url":"https://underline.io/events/395/posters/15279/poster/77590-dynamite-discovering-explosive-topic-evolutions-with-user-guidance","video_url":null},{"abstract":"Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping the model's raw behavior. However, existing work only studies the monolingual scenario, which lacks the cross-lingual transferability to perform editing simultaneously across languages. In this work, we focus on cross-lingual model editing. Firstly, we define the cross-lingual model editing task and corresponding metrics, where an edit in one language propagates to the others. Next, we propose a framework to naturally adapt monolingual model editing approaches to the cross-lingual scenario using parallel corpus. Further, we propose language anisotropic editing to improve cross-lingual editing by amplifying different subsets of parameters for each language. On the newly defined cross-lingual model editing task, we empirically demonstrate the failure of monolingual baselines in propagating the edit to multiple languages and the effectiveness of the proposed language anisotropic model editing. Our code is publicly available at https://github.com/franklear/LiME.","anthology_url":"https://aclanthology.org/2023.findings-acl.343","authors":["Yang Xu","Yutai Hou","Wanxiang Che","Min Zhang"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)"],"id":"P2106","is_paper":true,"keywords":["cross-lingual transfer"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.343.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77591/poster_document/19006c89a0ca5ec3216dfb8813f705ba.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Language Anisotropic Cross-Lingual Model Editing","tldr":"Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping ...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77591,"underline_url":"https://underline.io/events/395/posters/15279/poster/77591-language-anisotropic-cross-lingual-model-editing","video_url":null},{"abstract":"Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words~CITATION.\nThis similarity underestimation problem is particularly severe for high frequent words.\nAlthough this problem has been noted in prior work, no solution has been proposed thus far.\nWe observe that the $\\ell_2$ norm of contextualised embeddings of a word correlates with its log-frequency in the pretraining corpus.\nConsequently, the larger $\\ell_2$ norms associated with the high frequent words reduce the cosine similarity values measured between them, thus underestimating the similarity scores.\nTo solve this issue, we propose a method to \\emph{discount} the $\\ell_2$ norm of a contextualised word embedding by the frequency of that word in a corpus when measuring the cosine similarities between words.\nWe show that the so called \\emph{stop} words behave differently from the rest of the words, which require special consideration during their discounting process.\nExperimental results on a contextualised word similarity dataset show that our proposed discounting method accurately solves the similarity underestimation problem.{An anonymized version of the source code of our proposed method is submitted to the reviewing system.","anthology_url":"https://aclanthology.org/2023.findings-acl.550","authors":["Saeth Wannasuphoprasit","Yi Zhou","Danushka Bollegala"],"category":"Findings","demo_url":null,"display_track":"Semantics: Lexical","event_ids":["session-1_-semantics_-lexical-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2112","is_paper":true,"keywords":["word embeddings"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.550.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77592/poster/1fbb2c55cbb347b4628df8506cd77e1c.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77592/slideshow/55c9d431b82268e060ec9957c2657453.pdf","title":"Solving Cosine Similarity Underestimation between High Frequency Words by $\\ell_2$ Norm Discounting","tldr":"Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words~CITATION.\nThis similarity underestimation problem is particularly severe for high fr...","track":"Semantics: Lexical","underline_id":77592,"underline_url":"https://underline.io/events/395/posters/15200/poster/77592-solving-cosine-similarity-underestimation-between-high-frequency-words-by-dashell_2-norm-discounting","video_url":null},{"abstract":"Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-$p$) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability $p$. \nIn this work, we assess whether a top-$p$ set is indeed aligned with its probabilistic meaning in various linguistic contexts.\nWe employ conformal prediction, a calibration procedure that focuses on the construction of minimal prediction sets according to a desired confidence level, to calibrate the parameter $p$ as a function of the entropy of the next word distribution. We find that OPT models are overconfident, and that calibration shows a moderate inverse scaling with model size.","anthology_url":"https://aclanthology.org/2023.findings-acl.3","authors":["Shauli Ravfogel","Yoav Goldberg","Jacob Goldberger"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2127","is_paper":true,"keywords":["calibration/uncertainty"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.3.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77593/poster_document/afa05748761d30a7ccf2023dc655fe67.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Conformal Nucleus Sampling","tldr":"Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-$p$) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability $p$. \nIn this work, we assess whether a top-$p$ set is indeed alig...","track":"Interpretability and Analysis of Models for NLP","underline_id":77593,"underline_url":"https://underline.io/events/395/posters/15240/poster/77593-conformal-nucleus-sampling","video_url":null},{"abstract":"Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than thousands of data points, DePlot+LLM with just one-shot prompting achieves a 29.4\\% improvement over finetuned SOTA on human-written queries from the task of chart QA.","anthology_url":"https://aclanthology.org/2023.findings-acl.660","authors":["Fangyu Liu","Julian Martin Eisenschlos","Francesco Piccinno","Syrine Krichene","Chenxi Pang","Kenton Lee","Mandar Joshi","Wenhu Chen","Nigel Collier","Yasemin Altun"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2133","is_paper":true,"keywords":["multimodal qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.660.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77594/poster_document/06d0a8b0a189ef3ee1f0dfee9b67d216.pdf","preview_image":"https://assets.underline.io/lecture/77594/poster/392f9e7375a9168653351d26137b7d68.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77594/slideshow/7c14e230b0da6fb8119b1ae16c2a8623.pdf","title":"DePlot: One-shot visual language reasoning by plot-to-table translation","tldr":"Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially...","track":"Question Answering","underline_id":77594,"underline_url":"https://underline.io/events/395/posters/15240/poster/77594-structsp-efficient-fine-tuning-of-task-oriented-dialog-system-by-using-structure-aware-boosting-and-grammar-constraints","video_url":null},{"abstract":"In recent years, joint Vision-Language (VL) models have increased in popularity and capability. Very few studies have attempted to investigate bias in VL models, even though it is a well-known issue in both individual modalities.\nThis paper presents the first multi-dimensional analysis of bias in English VL models, focusing on gender, ethnicity, and age as dimensions.\nWhen subjects are input as images, pre-trained VL models complete a neutral template with a hurtful word 5\\% of the time, with higher percentages for female and young subjects.\nBias presence in downstream models has been tested on Visual Question Answering. We developed a novel bias metric called the Vision-Language Association Test based on questions designed to elicit biased associations between stereotypical concepts and targets. Our findings demonstrate that pre-trained VL models contain biases that are perpetuated in downstream tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.403","authors":["Gabriele Ruggeri","Debora Nozza"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-1_-ethics-and-nlp-(virtual-poster)"],"id":"P2145","is_paper":true,"keywords":["model bias/fairness evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.403.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77595/poster/b2fdb93966578db54134809786d53143.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77595/slideshow/bc69c743bcf8355dcff17a71fbab88ff.pdf","title":"A Multi-dimensional study on Bias in Vision-Language models","tldr":"In recent years, joint Vision-Language (VL) models have increased in popularity and capability. Very few studies have attempted to investigate bias in VL models, even though it is a well-known issue in both individual modalities.\nThis paper presents the first multi-dimensional analysis of bias in En...","track":"Ethics and NLP","underline_id":77595,"underline_url":"https://underline.io/events/395/posters/15200/poster/77595-a-multi-dimensional-study-on-bias-in-vision-language-models","video_url":null},{"abstract":"Deep learning has made significant progress in the past decade, and demonstrates potential to solve problems with extensive social impact. In high-stakes decision making areas such as law, experts often require interpretability for automatic systems to be utilized in practical settings. In this work, we attempt to address these requirements applied to the important problem of legal citation prediction (LCP). We design the task with parallels to the thought-process of lawyers, i.e., with reference to both precedents and legislative provisions. After initial experimental results, we refine the target citation predictions with the feedback of legal experts. Additionally, we introduce a prototype architecture to add interpretability, achieving strong performance while adhering to decision parameters used by lawyers. Our study builds on and leverages the state-of-the-art language processing models for law, while addressing vital considerations for high-stakes tasks with practical societal impact.","anthology_url":"https://aclanthology.org/2023.findings-acl.301","authors":["Chu Fei Luo","Rohan Bhambhoria","Samuel Dahan","Xiaodan Zhu"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)"],"id":"P2178","is_paper":true,"keywords":["legal nlp"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.301.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77599/poster_document/2386c93abb7dff93fae079493f6778a9.pdf","preview_image":"https://assets.underline.io/lecture/77599/poster/f03592de1d0618bb283cc8c2142e3819.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77599/slideshow/21624bcf3a4b88d5f85ab1b1189a3ab0.pdf","title":"Prototype-Based Interpretability for Legal Citation Prediction","tldr":"Deep learning has made significant progress in the past decade, and demonstrates potential to solve problems with extensive social impact. In high-stakes decision making areas such as law, experts often require interpretability for automatic systems to be utilized in practical settings. In this work...","track":"NLP Applications","underline_id":77599,"underline_url":"https://underline.io/events/395/posters/15240/poster/77599-prototype-based-interpretability-for-legal-citation-prediction","video_url":null},{"abstract":"Automated essay scoring (AES) aims to score essays written for a given prompt, which defines the writing topic. Most existing AES systems assume to grade essays of the same prompt as used in training and assign only a holistic score. However, such settings conflict with real-education situations; pre-graded essays for a particular prompt are lacking, and detailed trait scores of sub-rubrics are required. Thus, predicting various trait scores of unseen-prompt essays (called cross-prompt essay trait scoring) is a remaining challenge of AES. In this paper, we propose a robust model: prompt- and trait relation-aware cross-prompt essay trait scorer. We encode prompt-aware essay representation by essay-prompt attention and utilizing the topic-coherence feature extracted by the topic-modeling mechanism without access to labeled data; therefore, our model considers the prompt adherence of an essay, even in a cross-prompt setting. To facilitate multi-trait scoring, we design trait-similarity loss that encapsulates the correlations of traits. Experiments prove the efficacy of our model, showing state-of-the-art results for all prompts and traits. Significant improvements in low-resource-prompt and inferior traits further indicate our model's strength.","anthology_url":"https://aclanthology.org/2023.findings-acl.98","authors":["Heejin Do","Yunsu Kim","Gary Geunbae Lee"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2187","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.98.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77600/poster_document/60d3e38071282df86462a89a3e313821.pdf","preview_image":"https://assets.underline.io/lecture/77600/poster/9ac5408bb56de28bf86d53232c304ce1.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77600/slideshow/0daff31ee7596d50bc57f39a7a7b1f9a.pdf","title":"Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring","tldr":"Automated essay scoring (AES) aims to score essays written for a given prompt, which defines the writing topic. Most existing AES systems assume to grade essays of the same prompt as used in training and assign only a holistic score. However, such settings conflict with real-education situations; pr...","track":"NLP Applications","underline_id":77600,"underline_url":"https://underline.io/events/395/posters/15279/poster/77600-prompt-and-trait-relation-aware-cross-prompt-essay-trait-scoring","video_url":null},{"abstract":"Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to\nassign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar \ndata can substantially improve the performance of MIAs.\nHowever, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks.","anthology_url":"https://aclanthology.org/2023.findings-acl.719","authors":["Justus Mattern","Fatemehsadat Mireshghallah","Zhijing Jin","Bernhard Schoelkopf","Mrinmaya Sachan","Taylor Berg-Kirkpatrick"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-1_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2197","is_paper":true,"keywords":["security and privacy"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.719.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77602/poster_document/a765b80b7f8f301ded768c1dcb532c88.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77602/slideshow/0aa9bed955b2bebcabfe403c310f3c1e.pdf","title":"Membership Inference Attacks against Language Models via Neighbourhood Comparison","tldr":"Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to\nassign higher probab...","track":"Large Language Models","underline_id":77602,"underline_url":"https://underline.io/events/395/posters/15200/poster/77602-membership-inference-attacks-against-language-models-via-neighbourhood-comparison","video_url":null},{"abstract":"Large language models (LLMs) have exhibited remarkable capabilities in learning from expla- nations in prompts, but there has been limited understanding of exactly how these explana- tions function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two dif- ferent factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By per- turbing explanations on three controlled tasks, we show that both factors contribute to the ef- fectiveness of explanations. We further study how to form maximally effective sets of expla- nations for solving a given test query. We find that LLMs can benefit from the complemen- tarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as comple- mentary, which successfully improves the in- context learning performance across three real- world tasks on multiple LLMs.","anthology_url":"https://aclanthology.org/2023.findings-acl.273","authors":["Xi Ye","Srinivasan Iyer","Asli Celikyilmaz","Veselin Stoyanov","Greg Durrett","Ramakanth Pasunuru"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-1_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2204","is_paper":true,"keywords":["prompting","interpretability/analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.273.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77603/poster_document/936406bdb3c2860c0441ab069d94d0a8.pdf","preview_image":"https://assets.underline.io/lecture/77603/poster/ddc8b03af936064c22eb54aaa9ba5997.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Complementary Explanations for Effective In-Context Learning","tldr":"Large language models (LLMs) have exhibited remarkable capabilities in learning from expla- nations in prompts, but there has been limited understanding of exactly how these explana- tions function or why they are effective. This work aims to better understand the mechanisms by which explanations ar...","track":"Large Language Models","underline_id":77603,"underline_url":"https://underline.io/events/395/posters/15200/poster/77603-complementary-explanations-for-effective-in-context-learning","video_url":null},{"abstract":"A simile is a figure of speech that compares two different things (called the tenor and the vehicle) via shared properties. The tenor and the vehicle are usually connected with comparator words such as \"like\" or \"as\". The simile phenomena are unique and complex in a real-life dialogue scene where the tenor and the vehicle can be verbal phrases or sentences, mentioned by different speakers, exist in different sentences, or occur in reversed order. However, the current simile research usually focuses on similes in a triplet tuple (tenor, property, vehicle) or a single sentence where the tenor and vehicle are usually entities or noun phrases, which could not reflect complex simile phenomena in real scenarios. In this paper, we propose a novel and high-quality multilingual simile dialogue (MSD) dataset to facilitate the study of complex simile phenomena. The MSD is the largest manually annotated simile data ($\\sim$21K) and it contains both English and Chinese data. Meanwhile, the MSD data can also be used on dialogue tasks to test the ability of dialogue systems when using similes. We design 3 simile tasks (recognition, interpretation, and generation) and 2 dialogue tasks (retrieval and generation) with MSD. For each task, we provide experimental results from strong pre-trained or state-of-the-art models. The experiments demonstrate the challenge of MSD and we will release the data/code on GitHub.","anthology_url":"https://aclanthology.org/2023.findings-acl.453","authors":["Longxuan Ma","Wei-Nan Zhang","Shuhan Zhou","churui sun","Changxin Ke","Ting Liu"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-1_-resources-and-evaluation-(virtual-poster)"],"id":"P2221","is_paper":true,"keywords":["nlp datasets"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.453.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77607/poster_document/0d8302a4c734fc437430e30659e5d40d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77607/slideshow/c55d43c08be3a19f4ddb7aea186d8f01.pdf","title":"I run as fast as a rabbit, can you? A Multilingual Simile Dialogues Datasets","tldr":"A simile is a figure of speech that compares two different things (called the tenor and the vehicle) via shared properties. The tenor and the vehicle are usually connected with comparator words such as \"like\" or \"as\". The simile phenomena are unique and complex in a real-life dialogue scene where th...","track":"Resources and Evaluation","underline_id":77607,"underline_url":"https://underline.io/events/395/posters/15200/poster/77607-i-run-as-fast-as-a-rabbit-can-youquestion-a-multilingual-simile-dialogues-datasets","video_url":null},{"abstract":"A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong.","anthology_url":"https://aclanthology.org/2023.findings-acl.389","authors":["Amr Keleg","Walid Magdy"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)"],"id":"P2223","is_paper":true,"keywords":["multilingual benchmarks","multilingual evaluation"],"languages":["arabic","korean"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.389.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77608/poster_document/c8a5419db1b4e17c9ae2676568a31cd5.pdf","preview_image":"https://assets.underline.io/lecture/77608/poster/d24dc0e06cc749a0aabd356ea23c493c.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77608/slideshow/d88f4b5f33882e11adb26ed84bb79665.pdf","title":"DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models","tldr":"A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilin...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77608,"underline_url":"https://underline.io/events/395/posters/15240/poster/77608-dlama-a-framework-for-curating-culturally-diverse-facts-for-probing-the-knowledge-of-pretrained-language-models","video_url":null},{"abstract":"Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? We used such considerations to devise three criteria for our new protocol: Specification Polarity, Specification Importance, and Domain Transferability. As a case study, we apply our protocol to a popular debiasing method, Self-Debiasing, and compare it to  one we propose, called Instructive Debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. We show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked.","anthology_url":"https://aclanthology.org/2023.findings-acl.280","authors":["Robert A. Morabito","Jad Kabbara","Ali Emami"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2238","is_paper":true,"keywords":["model bias/fairness evaluation","model bias/unfairness mitigation","ethical considerations in nlp applications","transparency"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.280.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77610/poster_document/2f8eb7599d894d547cb44270173acc23.pdf","preview_image":"https://assets.underline.io/lecture/77610/poster/3adfb62239b711b2e767b01aab41f6fb.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77610/slideshow/46fc11bf881de517ef1e0d9e4ad5b1e0.pptx","title":"Debiasing should be Good and Bad: Measuring the Consistency of Debiasing  Techniques in Language Models","tldr":"Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent...","track":"Ethics and NLP","underline_id":77610,"underline_url":"https://underline.io/events/395/posters/15279/poster/77610-debiasing-should-be-good-and-bad-measuring-the-consistency-of-debiasing-techniques-in-language-models","video_url":null},{"abstract":"Although we have witnessed impressive progress in Semantic Role Labeling (SRL), most of the research in the area is carried out assuming that the majority of predicates are verbs.\nConversely, predicates can also be expressed using other parts of speech, e.g., nouns and adjectives.\nHowever, non-verbal predicates appear in the benchmarks we commonly use to measure progress in SRL less frequently than in some real-world settings -- newspaper headlines, dialogues, and tweets, among others.\nIn this paper, we put forward a new PropBank dataset which boasts wide coverage of multiple predicate types. Thanks to it, we demonstrate empirically that standard benchmarks do not provide an accurate picture of the current situation in SRL and that state-of-the-art systems are still incapable of transferring knowledge across different predicate types.\nHaving observed these issues, we also present a novel, manually-annotated challenge set designed to give equal importance to verbal, nominal, and adjectival predicate-argument structures. We use such dataset to investigate whether we can leverage different linguistic resources to promote knowledge transfer.\nIn conclusion, we claim that SRL is far from \"solved\", and its integration with other semantic tasks might enable significant improvements in the future, especially for the long tail of non-verbal predicates, thereby facilitating further research on SRL for non-verbal predicates.\nWe release our software and datasets at https://github.com/sapienzanlp/exploring-srl.","anthology_url":"https://aclanthology.org/2023.findings-acl.783","authors":["Riccardo Orlando","Simone Conia","Roberto Navigli"],"category":"Findings","demo_url":null,"display_track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","event_ids":["session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2240","is_paper":true,"keywords":["semantic textual similarity"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.783.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77612/poster_document/aebfd39fe3be6fb92b51544a4673b20c.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77612/slideshow/92d6d9eb26daad55fd4c7fb837b64828.pdf","title":"Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and Opportunities","tldr":"Although we have witnessed impressive progress in Semantic Role Labeling (SRL), most of the research in the area is carried out assuming that the majority of predicates are verbs.\nConversely, predicates can also be expressed using other parts of speech, e.g., nouns and adjectives.\nHowever, non-verba...","track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","underline_id":77612,"underline_url":"https://underline.io/events/395/posters/15200/poster/77612-exploring-non-verbal-predicates-in-semantic-role-labeling-challenges-and-opportunities","video_url":null},{"abstract":"Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT.\nHeterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute -- where different amounts of computations are used for different tokens in the input. Adaptivity comes naturally from routing decisions which send tokens to experts of different sizes. AutoMoE code, data, and trained models are available at https://aka.ms/AutoMoE.","anthology_url":"https://aclanthology.org/2023.findings-acl.580","authors":["Ganesh Jawahar","Subhabrata Mukherjee","Xiaodong Liu","Young Jin Kim","Muhammad Abdul-Mageed","Laks Lakshmanan, V.S.","Ahmed Hassan Awadallah","Sebastien Bubeck","Jianfeng Gao"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2243","is_paper":true,"keywords":["model compression methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.580.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77613/poster_document/36bcb1fe58744aee1e088b0eb7c029dd.pdf","preview_image":"https://assets.underline.io/lecture/77613/poster/8dd91a831c8d0eb876fc32899d76efd4.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77613/slideshow/abd0968353880944ae846d805bdb10b1.pdf","title":"AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation","tldr":"Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE wor...","track":"Machine Learning for NLP","underline_id":77613,"underline_url":"https://underline.io/events/395/posters/15200/poster/77613-automoe-heterogeneous-mixture-of-experts-with-adaptive-computation-for-efficient-neural-machine-translation","video_url":null},{"abstract":"In task-oriented dialogue (TOD) systems designed to aid users accomplish specific goals in one or more domains, the agent retrieves entities that satisfy user constraints from the database. However, when multiple database search results exist, an ambiguity occurs regarding which results to select and present to the user. Existing TOD systems handle this ambiguity by randomly selecting one or few results and presenting their names to the user. However, in a real scenario, users do not always accept a randomly recommended entity, and users should have access to more comprehensive information about the search results. To address this limitation, we propose a novel task called Comparison-Based database search Ambiguity handling (CBA), which handles ambiguity in database search results by comparing the properties of multiple entities to enable users to choose according to their preferences. Accordingly, we introduce a new framework for automatically collecting high-quality dialogue data along with the Disambiguating Schema-guided Dialogue (DSD) dataset, an augmented version of the SGD dataset. Experimental studies on the DSD dataset demonstrate that training baseline models with the dataset effectively address the CBA task. Our dataset and code will be publicized.","anthology_url":"https://aclanthology.org/2023.findings-acl.249","authors":["Yongil Kim","Yerin Hwang","Joongbo Shin","Hyunkyung Bae","Kyomin Jung"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-7_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2251","is_paper":true,"keywords":["task-oriented"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.249.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77614/poster_document/fbb53de0a2ab62d453c297bf07d89bd6.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77614/slideshow/94f0eac2d067c546f24ab21b8f2fa5f6.pdf","title":"Injecting Comparison Skills in Task-Oriented Dialogue Systems for Database Search Results Disambiguation","tldr":"In task-oriented dialogue (TOD) systems designed to aid users accomplish specific goals in one or more domains, the agent retrieves entities that satisfy user constraints from the database. However, when multiple database search results exist, an ambiguity occurs regarding which results to select an...","track":"Dialogue and Interactive Systems","underline_id":77614,"underline_url":"https://underline.io/events/395/posters/15279/poster/77614-injecting-comparison-skills-in-task-oriented-dialogue-systems-for-database-search-results-disambiguation","video_url":null},{"abstract":"Document retrieval is a key stage of standard Web search engines. \nExisting dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. \nTo overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\nHowever, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases.\nIn this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval.\n\nSurprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions.\nIn this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers.\nExperiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings.\nThe code for this work can be found at {https://github.com/Ziems/llm-url}.","anthology_url":"https://aclanthology.org/2023.findings-acl.167","authors":["Noah Ziems","Wenhao Yu","Zhihan Zhang","Meng Jiang"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-7_-information-retrieval-and-text-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2269","is_paper":true,"keywords":["passage retrieval","dense retrieval","document representation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.167.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77616/poster_document/67d83395143c19a8076ae18b9c7d5ad9.pdf","preview_image":"https://assets.underline.io/lecture/77616/poster/abade21cd4e7f9d6d86857826b68df79.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Large Language Models are Built-in Autoregressive Search Engines","tldr":"Document retrieval is a key stage of standard Web search engines. \nExisting dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. \nTo overcome this limitation, recent autoregressive search engines replace ...","track":"Information Retrieval and Text Mining","underline_id":77616,"underline_url":"https://underline.io/events/395/posters/15279/poster/77616-large-language-models-are-built-in-autoregressive-search-engines","video_url":null},{"abstract":"Despite recent advancements, NLP models continue to be vulnerable to bias. This bias often originates from the uneven distribution of real-world data and can propagate through the annotation process. Escalated integration of these models in our lives calls for methods to mitigate bias without overbearing annotation costs. While active learning (AL) has shown promise in training models with a small amount of annotated data, AL's reliance on the model's behavior for selective sampling can lead to an accumulation of unwanted bias rather than bias mitigation. However, infusing clustering with AL can overcome the bias issue of both AL and traditional annotation methods while exploiting AL's annotation efficiency. In this paper, we propose a novel adaptive clustering-based active learning algorithm, D-CALM, that dynamically adjusts clustering and annotation efforts in response to an estimated classifier error-rate. Experiments on eight datasets for a diverse set of text classification tasks, including emotion, hatespeech, dialog act, and book type detection, demonstrate that our proposed algorithm significantly outperforms baseline AL approaches with both pretrained transformers and traditional Support Vector Machines. D-CALM showcases robustness against different measures of information gain and, as evident from our analysis of label and error distribution, can significantly reduce unwanted model bias.","anthology_url":"https://aclanthology.org/2023.findings-acl.342","authors":["Sabit Hassan","Malihe Alikhani"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-1_-ethics-and-nlp-(virtual-poster)"],"id":"P2270","is_paper":true,"keywords":["model bias/fairness evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.342.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77617/poster_document/8db66c1598b3a46b7ef0d5a31f7863af.pdf","preview_image":"https://assets.underline.io/lecture/77617/poster/66154aa4befc4daae5a36bc2e8b64d42.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77617/slideshow/0570b14e61739604014034e7f6812524.pdf","title":"D-CALM: A Dynamic Clustering-based Active Learning Approach for Mitigating Bias","tldr":"Despite recent advancements, NLP models continue to be vulnerable to bias. This bias often originates from the uneven distribution of real-world data and can propagate through the annotation process. Escalated integration of these models in our lives calls for methods to mitigate bias without overbe...","track":"Ethics and NLP","underline_id":77617,"underline_url":"https://underline.io/events/395/posters/15200/poster/77617-d-calm-a-dynamic-clustering-based-active-learning-approach-for-mitigating-bias","video_url":null},{"abstract":"Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available. The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus. We propose a Quality Estimation based Filtering approach to extract high-quality parallel data from the pseudo-parallel corpus. To the best of our knowledge, this is a novel adaptation of QE framework to extracting quality parallel corpus from the pseudo-parallel corpus.. By training with this filtered corpus, we observe an improvement in the Machine Translation (MT) system's performance by up to 1.8 BLEU points, for English-Marathi, Chinese-English, and Hindi-Bengali language pairs, over the baseline model. The baseline model is the one that is trained on the whole pseudo-parallel corpus. Our Few-shot QE model transfer learned from the English-Marathi QE model and fine-tuned on only 500 Hindi-Bengali training instances, shows an improvement of up to 0.6 BLEU points for Hindi-Bengali language pair, compared to the baseline model. This demonstrates the promise of transfer learning in the setting under discussion. QE systems typically require in the order of (7K-25K) of training data. Our Hindi-Bengali QE is trained on only 500 instances of training that is 1/40th of the normal requirement and achieves comparable performance. All the scripts and datasets utilized in this study will be publicly available.","anthology_url":"https://aclanthology.org/2023.findings-acl.892","authors":["Akshay Batheja","Pushpak Bhattacharyya"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-1_-machine-translation-(virtual-poster)"],"id":"P2272","is_paper":true,"keywords":["few-shot/zero-shot mt"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.892.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77618/poster_document/8277da91b1c2dddf0d124f11dd25942d.pdf","preview_image":"https://assets.underline.io/lecture/77618/poster/e3f2e2960855e088459ef6a4cb156be6.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77618/slideshow/08e2810f482dae8baeba3c70a6cff886.pptx","title":"A Little is Enough: Few-Shot Quality Estimation based Corpus Filtering improves Machine Translation","tldr":"Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available. The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus. We propose a Quality Est...","track":"Machine Translation","underline_id":77618,"underline_url":"https://underline.io/events/395/posters/15200/poster/77618-a-little-is-enough-few-shot-quality-estimation-based-corpus-filtering-improves-machine-translation","video_url":null},{"abstract":"Tasks involving text generation based on multiple input texts, such as multi-document summarization, long-form question answering and contemporary dialogue applications, challenge models for their ability to properly consolidate partly-overlapping multi-text information.\nHowever, these tasks entangle the consolidation phase with the often subjective and ill-defined content selection requirement, impeding proper assessment of models' consolidation capabilities. \nIn this paper, we suggest revisiting the sentence union generation task as an effective well-defined testbed for assessing text consolidation capabilities, decoupling the consolidation challenge from subjective content selection.\nTo support research on this task, we present refined annotation methodology and tools for crowdsourcing sentence union, create the largest union dataset to date and provide an analysis of its rich coverage of various consolidation aspects.\nWe then propose a comprehensive evaluation protocol for union generation, including both human and automatic evaluation. \nFinally, as baselines, we evaluate state-of-the-art language models on the task, along with a detailed analysis of their capacity to address multi-text consolidation challenges and their limitations.","anthology_url":"https://aclanthology.org/2023.findings-acl.440","authors":["Eran Hirsch","Valentina Pyatkin","Ruben Wolhandler","Avi Caciularu","Asi Shefer","Ido Dagan"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-1_-generation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2274","is_paper":true,"keywords":["text-to-text generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.440.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77619/poster_document/eb6f971567d7faa9df25d112900d0aeb.pdf","preview_image":"https://assets.underline.io/lecture/77619/poster/d8f450626784807a4c0ddeab430bd484.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77619/slideshow/eab1b23d996f14775070324748a3ac99.pdf","title":"Revisiting Sentence Union Generation as a Testbed for Text Consolidation","tldr":"Tasks involving text generation based on multiple input texts, such as multi-document summarization, long-form question answering and contemporary dialogue applications, challenge models for their ability to properly consolidate partly-overlapping multi-text information.\nHowever, these tasks entangl...","track":"Generation","underline_id":77619,"underline_url":"https://underline.io/events/395/posters/15200/poster/77619-towards-reference-free-text-simplification-evaluation-with-a-bert-siamese-network-architecture","video_url":null},{"abstract":"Zero-shot cross-lingual transfer is when a multilingual model is trained to perform a task in one language and then is applied to another language. \nAlthough the zero-shot cross-lingual transfer approach has achieved success in various classification tasks, its performance on natural language generation tasks falls short in quality and sometimes outputs an incorrect language. In our study, we show that the fine-tuning process learns language invariant representations, which is beneficial for classification tasks but harmful for generation tasks. Motivated by this, we propose a simple method to regularize the model from learning language invariant representations and a method to select model checkpoints without a development set in the target language, both resulting in better generation quality. Experiments on three semantically diverse generation tasks show that our method reduces the accidental translation problem by 68\\% and improves the ROUGE-L score by 1.5 on average.","anthology_url":"https://aclanthology.org/2023.findings-acl.789","authors":["Tianjian Li","Kenton Murray"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2288","is_paper":true,"keywords":["cross-lingual transfer"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.789.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77620/poster/2222606dc11ba1fc1ec4f8264c1cf852.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a Solution","tldr":"Zero-shot cross-lingual transfer is when a multilingual model is trained to perform a task in one language and then is applied to another language. \nAlthough the zero-shot cross-lingual transfer approach has achieved success in various classification tasks, its performance on natural language genera...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77620,"underline_url":"https://underline.io/events/395/posters/15279/poster/77620-gvdoc-graph-based-visual-document-classification","video_url":null},{"abstract":"Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.","anthology_url":"https://aclanthology.org/2023.findings-acl.539","authors":["Neal G Lawton","Anoop Kumar","Govind Thattai","Aram Galstyan","Greg Ver Steeg"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2289","is_paper":true,"keywords":["parameter-efficient finetuning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.539.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77621/poster_document/b7cc9f0a411b23a98dacf6ced28344da.pdf","preview_image":"https://assets.underline.io/lecture/77621/poster/f51bb0de1d62cf5be3e04d7680394f89.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77621/slideshow/77fc143e41fdbc114085d7deba0527d6.pdf","title":"Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models","tldr":"Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET archit...","track":"Machine Learning for NLP","underline_id":77621,"underline_url":"https://underline.io/events/395/posters/15200/poster/77621-neural-architecture-search-for-parameter-efficient-fine-tuning-of-large-pre-trained-language-models","video_url":null},{"abstract":"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks\u2014toxic output mitigation, gender bias reduction, and sentiment control\u2014and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12\\% or more in relative gain on our main metrics for each task.","anthology_url":"https://aclanthology.org/2023.findings-acl.636","authors":["Jonathan Pei","Kevin Yang","Dan Klein"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-7_-generation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2297","is_paper":true,"keywords":["inference methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.636.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77622/poster_document/b1925d1ded1b2279ed10d6be159aa105.pdf","preview_image":"https://assets.underline.io/lecture/77622/poster/9a4f2bbce44d1691ffb30413b3dc1e11.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77622/slideshow/de4ae26b5a7c1ba34a58e630c78305af.pdf","title":"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation","tldr":"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Sp...","track":"Generation","underline_id":77622,"underline_url":"https://underline.io/events/395/posters/15279/poster/77622-task-aware-retrieval-with-instructions","video_url":null},{"abstract":"It is common sense that one should prefer to eat a salad with a fork rather than with a chainsaw. However, for eating a bowl of rice, the choice between a fork and a pair of chopsticks is culturally relative. We introduce FORK, a small, manually-curated set of CommonsenseQA-style questions for probing cultural biases and assumptions present in commonsense reasoning systems, with a specific focus on food-related customs. We test several CommonsenseQA systems on FORK, and while we see high performance on questions about the US culture, the poor performance of these systems on questions about non-US cultures highlights systematic cultural assumptions aligned with US over non-US cultures.","anthology_url":"https://aclanthology.org/2023.findings-acl.631","authors":["Shramay Palta","Rachel Rudinger"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2298","is_paper":true,"keywords":["model bias/fairness evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.631.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77623/poster_document/5ca66cf669e99f335ce98858355d912b.pdf","preview_image":"https://assets.underline.io/lecture/77623/poster/6548c44a8774cdf7e8f8b33266f45657.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77623/slideshow/a31b8f2ebf74f2eb71e60e08af92768e.pdf","title":"FORK: A Bite-Sized Test Set for Probing Culinary Cultural Biases in Commonsense Reasoning Models","tldr":"It is common sense that one should prefer to eat a salad with a fork rather than with a chainsaw. However, for eating a bowl of rice, the choice between a fork and a pair of chopsticks is culturally relative. We introduce FORK, a small, manually-curated set of CommonsenseQA-style questions for probi...","track":"Ethics and NLP","underline_id":77623,"underline_url":"https://underline.io/events/395/posters/15279/poster/77623-fork-a-bite-sized-test-set-for-probing-culinary-cultural-biases-in-commonsense-reasoning-models","video_url":null},{"abstract":"Multi-Modal Relation Extraction (MMRE) aims at identifying the relation between two entities in texts that contain visual clues. Rich visual content is valuable for the MMRE task, but existing works cannot well model finer associations among different modalities, failing to capture the truly helpful visual information and thus limiting relation extraction performance. In this paper, we propose a novel MMRE framework to better capture the deeper correlations of text, entity pair, and image/objects, so as to mine more helpful information for the task, termed as DGF-PT. We first propose a prompt-based autoregressive encoder, which builds the associations of intra-modal and inter-modal features related to the task, respectively by entity-oriented and object-oriented prefixes. To better integrate helpful visual information, we design a dual-gated fusion module to distinguish the importance of image/objects and further enrich text representations. In addition, a generative decoder is introduced with entity type restriction on relations, better filtering out candidates. Extensive experiments conducted on the benchmark dataset show that our approach achieves excellent performance compared to strong competitors, even in the few-shot situation.","anthology_url":"https://aclanthology.org/2023.findings-acl.572","authors":["Qian Li","Shu Guo","Cheng Ji","Xutan Peng","Shiyao Cui","Jianxin Li","Lihong Wang"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-4_-information-extraction-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P23","is_paper":true,"keywords":["named entity recognition and relation extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.572.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77254/poster_document/e2d895cc6b660eedc613bfd1f9a7f506.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77254/slideshow/6f2fdf19cb21a0a6794c89678216c2bb.pdf","title":"Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction","tldr":"Multi-Modal Relation Extraction (MMRE) aims at identifying the relation between two entities in texts that contain visual clues. Rich visual content is valuable for the MMRE task, but existing works cannot well model finer associations among different modalities, failing to capture the truly helpful...","track":"Information Extraction","underline_id":77254,"underline_url":"https://underline.io/events/395/posters/15240/poster/77254-dual-gated-fusion-with-prefix-tuning-for-multi-modal-relation-extraction","video_url":null},{"abstract":"Linguistic annotations, especially for controversial topics like hate speech detection, are frequently contested due to annotator backgrounds and positionalities. In such situations, preserving this disagreement through the machine learning pipeline can be important for downstream use cases. However, capturing disagreement can increase annotation time and expense. Fortunately, for many tasks, not all examples are equally controversial; we develop an active learning approach, Disagreement Aware Active Learning (DAAL) that concentrates annotations on examples where model entropy and annotator entropy are the most different. Because we cannot know the true entropy of annotations on unlabeled examples, we estimate a model that predicts annotator entropy trained using very few multiply-labeled examples. We find that traditional uncertainty-based active learning underperforms simple passive learning on tasks with high levels of disagreement, but that our active learning approach is able to successfully improve on passive and active baselines, reducing the number of annotations required by at least 24\\% on average across several datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.658","authors":["Connor T Baumler","Anna Sotnikova","Hal Daum\u00e9 III"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2311","is_paper":true,"keywords":["human-in-the-loop / active learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.658.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77624/poster_document/4a6c5a492823c1a61b7b6500f0681322.pdf","preview_image":"https://assets.underline.io/lecture/77624/poster/e3b5f4b46daca267e42fa5c61e1bd9af.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77624/slideshow/69f17787be64589787eddf8d42516bed.pdf","title":"Which Examples Should be Multiply Annotated? Active Learning When Annotators May Disagree","tldr":"Linguistic annotations, especially for controversial topics like hate speech detection, are frequently contested due to annotator backgrounds and positionalities. In such situations, preserving this disagreement through the machine learning pipeline can be important for downstream use cases. However...","track":"Machine Learning for NLP","underline_id":77624,"underline_url":"https://underline.io/events/395/posters/15240/poster/77624-which-examples-should-be-multiply-annotatedquestion-active-learning-when-annotators-may-disagree","video_url":null},{"abstract":"NLP models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. Recent work sought to develop robust, unbiased models by filtering biased examples from training sets. In this work, we argue that such filtering can obscure the true capabilities of models to overcome biases, which might never be removed in full from the dataset. We suggest that in order to drive the development of models robust to subtle biases, dataset biases should be amplified in the training set. We introduce an evaluation framework defined by a bias-amplified training set and an anti-biased test set, both automatically extracted from existing datasets. Experiments across three notions of bias, four datasets and two models show that our framework is substantially more challenging for models than the original data splits, and even more challenging than hand-crafted challenge sets. Our evaluation framework can use any existing dataset, even those considered obsolete, to test model robustness. We hope our work will guide the development of robust models that do not rely on superficial biases and correlations. To this end, we publicly release our code and data.","anthology_url":"https://aclanthology.org/2023.findings-acl.833","authors":["Yuval Reif","Roy Schwartz"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2320","is_paper":true,"keywords":["data shortcuts/artifacts","hardness of samples","robustness"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.833.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77626/poster/3742dddf603f4c6eb8b0f99ef6b1395a.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77626/slideshow/3d89d0101a61d395bc7a7e320ad4fd8c.pdf","title":"Fighting Bias With Bias: Promoting Model Robustness by Amplifying Dataset Biases","tldr":"NLP models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. Recent work sought to develop robust, unbiased models by filtering biased examples from training sets. In this work, we argue that such filter...","track":"Interpretability and Analysis of Models for NLP","underline_id":77626,"underline_url":"https://underline.io/events/395/posters/15279/poster/77626-fighting-bias-with-bias-promoting-model-robustness-by-amplifying-dataset-biases","video_url":null},{"abstract":"Evaluating automatically-generated text summaries is a challenging task. While there have been many interesting approaches, they still fall short of human evaluations. We present RISE, a new approach for evaluating summaries by leveraging techniques from information retrieval. RISE is first trained as a retrieval task using a dual-encoder retrieval setup, and can then be subsequently utilized for evaluating a generated summary given an input document, without gold reference summaries. RISE is especially well suited when working on new datasets where one may not have reference summaries available for evaluation. We conduct comprehensive experiments on the SummEval benchmark (Fabbri et al., 2021) and a long document summarization benchmark. The results show that RISE consistently achieves higher correlation with human evaluations compared to many past approaches to summarization evaluation. Furthermore, RISE also demonstrates data-efficiency and generalizability across languages.","anthology_url":"https://aclanthology.org/2023.findings-acl.865","authors":["David Uthus","Jianmo Ni"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-7_-summarization-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2321","is_paper":true,"keywords":["evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.865.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77627/poster_document/40cc6c29069bf6d9d1cbcadf63c65e05.pdf","preview_image":"https://assets.underline.io/lecture/77627/poster/5e4433c377caf4071017a80ddcbf8bed.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77627/slideshow/6d326e7a78d7bd3cad4aa2f9abd6637b.pdf","title":"RISE: Leveraging Retrieval Techniques for Summarization Evaluation","tldr":"Evaluating automatically-generated text summaries is a challenging task. While there have been many interesting approaches, they still fall short of human evaluations. We present RISE, a new approach for evaluating summaries by leveraging techniques from information retrieval. RISE is first trained ...","track":"Summarization","underline_id":77627,"underline_url":"https://underline.io/events/395/posters/15279/poster/77627-rise-leveraging-retrieval-techniques-for-summarization-evaluation","video_url":null},{"abstract":"Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings.  In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich low-resource language (LRL) of Eastern India. AxomiyaBERTa is trained only on the masked language modeling (MLM) task, without the typical additional next sentence prediction (NSP) objective, and our results show that in resource-scarce settings for very low-resource languages like Assamese, MLM alone can be successfully leveraged for a range of tasks.  AxomiyaBERTa achieves SOTA on token-level tasks like Named Entity Recognition and also performs well on \"longer-context\" tasks like Cloze-style QA and Wiki Title Prediction, with the assistance of a novel embedding disperser and phonological signals respectively. Moreover, we show that AxomiyaBERTa can leverage phonological signals for even more challenging tasks, such as a novel cross-document coreference task on a translated version of the ECB+ corpus, where we present a new SOTA result for an LRL. Our source code and evaluation scripts may be found at https://github.com/csu-signal/axomiyaberta.","anthology_url":"https://aclanthology.org/2023.findings-acl.739","authors":["Abhijnan Nath","Sheikh Abdul Mannan","Nikhil Krishnaswamy"],"category":"Findings","demo_url":null,"display_track":"Linguistic Diversity","event_ids":["session-4_-linguistic-diversity-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2333","is_paper":true,"keywords":["less-resourced languages","indigenous languages","resources for less-resourced languages"],"languages":["assamese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.739.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77629/poster_document/377e169fa0267bf1645be8ae2cfabe87.pdf","preview_image":"https://assets.underline.io/lecture/77629/poster/44a63af809030bcf3686b928cd4fd625.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77629/slideshow/f874ca693ba6011183d4fbd39a6ff8f3.pdf","title":"AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese","tldr":"Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings.  In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich low-resource language (LRL) of Eastern In...","track":"Linguistic Diversity","underline_id":77629,"underline_url":"https://underline.io/events/395/posters/15240/poster/77629-axomiyaberta-a-phonologically-aware-transformer-model-for-assamese","video_url":null},{"abstract":"Warning: This paper contains content that may be offensive or upsetting. \nUnderstanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance \"your English is very good\u201d may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. \n\nWe introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. \n\nTo study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29\\% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.","anthology_url":"https://aclanthology.org/2023.findings-acl.392","authors":["Xuhui Zhou","Hao Zhu","Akhila Yerukola","Thomas Davidson","Jena D. Hwang","Swabha Swayamdipta","Maarten Sap"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)"],"id":"P2343","is_paper":true,"keywords":["hate-speech detection","sociolinguistics"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.392.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77630/poster_document/08ef3a53017d5ea7268aa69369c28c03.pdf","preview_image":"https://assets.underline.io/lecture/77630/poster/eb2b263867e10a08cf60c20e5ba6626f.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77630/slideshow/6a65ea3d925a5af474dff6aaf05b8d78.pdf","title":"COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements","tldr":"Warning: This paper contains content that may be offensive or upsetting. \nUnderstanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance \"your English is very good\u201d may implicitly signal an ...","track":"Computational Social Science and Cultural Analytics","underline_id":77630,"underline_url":"https://underline.io/events/395/posters/15200/poster/77630-bootstrapping-neural-relation-and-explanation-classifiers","video_url":null},{"abstract":"Event Coreference Resolution (ECR) is the task of linking mentions of the same event either within or across documents. Most mention pairs are not coreferent, yet many that are coreferent can be identified through simple techniques such as lemma matching of the event triggers or the sentences in which they appear. Existing methods for training coreference systems sample from a largely skewed distribution, making it difficult for the algorithm to learn coreference beyond surface matching. Additionally, these methods are intractable because of the quadratic operations needed. To address these challenges, we break the problem of ECR into two parts: a) a heuristic to efficiently filter out a large number of non-coreferent pairs, and b) a training approach on a balanced set of coreferent and non-coreferent mention pairs. By following this approach, we show that we get comparable results to the state of the art on two popular ECR datasets while significantly reducing compute requirements. We also analyze the mention pairs that are \"hard\" to accurately classify as coreferent or non-coreferent{code repo:  $\\mathtt{github.com/ahmeshaf/lemma\\_ce\\_coref}$}.","anthology_url":"https://aclanthology.org/2023.findings-acl.100","authors":["Shafiuddin Rehan Ahmed","Abhijnan Nath","James H. Martin","Nikhil Krishnaswamy"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-1_-discourse-and-pragmatics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2383","is_paper":true,"keywords":["coreference resolution"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.100.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77633/poster_document/678f9f3acfbf6c7911b1ca5b4c090e05.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77633/slideshow/5ce3c3ff8720a891a9ed0ae78a87cfd5.pdf","title":"$2*n$ is better than $n^2$: Decomposing Event Coreference Resolution into Two Tractable Problems","tldr":"Event Coreference Resolution (ECR) is the task of linking mentions of the same event either within or across documents. Most mention pairs are not coreferent, yet many that are coreferent can be identified through simple techniques such as lemma matching of the event triggers or the sentences in whi...","track":"Discourse and Pragmatics","underline_id":77633,"underline_url":"https://underline.io/events/395/posters/15200/poster/77633-2-n-is-better-than-ncarret2-decomposing-event-coreference-resolution-into-two-tractable-problems","video_url":null},{"abstract":"Self-training has been shown to be helpful in addressing data scarcity for many domains, including vision, speech, and language. Specifically, self-training, or pseudo-labeling, labels unsupervised data and adds that to the training pool. In this work, we investigate and use pseudo-labeling for a recently proposed novel setup: joint transcription and translation of speech, which suffers from an absence of sufficient parallel data resources. We show that under such data-deficient circumstances, the unlabeled data can significantly vary in domain from the supervised data, which results in pseudo-label quality degradation. We investigate two categories of remedies that require no additional supervision and target the domain mismatch: pseudo-label filtering and data augmentation. We show that pseudo-label analysis and processing in this way results in additional gains on top of the vanilla pseudo-labeling setup providing a total improvement of up to 0.4\\% absolute WER and 2.1 BLEU points for En\u2013De and 0.6\\% absolute WER and 2.2 BLEU points for En\u2013Zh.","anthology_url":"https://aclanthology.org/2023.findings-acl.483","authors":["Mozhdeh Gheini","Tatiana Likhomanenko","Matthias Sperber","Hendra Setiawan"],"category":"Findings","demo_url":null,"display_track":"Speech and Multimodality","event_ids":["session-7_-speech-and-multimodality-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2393","is_paper":true,"keywords":["automatic speech recognition","spoken language translation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.483.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Joint Speech Transcription and Translation: Pseudo-Labeling with Out-of-Distribution Data","tldr":"Self-training has been shown to be helpful in addressing data scarcity for many domains, including vision, speech, and language. Specifically, self-training, or pseudo-labeling, labels unsupervised data and adds that to the training pool. In this work, we investigate and use pseudo-labeling for a re...","track":"Speech and Multimodality","underline_id":77634,"underline_url":"https://underline.io/events/395/posters/15279/poster/77634-joint-speech-transcription-and-translation-pseudo-labeling-with-out-of-distribution-data","video_url":null},{"abstract":"Among the problems with leaderboard culture in NLP has been the widespread lack of confidence estimation in reported results. In this work, we present a framework and simulator for estimating p-values for comparisons between the results of two systems, in order to understand the confidence that one is actually better (i.e. ranked higher) than the other. What has made this difficult in the past is that each system must itself be evaluated by comparison to a gold standard. We define a null hypothesis that each system's metric scores are drawn from the same distribution, using variance found naturally (though rarely reported) in test set items and individual labels on an item (responses) to produce the metric distributions. We create a test set that evenly mixes the responses of the two systems under the assumption the null hypothesis is true. Exploring how to best estimate the true p-value from a single test set under different metrics, tests, and sampling methods, we find that the presence of response variance (from multiple raters or multiple model versions) has a profound impact on p-value estimates for model comparison, and that choice of metric and sampling method is critical to providing statistical guarantees on model comparisons.","anthology_url":"https://aclanthology.org/2023.findings-acl.196","authors":["Shira Wein","Christopher Homan","Lora Aroyo","Chris Welty"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-1_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2406","is_paper":true,"keywords":["evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.196.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77637/poster_document/faee7776b25cec916105eba5b3e8d694.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77637/slideshow/2081e3c32f46fd504e80f5a1807fa7a7.pdf","title":"Follow the leader(board) with confidence: Estimating p-values from a single test set with item and response variance","tldr":"Among the problems with leaderboard culture in NLP has been the widespread lack of confidence estimation in reported results. In this work, we present a framework and simulator for estimating p-values for comparisons between the results of two systems, in order to understand the confidence that one ...","track":"Theme: Reality Check","underline_id":77637,"underline_url":"https://underline.io/events/395/posters/15200/poster/77637-follow-the-leader-board-with-confidence-estimating-p-values-from-a-single-test-set-with-item-and-response-variance","video_url":null},{"abstract":"Patients who effectively manage their symptoms often demonstrate higher levels of engagement in conversations and interventions with healthcare practitioners. This engagement is multifaceted, encompassing cognitive and social dimensions. \nConsequently, it is crucial for AI systems to understand the engagement in natural conversations between patients and practitioners to better contribute toward patient care. In this paper, we present a novel dataset (MedNgage), which consists of patient-nurse conversations about cancer symptom management. We manually annotate the dataset with a novel framework of categories of patient engagement from two different angles, namely: i) socio-affective engagement (3.1K spans), and ii) cognitive engagement (1.8K spans). \nThrough statistical analysis of the data that is annotated using our framework, we show a positive correlation between patient symptom management outcomes and their engagement in conversations. Additionally, we demonstrate that pre-trained transformer models fine-tuned on our dataset can reliably predict engagement categories in patient-nurse conversations. Lastly, we use LIME (Ribeiro et al., 2016) to analyze the underlying challenges of the tasks that state-of-the-art transformer models encounter. The de-identified data is available for research purposes upon request.","anthology_url":"https://aclanthology.org/2023.findings-acl.282","authors":["Yan Wang","Heidi A.S. Donovan","Sabit Hassan","Malihe Alikhani"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)"],"id":"P2407","is_paper":true,"keywords":["corpus creation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.282.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77638/poster_document/99e23cc6e4812adecfe8c61480f479f4.pdf","preview_image":"https://assets.underline.io/lecture/77638/poster/e863e3568effb440100c3ad104e6f3f6.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"MedNgage: A Dataset for Understanding Engagement in Patient-Nurse Conversations","tldr":"Patients who effectively manage their symptoms often demonstrate higher levels of engagement in conversations and interventions with healthcare practitioners. This engagement is multifaceted, encompassing cognitive and social dimensions. \nConsequently, it is crucial for AI systems to understand the ...","track":"Resources and Evaluation","underline_id":77638,"underline_url":"https://underline.io/events/395/posters/15279/poster/77638-medngage-a-dataset-for-understanding-engagement-in-patient-nurse-conversations","video_url":null},{"abstract":"Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-specific examples. This paradigm seriously hinders the development of task generalization since preparing a task-specific example set is costly. To build a system that can quickly and easily generalize to new tasks, task instructions have been adopted as an emerging trend of supervision recently. These instructions give the model the definition of the task and allow the model to output the appropriate answer based on the instructions and inputs. However, task instructions are often expressed in different forms, which can be interpreted from two threads: first, some instructions are short sentences and are pretrained language model (PLM) oriented, such as prompts, while other instructions are paragraphs and are human-oriented, such as those in Amazon MTurk; second, different end-users very likely explain the same task with instructions of different textual expressions. A robust system for task generalization should be able to handle any new tasks regardless of the variability of instructions.  \n\nHowever, the system robustness in dealing with instruction-driven task generalization is still unexplored. This work investigates the system robustness when the instructions of new tasks are (i) manipulated, (ii) paraphrased, or (iii) from different levels of conciseness. To our knowledge, this is the first work that systematically studies how robust a PLM is when it is supervised by instructions with different factors of variability.","anthology_url":"https://aclanthology.org/2023.findings-acl.875","authors":["Jiasheng Gu","Hongyu Zhao","Hanzi Xu","Liangyu Nie","Hongyuan Mei","Wenpeng Yin"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2411","is_paper":true,"keywords":["robustness"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.875.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77640/poster_document/d86c54a30f39d084d3de241b9cf23f53.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Robustness of Learning from Task Instructions","tldr":"Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-specific examples. This paradigm seriously hinders the development of task generalization since preparing a task-specific example set is costly. To build a system that can quickly and easily...","track":"Interpretability and Analysis of Models for NLP","underline_id":77640,"underline_url":"https://underline.io/events/395/posters/15279/poster/77640-robustness-of-learning-from-task-instructions","video_url":null},{"abstract":"Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar sentences retrieved from a high-resource language (HRL). PARC improves zero-shot performance on three downstream tasks (sentiment classification, topic categorization, natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in  unlabeled (+5.1\\%) and labeled settings (+16.3\\%). PARC also outperforms finetuning by 3.7\\%.  We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between high- and low-resource languages as well as the amount of low-resource pretraining data on the other side. A robustness analysis suggests that PARC has the potential to achieve even stronger performance with more powerful MPLMs.","anthology_url":"https://aclanthology.org/2023.findings-acl.528","authors":["Ercong Nie","Sheng Liang","Helmut Schmid","Hinrich Sch\u00fctze"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2416","is_paper":true,"keywords":["cross-lingual transfer"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.528.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77641/poster_document/fd39fe3c2d001305104f735752db92b0.pdf","preview_image":"https://assets.underline.io/lecture/77641/poster/bfd52cf18be9d146dabaeae243b45040.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77641/slideshow/0378d64f49ddb3feea101719fde70985.pdf","title":"Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages","tldr":"Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar sent...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77641,"underline_url":"https://underline.io/events/395/posters/15279/poster/77641-cross-lingual-retrieval-augmented-prompt-for-low-resource-languages","video_url":null},{"abstract":"While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve.","anthology_url":"https://aclanthology.org/2023.findings-acl.321","authors":["Zhun Yang","Adam Ishay","Joohyung Lee"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2419","is_paper":true,"keywords":["reasoning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.321.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77643/poster_document/4d8bcf59f34abb3583d8828835ad0b37.pdf","preview_image":"https://assets.underline.io/lecture/77643/poster/d596d86d2ae369094ad0e533d8fd94e8.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77643/slideshow/5f453c69f98bed7e63c0ec31360ca0ab.pdf","title":"Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text","tldr":"While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effectiv...","track":"Question Answering","underline_id":77643,"underline_url":"https://underline.io/events/395/posters/15240/poster/77643-regen-zero-shot-text-classification-via-training-data-generation-with-progressive-dense-retrieval","video_url":null},{"abstract":"Relation Extraction (RE) has been extended to cross-document scenarios because many relations are not simply described in a single document.\nThis inevitably brings the challenge of efficient open-space evidence retrieval to support the inference of cross-document relations,\nalong with the challenge of multi-hop reasoning on top of entities and evidence scattered in an open set of documents.\nTo combat these challenges, we propose Mr.Cod (Multi-hop evidence retrieval for Cross-document relation extraction), which is a multi-hop evidence retrieval method based on evidence path mining and ranking.\nWe explore multiple variants of retrievers to show evidence retrieval is essential in cross-document RE.\nWe also propose a contextual dense retriever for this setting.\nExperiments on CodRED show that evidence retrieval with Mr.Cod effectively acquires cross-document evidence and boosts end-to-end RE performance in both closed and open settings.","anthology_url":"https://aclanthology.org/2023.findings-acl.657","authors":["Keming Lu","I-Hung Hsu","Wenxuan Zhou","Mingyu Derek Ma","Muhao Chen"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-4_-information-extraction-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2422","is_paper":true,"keywords":["named entity recognition and relation extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.657.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77644/poster_document/ca4dd5b4aad8411bc0cdf178c9fb8954.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Multi-hop Evidence Retrieval for Cross-document Relation Extraction","tldr":"Relation Extraction (RE) has been extended to cross-document scenarios because many relations are not simply described in a single document.\nThis inevitably brings the challenge of efficient open-space evidence retrieval to support the inference of cross-document relations,\nalong with the challenge ...","track":"Information Extraction","underline_id":77644,"underline_url":"https://underline.io/events/395/posters/15240/poster/77644-multi-hop-evidence-retrieval-for-cross-document-relation-extraction","video_url":null},{"abstract":"\"He is a person\", \"Paris is located on the earth\". Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given \"Toronto is located in [MASK].\", we want to test whether a more specific answer will be better filled in by PLMs, e.g., Ontario instead of Canada. From our evaluations, we show that existing PLMs have only a slight preference for more specific answers. We identify underlying factors affecting the specificity and design two prompt-based methods to improve the specificity. Results show that the specificity of the models can be improved by the proposed methods without additional training. We hope this work can bring to awareness the notion of specificity of language models and encourage the research community to further explore this important but understudied problem.","anthology_url":"https://aclanthology.org/2023.findings-acl.45","authors":["Jie Huang","Kevin Chen-Chuan Chang","Jinjun Xiong","Wen-mei Hwu"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-1_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2429","is_paper":true,"keywords":["negative results"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.45.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77647/poster_document/68f97778571d1e5367e1a46039dc086e.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77647/slideshow/7b9d495759c2e1c175690672a8c480db.pdf","title":"Can Language Models Be Specific? How?","tldr":"\"He is a person\", \"Paris is located on the earth\". Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark fo...","track":"Theme: Reality Check","underline_id":77647,"underline_url":"https://underline.io/events/395/posters/15200/poster/77647-can-language-models-be-specificquestion-howquestion","video_url":null},{"abstract":"Large-scale pre-trained models~(PTMs) show great zero-shot capabilities.\nIn this paper, we study how to leverage them for zero-shot visual question answering~(VQA).\nOur approach is motivated by a few observations.\nFirst, VQA questions often require multiple steps of reasoning, which is still a capability that most PTMs lack.\nSecond, different steps in VQA reasoning chains require different skills such as object detection and relational reasoning, but a single PTM may not possess all these skills.\nThird, recent work on zero-shot VQA does not explicitly consider multi-step reasoning chains, which makes them less interpretable compared with a decomposition-based approach.\nWe propose a modularized zero-shot network that explicitly decomposes questions into sub reasoning steps and is highly interpretable. \nWe convert sub reasoning tasks to acceptable objectives of PTMs and assign tasks to proper PTMs without any adaptation.\nOur experiments on two VQA benchmarks under the zero-shot setting demonstrate the effectiveness of our method and better interpretability compared with several baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.5","authors":["RUI CAO","Jing Jiang"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-7_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2437","is_paper":true,"keywords":["vision question answering"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.5.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77649/poster_document/6ab34a8a6adcf2b06a541f8b7184fe5f.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Modularized Zero-shot VQA with Pre-trained Models","tldr":"Large-scale pre-trained models~(PTMs) show great zero-shot capabilities.\nIn this paper, we study how to leverage them for zero-shot visual question answering~(VQA).\nOur approach is motivated by a few observations.\nFirst, VQA questions often require multiple steps of reasoning, which is still a capab...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77649,"underline_url":"https://underline.io/events/395/posters/15279/poster/77649-modularized-zero-shot-vqa-with-pre-trained-models","video_url":null},{"abstract":"Recent work has shown that infusing layout features into language models (LMs) improves processing of visually-rich documents such as scientific papers. Layout-infused LMs are often evaluated on documents with familiar layout features (e.g., papers from the same publisher), but in practice models encounter documents with unfamiliar distributions of layout features, such as new combinations of text sizes and styles, or new spatial configurations of textual elements. In this work we test whether layout-infused LMs are robust to layout distribution shifts. As a case study we use the task of scientific document structure recovery, segmenting a scientific paper into its structural categories (e.g., \"title\", \"caption\", \"reference\"). To emulate distribution shifts that occur in practice we re-partition the GROTOAP2 dataset. We find that under layout distribution shifts model performance degrades by up to 20 F1. Simple training strategies, such as increasing training diversity, can reduce this degradation by over 35\\% relative F1; however, models fail to reach in-distribution performance in any tested out-of-distribution conditions. This work highlights the need to consider layout distribution shifts during model evaluation, and presents a methodology for conducting such evaluations.","anthology_url":"https://aclanthology.org/2023.findings-acl.844","authors":["Catherine Chen","Zejiang Shen","Dan Klein","Gabriel Stanovsky","Doug Downey","Kyle Lo"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-1_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2461","is_paper":true,"keywords":["(non-)generalizability"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.844.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77651/poster_document/640fba2c2fe8c07784d1c16c3033c6c6.pdf","preview_image":"https://assets.underline.io/lecture/77651/poster/e60037a19e26421cff3001f3f708a291.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77651/slideshow/7bf812d1c08f6c6abbb67615c0575aa4.pptx","title":"Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents","tldr":"Recent work has shown that infusing layout features into language models (LMs) improves processing of visually-rich documents such as scientific papers. Layout-infused LMs are often evaluated on documents with familiar layout features (e.g., papers from the same publisher), but in practice models en...","track":"Theme: Reality Check","underline_id":77651,"underline_url":"https://underline.io/events/395/posters/15200/poster/77651-are-layout-infused-language-models-robust-to-layout-distribution-shiftsquestion-a-case-study-with-scientific-documents","video_url":null},{"abstract":"While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts --- thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship (IGR) labels. Counterfactual probing further reveals that while neural models finetuned for predicting IGR reliably use affect in classification, the model's usage of specificity is inconclusive.","anthology_url":"https://aclanthology.org/2023.findings-acl.813","authors":["Venkata Subrahmanyan Govindarajan","David I Beaver","Kyle Mahowald","Junyi Jessy Li"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2462","is_paper":true,"keywords":["nlp tools for social analysis","quantiative analyses of news and/or social media"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.813.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77652/poster_document/c964f7c1e7208280ca6d2d86c9fe3a07.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Counterfactual Probing for the Influence of Affect and Specificity on Intergroup Bias","tldr":"While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and ...","track":"Computational Social Science and Cultural Analytics","underline_id":77652,"underline_url":"https://underline.io/events/395/posters/15200/poster/77652-counterfactual-probing-for-the-influence-of-affect-and-specificity-on-intergroup-bias","video_url":null},{"abstract":"Users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such text may arise from everyday scenarios and are challenging to detect as harmful. We propose FARM, a novel framework leveraging external knowledge for trustworthy rationale generation in the context of safety. In particular, FARM foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. This knowledge is used to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. Our experiments show that FARM obtains state-of-the-art results on the SafeText dataset, showing absolute improvement in safety classification accuracy by 5.9\\%.","anthology_url":"https://aclanthology.org/2023.findings-acl.701","authors":["Alex Mei","Sharon Levy","William Yang Wang"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2465","is_paper":true,"keywords":["participatory/community-based nlp"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.701.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77653/poster_document/072e3a82dd65667c5c7189076866d758.pdf","preview_image":"https://assets.underline.io/lecture/77653/poster/d22fa2cf2d9fa68d263d9c4784bbd436.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77653/slideshow/09f13e0d5327fa408f4a880f08928b58.pdf","title":"Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI","tldr":"Users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such text may arise from everyday sce...","track":"Ethics and NLP","underline_id":77653,"underline_url":"https://underline.io/events/395/posters/15279/poster/77653-matcha-enhancing-visual-language-pretraining-with-math-reasoning-and-chart-derendering","video_url":null},{"abstract":"Despite their widespread adoption, neural conversation models have yet to exhibit natural chat capabilities with humans. In this research, we examine user utterances as causes and generated responses as effects, recognizing that changes in a cause should produce a different effect. To further explore this concept, we have compiled and expanded upon a new dataset called CausalDialogue through crowd-sourcing. This dataset includes multiple cause-effect pairs within a directed acyclic graph (DAG) structure. Our analysis reveals that traditional loss functions struggle to effectively incorporate the DAG structure, leading us to propose a causality-enhanced method called Exponential Maximum Average Treatment Effect (ExMATE) to enhance the impact of causality at the utterance level in training neural conversation models. To evaluate the needs of considering causality in dialogue generation, we built a comprehensive benchmark on CausalDialogue dataset using different models, inference, and training methods. Through experiments, we find that a causality-inspired loss like ExMATE can improve the diversity and agility of conventional loss function and there is still room for improvement to reach human-level quality on this new dataset.","anthology_url":"https://aclanthology.org/2023.findings-acl.792","authors":["Yi-Lin Tuan","Alon Albalak","Wenda Xu","Michael S Saxon","Connor F Pryor","Lise Getoor","William Yang Wang"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2466","is_paper":true,"keywords":["conversational modeling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.792.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77654/poster_document/2e6b20312283ed7e964e010b755de87a.pdf","preview_image":"https://assets.underline.io/lecture/77654/poster/1f6104647dfc66706b46f326c8b31ba4.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77654/slideshow/5c53c70a5800b2c2054f7570efea75a7.pdf","title":"CausalDialogue: Modeling Utterance-level Causality in Conversations","tldr":"Despite their widespread adoption, neural conversation models have yet to exhibit natural chat capabilities with humans. In this research, we examine user utterances as causes and generated responses as effects, recognizing that changes in a cause should produce a different effect. To further explor...","track":"Dialogue and Interactive Systems","underline_id":77654,"underline_url":"https://underline.io/events/395/posters/15200/poster/77654-causaldialogue-modeling-utterance-level-causality-in-conversations","video_url":null},{"abstract":"Information extraction (IE) has been studied extensively. The existing methods always follow a fixed extraction order for complex IE tasks with multiple elements to be extracted in one instance such as event extraction. However, we conduct experiments on several complex IE datasets and observe that different extraction orders can significantly affect the extraction results for a great portion of instances, and the ratio of sentences that are sensitive to extraction orders increases dramatically with the complexity of the IE task. Therefore, this paper proposes a novel adaptive ordered IE paradigm to find the optimal element extraction order for different instances, so as to achieve the best extraction results. We also propose an reinforcement learning (RL) based framework to generate optimal extraction order for each instance dynamically. Additionally, we propose a co-training framework adapted to RL to mitigate the exposure bias during the extractor training phase. Extensive experiments conducted on several public datasets demonstrate that our proposed method can beat previous methods and effectively improve the performance of various IE tasks, especially for complex ones.","anthology_url":"https://aclanthology.org/2023.findings-acl.863","authors":["Wenhao Huang","Jiaqing Liang","Zhixu Li","Yanghua Xiao","chuanjun ji"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-4_-information-extraction-(virtual-poster)"],"id":"P2480","is_paper":true,"keywords":["named entity recognition and relation extraction","event extraction","document-level extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.863.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77656/poster_document/140c930672da9b1d4f62c1ea535e6707.pdf","preview_image":"https://assets.underline.io/lecture/77656/poster/c6b6435895bfbaabe3fca06808d6f8dd.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77656/slideshow/42680682d3ec37975d9d80a0d54b4bd5.pptx","title":"Adaptive Ordered Information Extraction with Deep Reinforcement Learning","tldr":"Information extraction (IE) has been studied extensively. The existing methods always follow a fixed extraction order for complex IE tasks with multiple elements to be extracted in one instance such as event extraction. However, we conduct experiments on several complex IE datasets and observe that ...","track":"Information Extraction","underline_id":77656,"underline_url":"https://underline.io/events/395/posters/15240/poster/77656-adaptive-ordered-information-extraction-with-deep-reinforcement-learning","video_url":null},{"abstract":"Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute in- formation extractor. The cross-modality integration faces several unique challenges: (C1) images and textual descriptions are loosely paired intra-sample and inter-samples; (C2) images usually contain rich backgrounds that can mislead the prediction; (C3) weakly supervised labels from textual-established ex- tractors are biased for multimodal training. We present PV2TEA, an encoder-decoder architecture equipped with three bias reduction schemes: (S1) Augmented label-smoothed contrast to improve the cross-modality alignment for loosely-paired image and text; (S2) Attention-pruning that adaptively distinguishes the visual foreground; (S3) Two-level neighborhood regularization that mitigates the label textual bias via reliability estimation. Empirical results on real-world e-Commerce datasets1 demonstrate up to 11.74\\% absolute (20.97\\% relatively) F1 increase over unimodal baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.127","authors":["Hejie Cui","Rongmei Lin","Nasser Zalmout","Chenwei Zhang","Jingbo Shang","Carl Yang","Xian Li"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)"],"id":"P2484","is_paper":true,"keywords":["cross-modal information extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.127.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"PV2TEA: Patching Visual Modality to Textual-Established Information Extraction","tldr":"Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimo...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77657,"underline_url":"https://underline.io/events/395/posters/15200/poster/77657-pv2tea-patching-visual-modality-to-textual-established-information-extraction","video_url":null},{"abstract":"The ability to converse with humans and follow natural language commands is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can relieve people's burden of holding a controller all the time, allow multitasking, and make drone control more accessible for people with disabilities or with their hands occupied. To this end, we introduce Aerial Vision-and-Dialog Navigation (AVDN), to navigate a drone via natural language conversation. We build a drone simulator with a continuous photorealistic environment and collect a new AVDN dataset of over 3k recorded navigation trajectories with asynchronous human-human dialogs between commanders and followers. The commander provides initial navigation instruction and further guidance by request, while the follower navigates the drone in the simulator and asks questions when needed. During data collection, followers' attention on the drone's visual observation is also recorded. Based on the AVDN dataset, we study the tasks of aerial navigation from (full) dialog history and propose an effective Human Attention Aided Transformer model (HAA-Transformer), which learns to predict both navigation waypoints and human attention.","anthology_url":"https://aclanthology.org/2023.findings-acl.190","authors":["Yue Fan","Winson Chen","Tongzhou Jiang","Chun Zhou","Yi Zhang","Xin Eric Wang"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P25","is_paper":true,"keywords":["vision language navigation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.190.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77255/poster_document/2ab50db04f2799c604866c38beaf8145.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77255/slideshow/773734b1a8d79c6aeb7260ce9120d650.pdf","title":"Aerial Vision-and-Dialog Navigation","tldr":"The ability to converse with humans and follow natural language commands is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can relieve people's burden of holding a controller all the time, allow multitasking, and make drone control more accessible for people with disabilities o...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77255,"underline_url":"https://underline.io/events/395/posters/15240/poster/77255-aerial-vision-and-dialog-navigation","video_url":null},{"abstract":"In the perspective of a layer normalization (LN) position, the architecture of Transformers can be categorized into two types: Post-LN and Pre-LN.\nRecent Transformers prefer to select Pre-LN because the training in Post-LN with deep Transformers, e.g., ten or more layers, often becomes unstable, resulting in useless models.\nHowever, in contrast, Post-LN has also consistently achieved better performance than Pre-LN in relatively shallow Transformers, e.g., six or fewer layers.\nThis study first investigates the reason for these discrepant observations empirically and theoretically and discovers 1, the LN in Post-LN is the source of the vanishing gradient problem that mainly leads the unstable training whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher layers during the back-propagation that may lead an effective training.\nExploiting the new findings, we propose a method that can equip both higher stability and effective training by a simple modification from Post-LN.\nWe conduct experiments on a wide range of text generation tasks and demonstrate that our method outperforms Pre-LN, and stable training regardless of the shallow or deep layer settings.","anthology_url":"https://aclanthology.org/2023.findings-acl.192","authors":["Sho Takase","Shun Kiyono","Sosuke Kobayashi","Jun Suzuki"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2526","is_paper":true,"keywords":["generative models","optimization methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.192.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77661/poster/ba13d72ae91415ceee99297e26c0deeb.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77661/slideshow/48e051cb0a434fe804700a23245250ae.pdf","title":"B2T Connection: Serving Stability and Performance in Deep Transformers","tldr":"In the perspective of a layer normalization (LN) position, the architecture of Transformers can be categorized into two types: Post-LN and Pre-LN.\nRecent Transformers prefer to select Pre-LN because the training in Post-LN with deep Transformers, e.g., ten or more layers, often becomes unstable, res...","track":"Machine Learning for NLP","underline_id":77661,"underline_url":"https://underline.io/events/395/posters/15279/poster/77661-b2t-connection-serving-stability-and-performance-in-deep-transformers","video_url":null},{"abstract":"Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. The parameter-efficient prompt tuning methods that optimize soft embeddings while keeping the pretrained model frozen demonstrate advantages in low computation costs and almost lossless performance. In this work, we explore the transfer of prompt tuning to multimodal pretrained models. Specifically, we implement prompt tuning to a unified sequence-to-sequence pretrained model by adding a sequence of learnable embeddings to each layer and finetuning the pretrained model on downstream task with only the learnable embeddings being optimized. Experimental results on a series of multimodal understanding and generation tasks demonstrate that our method OFA-PT can achieve comparable performance with finetuning across a series of multimodal generation and understanding tasks. Additionally, it significantly outperforms the unified multimodal pretrained model with other parameter-efficient tuning methods, e.g., Adapter, BitFit. etc. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning.","anthology_url":"https://aclanthology.org/2023.findings-acl.27","authors":["Hao Yang","Junyang Lin","An Yang","Peng Wang","Chang Zhou"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-7_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)"],"id":"P2529","is_paper":true,"keywords":["cross-modal pretraining","cross-modal application"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.27.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77662/poster_document/603e4d941cedc040611e865590c87774.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Prompt Tuning for Unified Multimodal Pretrained Models","tldr":"Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. The parameter-efficient prompt tuning methods that optimize soft embeddings while keeping the pretrained model frozen demonstrate advantages in low co...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77662,"underline_url":"https://underline.io/events/395/posters/15279/poster/77662-prompt-tuning-for-unified-multimodal-pretrained-models","video_url":null},{"abstract":"Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models leverage to perform tasks. These capabilities are (i) the ability to quickly generate good candidate utterances (the search capability) (ii) the ability to predict how a listener interprets those utterances and choose the most appropriate one (the pragmatic capability). We design an evaluation scheme for comparing these capabilities of a language model with those of a human. Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking. This insight leads us to augment them with better models of the listener and obtain a significant boost of 11\\% in success rate in guiding real humans. Our work advocates for having a principled procedure for aligning language models with humans that involves (i) formulating task-oriented capabilities, (ii) devising a method to quantify their deficiency, and (iii) iteratively improving them.","anthology_url":"https://aclanthology.org/2023.findings-acl.227","authors":["Lingjun Zhao","Khanh Nguyen","Hal Daum\u00e9 III"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)"],"id":"P2533","is_paper":true,"keywords":["vision language navigation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.227.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77663/poster_document/31a26e7a378af603d3a0a8561fc8efa8.pdf","preview_image":"https://assets.underline.io/lecture/77663/poster/0d6f652295648fbc2c631e637b3c30e0.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models","tldr":"Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests wou...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77663,"underline_url":"https://underline.io/events/395/posters/15200/poster/77663-define-evaluate-and-improve-task-oriented-cognitive-capabilities-for-instruction-generation-models","video_url":null},{"abstract":"ELECTRA, the generator-discriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning. In this study, a multi-perspective course learning (MCL) method is proposed to fetch a many degrees and visual angles for sample-efficient pre-training, and to fully leverage the relationship between generator and discriminator. Concretely, three self-supervision courses are designed to alleviate inherent flaws of MLM and balance the label in a multi-perspective way. Besides, two self-correction courses are proposed to bridge the chasm between the two encoders by creating a \"correction notebook\" for secondary-supervision. Moreover, a course soups trial is conducted to solve the \"tug-of-war\" dynamics problem of MCL, evolving a stronger pre-trained model. Experimental results show that our method significantly improves ELECTRA's average performance by 2.8\\% and 3.2\\% absolute points respectively on GLUE and SQuAD 2.0 benchmarks, and overshadows recent advanced ELECTRA-style models under the same settings. The pre-trained MCL model is available at https://huggingface.co/McmanusChen/MCL-base.","anthology_url":"https://aclanthology.org/2023.findings-acl.9","authors":["Beiduo Chen","Shaohan Huang","Zihan Zhang","Wu Guo","Zhenhua Ling","Haizhen Huang","Furu Wei","Weiwei Deng","Qi Zhang"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-4_-large-language-models-(virtual-poster)"],"id":"P2536","is_paper":true,"keywords":["pre-training"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.9.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77664/poster_document/905e80bd424fb0a653fc5d22d4ac021d.pdf","preview_image":"https://assets.underline.io/lecture/77664/poster/0431c2e7c16aa66c965d1a844be818d7.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77664/slideshow/f1706694492b9ae3393508ea0bb51360.pdf","title":"Pre-training Language Model as a Multi-perspective Course Learner","tldr":"ELECTRA, the generator-discriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked la...","track":"Large Language Models","underline_id":77664,"underline_url":"https://underline.io/events/395/posters/15240/poster/77664-pre-training-language-model-as-a-multi-perspective-course-learner","video_url":null},{"abstract":"Multimodal Sentiment Analysis (MSA) has made great progress that benefits from extraordinary fusion scheme. However, there is a lack of labeled data, resulting in severe overfitting and poor generalization for supervised models applied in this field. In this paper, we propose Sentiment Knowledge Enhanced Self-supervised Learning (SKESL) to capture common sentimental patterns in unlabeled videos, which facilitates further learning on limited labeled data. Specifically, with the help of sentiment knowledge and non-verbal behavior, SKESL conducts sentiment word masking and predicts fine-grained word sentiment intensity, so as to embed sentiment information at the word level into pre-trained multimodal representation. In addition, a non-verbal injection method is also proposed to integrate non-verbal information into the word semantics. Experiments on two standard benchmarks of MSA clearly show that SKESL significantly outperforms the baseline, and achieves new State-Of-The-Art (SOTA) results.","anthology_url":"https://aclanthology.org/2023.findings-acl.821","authors":["Fan Qian","Jiqing Han","Yongjun He","Tieran Zheng","Guibin Zheng"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)"],"id":"P2538","is_paper":true,"keywords":["argument mining","applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.821.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77665/poster_document/440c9fde69595c01a837f6f412a5a109.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Sentiment Knowledge Enhanced Self-supervised Learning for Multimodal Sentiment Analysis","tldr":"Multimodal Sentiment Analysis (MSA) has made great progress that benefits from extraordinary fusion scheme. However, there is a lack of labeled data, resulting in severe overfitting and poor generalization for supervised models applied in this field. In this paper, we propose Sentiment Knowledge Enh...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77665,"underline_url":"https://underline.io/events/395/posters/15200/poster/77665-sentiment-knowledge-enhanced-self-supervised-learning-for-multimodal-sentiment-analysis","video_url":null},{"abstract":"As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After revisiting the coupled requirement of deep neural representation and semantics logic of language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by introducing the alignment processing between uninterpretable neural representation and interpretable statistical logic. Moreover, a clustering process is also designed to connect the word- and context-level semantics. Specifically, an associative knowledge network (AKN), considered interpretable statistical logic, is introduced in the alignment process for word-level semantics. Furthermore, the context-relative distance is employed as the semantic feature for the downstream classifier, which is greatly different from the current uninterpretable semantic representations of pre-trained models. Our experiments for performance evaluation and interpretable analysis are executed on several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a novel evaluation strategy for the interpretability of machine learning models is first proposed. According to the experimental results, our language model can achieve better performance and highly credible interpretable ability compared to related state-of-the-art methods.","anthology_url":"https://aclanthology.org/2023.findings-acl.532","authors":["Fanyu Wang","Zhenping Xie"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P2562","is_paper":true,"keywords":["free-text/natural language explanations"],"languages":["chinese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.532.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77667/poster_document/2978c19fe27499a6560f7c1bfb7b7fb1.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77667/slideshow/a4e5f1c10f60500403fe4582e96ec980.pdf","title":"Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling","tldr":"As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After re...","track":"Interpretability and Analysis of Models for NLP","underline_id":77667,"underline_url":"https://underline.io/events/395/posters/15200/poster/77667-constructing-word-context-coupled-space-aligned-with-associative-knowledge-relations-for-interpretable-language-modeling","video_url":null},{"abstract":"Existing factual consistency evaluation approaches for text summarization provide binary predictions and limited insights into the weakness of summarization systems. Therefore, we propose the task of fine-grained inconsistency detection, the goal of which is to predict the fine-grained types of factual errors in a summary. Motivated by how humans inspect factual inconsistency in summaries, we propose an interpretable fine-grained inconsistency detection model, FineGrainFact, which explicitly represents the facts in the documents and summaries with semantic frames extracted by semantic role labeling, and highlights the related semantic frames to predict inconsistency. The highlighted semantic frames help verify predicted error types and correct inconsistent summaries. Experiment results demonstrate that our model outperforms strong baselines and provides evidence to support or refute the summary.","anthology_url":"https://aclanthology.org/2023.findings-acl.402","authors":["Hou Pong Chan","Qi Zeng","Heng Ji"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-1_-summarization-(virtual-poster)"],"id":"P2582","is_paper":true,"keywords":["evaluation","factuality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.402.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77670/poster_document/de14920b194811ab59fc84a436927c3f.pdf","preview_image":"https://assets.underline.io/lecture/77670/poster/39fa1e2dd868f29ab3265f5a5d60e1bc.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77670/slideshow/1372c669512c804e8c008b83bfe276d6.pdf","title":"Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization","tldr":"Existing factual consistency evaluation approaches for text summarization provide binary predictions and limited insights into the weakness of summarization systems. Therefore, we propose the task of fine-grained inconsistency detection, the goal of which is to predict the fine-grained types of fact...","track":"Summarization","underline_id":77670,"underline_url":"https://underline.io/events/395/posters/15200/poster/77670-interpretable-automatic-fine-grained-inconsistency-detection-in-text-summarization","video_url":null},{"abstract":"Word embedding methods like word2vec and GloVe have been shown to learn strong representations of words.  However, these methods only learn representations for words in the training corpus and therefore struggle to handle unknown and new words, known as out-of-vocabulary (OOV) words.  As a result, there have been multiple attempts to learn OOV word representations in a similar fashion to how humans learn new words, using word roots/subwords and/or surrounding words.  However, while most of these approaches use advanced architectures like attention on the context of the OOV word, they tend to use simple structures like ngram addition or character based convolutional neural networks (CNN) to handle processing subword information.  In response to this, we propose SubAtt, a transformer based OOV estimation model that uses attention mechanisms on both the context and the subwords.  In addition to attention, we also show that pretraining subword representations also leads to improvement in OOV estimation.  We show SubAtt outperforms current state-of-the-art OOV estimation models.","anthology_url":"https://aclanthology.org/2023.findings-acl.221","authors":["Raj Patel","Carlotta Domeniconi"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)"],"id":"P2592","is_paper":true,"keywords":["word embeddings"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.221.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77671/poster_document/603138c611365d026e2bfd850510668c.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Enhancing Out-of-Vocabulary Estimation with Subword Attention","tldr":"Word embedding methods like word2vec and GloVe have been shown to learn strong representations of words.  However, these methods only learn representations for words in the training corpus and therefore struggle to handle unknown and new words, known as out-of-vocabulary (OOV) words.  As a result, t...","track":"Machine Learning for NLP","underline_id":77671,"underline_url":"https://underline.io/events/395/posters/15200/poster/77671-enhancing-out-of-vocabulary-estimation-with-subword-attention","video_url":null},{"abstract":"Knowledge distillation (KD) involves training a small \"student'' model to replicate the strong performance of a high-capacity \"teacher'' model, enabling efficient deployment in resource-constrained settings. Top-performing methods tend to be task- or architecture-specific and lack generalizability. Several existing approaches require pretraining of the teacher on task-specific datasets, which can be costly for large and unstable for small datasets. Here we propose an approach for improving KD through a novel distillation loss agnostic to the task and model architecture. We successfully apply our method to the distillation of the BERT-base and achieve highly competitive results from the distilled student across a range of GLUE tasks, especially for tasks with smaller datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.463","authors":["Sayantan Dasgupta","Trevor Cohn","Timothy Baldwin"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)"],"id":"P2609","is_paper":true,"keywords":["model compression methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.463.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77674/poster_document/946ebf7b6395522500171218c882fefa.pdf","preview_image":"https://assets.underline.io/lecture/77674/poster/babe381378655d8c08456688e99aa1ac.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77674/slideshow/850d918413d01ed391a669f8f3516c1f.pdf","title":"Cost-effective Distillation of Large Language Models","tldr":"Knowledge distillation (KD) involves training a small \"student'' model to replicate the strong performance of a high-capacity \"teacher'' model, enabling efficient deployment in resource-constrained settings. Top-performing methods tend to be task- or architecture-specific and lack generalizability. ...","track":"Machine Learning for NLP","underline_id":77674,"underline_url":"https://underline.io/events/395/posters/15279/poster/77674-cost-effective-distillation-of-large-language-models","video_url":null},{"abstract":"Translation difficulty arises when translators are required to resolve translation ambiguity from multiple possible translations. Translation difficulty can be measured by recording the diversity of responses provided by human translators and the time taken to provide these responses, but these behavioral measures are costly and do not scale. In this work, we use word alignments computed over large scale bilingual corpora to develop predictors of lexical translation difficulty. We evaluate our approach using behavioural data from translations provided both in and out of context, and report results that improve on a previous embedding-based approach (Thompson et al., 2020). Our work can therefore contribute to a deeper understanding of cross-lingual differences and of causes of translation difficulty.","anthology_url":"https://aclanthology.org/2023.findings-acl.736","authors":["Zheng Wei Lim","Trevor Cohn","Charles Kemp","Ekaterina Vylomova"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2629","is_paper":true,"keywords":["multilingualism"],"languages":["malay","chinese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.736.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77678/poster_document/c6327dfc1b46983350021949aafe0a5d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77678/slideshow/fd735182d0e389dae01330ae94f364e3.pdf","title":"Predicting Human Translation Difficulty Using Automatic Word Alignment","tldr":"Translation difficulty arises when translators are required to resolve translation ambiguity from multiple possible translations. Translation difficulty can be measured by recording the diversity of responses provided by human translators and the time taken to provide these responses, but these beha...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77678,"underline_url":"https://underline.io/events/395/posters/15240/poster/77678-predicting-human-translation-difficulty-using-automatic-word-alignment","video_url":null},{"abstract":"We introduce and study the problem of Continual Multilingual Learning (CML) where a previously trained multilingual model is periodically updated using new data arriving in stages. If the new data is present only in a subset of languages, we find that the resulting model shows improved performance only on the languages included in the latest update (and a few closely related languages) while its performance on all the remaining languages degrade significantly.  We address this challenge by proposing LAFT-URIEL, a parameter-efficient finetuning strategy which aims to increase the number of languages on which the model improves after an update, while reducing the magnitude of loss in performance for the remaining languages. LAFT-URIEL uses linguistic knowledge to balance overfitting and knowledge sharing across languages, allowing for an additional 25\\% of task languages to see an improvement in performance after an update, while also reducing the average magnitude of losses on the remaining languages by 78\\% relative.","anthology_url":"https://aclanthology.org/2023.findings-acl.619","authors":["Kartikeya Badola","Shachi Dave","Partha Talukdar"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)"],"id":"P2634","is_paper":true,"keywords":["multilingualism","cross-lingual transfer","multilingual evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.619.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77679/poster_document/86ee011e9e7c6c6a0a88bdd6baa1a86f.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Parameter-Efficient Finetuning for Robust Continual Multilingual Learning","tldr":"We introduce and study the problem of Continual Multilingual Learning (CML) where a previously trained multilingual model is periodically updated using new data arriving in stages. If the new data is present only in a subset of languages, we find that the resulting model shows improved performance o...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77679,"underline_url":"https://underline.io/events/395/posters/15240/poster/77679-parameter-efficient-finetuning-for-robust-continual-multilingual-learning","video_url":null},{"abstract":"Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.","anthology_url":"https://aclanthology.org/2023.findings-acl.806","authors":["Linyi Yang","Shuibai Zhang","Libo Qin","Yafu Li","Yidong Wang","Hanmeng Liu","Jindong Wang","Xing Xie","Yue Zhang"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-4_-theme_-reality-check-(virtual-poster)"],"id":"P2637","is_paper":true,"keywords":["evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.806.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective","tldr":"Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limi...","track":"Theme: Reality Check","underline_id":77680,"underline_url":"https://underline.io/events/395/posters/15240/poster/77680-glue-x-evaluating-natural-language-understanding-models-from-an-out-of-distribution-generalization-perspective","video_url":null},{"abstract":"As the core of task-oriented dialogue systems, dialogue state tracking (DST) is designed to track the dialogue state through the conversation between users and systems. Multi-domain DST has been an important challenge in which the dialogue states across multiple domains need to consider. In recent mainstream approaches, each domain and slot are aggregated and regarded as a single query feeding into attention with the dialogue history to obtain domain-slot specific representations. In this work, we propose disentangled domain-slot attention for multi-domain dialogue state tracking. The proposed approach disentangles the domain-slot specific information extraction in a flexible and context-dependent manner by separating the query about domains and slots in the attention component. Through a series of experiments on MultiWOZ 2.0 and MultiWOZ 2.4 datasets, we demonstrate that our proposed approach outperforms the standard multi-head attention with aggregated domain-slot query.","anthology_url":"https://aclanthology.org/2023.findings-acl.304","authors":["Longfei Yang","Jiyi Li","Sheng Li","Takahiro Shinozaki"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-7_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2651","is_paper":true,"keywords":["dialogue state tracking"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.304.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77682/poster_document/eae08d771437e44a8557fe8decf2a3ed.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Multi-Domain Dialogue State Tracking with Disentangled Domain-Slot Attention","tldr":"As the core of task-oriented dialogue systems, dialogue state tracking (DST) is designed to track the dialogue state through the conversation between users and systems. Multi-domain DST has been an important challenge in which the dialogue states across multiple domains need to consider. In recent m...","track":"Dialogue and Interactive Systems","underline_id":77682,"underline_url":"https://underline.io/events/395/posters/15279/poster/77682-multi-domain-dialogue-state-tracking-with-disentangled-domain-slot-attention","video_url":null},{"abstract":"Large language models have made remarkable progress on a variety of NLP tasks. However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples. In this paper, we propose a human attention guided approach to identifying and mitigating shortcut learning, which encourages the LLM-based target model to learn relevant features. We define an attention-based measurement to capture both model and data bias and identify shortcut tokens by exploring both human and neural attention. In a self-distillation framework, we mitigate shortcut learning by dynamically adjusting the distillation temperature according to the detected shortcut tokens and estimated shortcut degree. Additionally, we utilize human attention as a supervisory signal to constrain large language models to pay more attention to relevant tokens. Experimental results on multiple NLP tasks show that our proposed method can effectively identify shortcut tokens, and significantly improve the robustness of large language models on OOD samples, while not undermining the performance on IID data.","anthology_url":"https://aclanthology.org/2023.findings-acl.781","authors":["Yuqi Ren","Deyi Xiong"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P2700","is_paper":true,"keywords":["data shortcuts/artifacts"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.781.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77686/poster_document/3042342c5a128c0656ac86bdc03af604.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"HuaSLIM: Human Attention Motivated Shortcut Learning Identification and Mitigation for Large Language models","tldr":"Large language models have made remarkable progress on a variety of NLP tasks. However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples. In this paper, we propose a h...","track":"Interpretability and Analysis of Models for NLP","underline_id":77686,"underline_url":"https://underline.io/events/395/posters/15240/poster/77686-huaslim-human-attention-motivated-shortcut-learning-identification-and-mitigation-for-large-language-models","video_url":null},{"abstract":"Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.","anthology_url":"https://aclanthology.org/2023.findings-acl.132","authors":["Sewon Min","Weijia Shi","Mike Lewis","Xilun Chen","Wen-tau Yih","Hannaneh Hajishirzi","Luke Zettlemoyer"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-7_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2706","is_paper":true,"keywords":["pre-training","prompting","retrieval-augmented models"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.132.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77687/poster/514ded388b6329e34bc2b08347be8d4f.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77687/slideshow/f069f7a25ef5db0dd94d9c593b7ade89.pdf","title":"Nonparametric Masked Language Modeling","tldr":"Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a re...","track":"Large Language Models","underline_id":77687,"underline_url":"https://underline.io/events/395/posters/15279/poster/77687-nonparametric-masked-language-modeling","video_url":null},{"abstract":"Zero-shot prompt-based learning has made much progress in sentiment analysis, and considerable effort has been dedicated to designing high-performing prompt templates. However, two problems exist; First, large language models are often biased to their pre-training data, leading to poor performance in prompt templates that models have rarely seen. Second, in order to adapt to different domains, re-designing prompt templates is usually required, which is time-consuming and inefficient. To remedy both shortcomings, we propose a simple yet strong data construction method to de-bias a given prompt template, yielding a large performance improvement in sentiment analysis tasks across different domains, pre-trained language models, and prompt templates. Also, we demonstrate the advantage of using domain-agnostic generic responses over the in-domain ground-truth data.","anthology_url":"https://aclanthology.org/2023.findings-acl.242","authors":["Yang Zhao","Tetsuya Nasukawa","Masayasu Muraoka","Bishwaranjan Bhattacharjee"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2712","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.242.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77688/slideshow/a0ca3591db612a5ebf7e854e7ebba62b.pdf","title":"A Simple Yet Strong Domain-Agnostic De-bias Method for Zero-Shot Sentiment Classification","tldr":"Zero-shot prompt-based learning has made much progress in sentiment analysis, and considerable effort has been dedicated to designing high-performing prompt templates. However, two problems exist; First, large language models are often biased to their pre-training data, leading to poor performance i...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77688,"underline_url":"https://underline.io/events/395/posters/15240/poster/77688-neural-architecture-search-for-parameter-efficient-fine-tuning-of-large-pre-trained-language-models","video_url":null},{"abstract":"Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language (V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over $8\\%$ performance gains.","anthology_url":"https://aclanthology.org/2023.findings-acl.85","authors":["Mingyang Zhou","Yi Fung","Long Chen","Christopher Thomas","Heng Ji","Shih-Fu Chang"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2718","is_paper":true,"keywords":["cross-modal content generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.85.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77690/poster_document/3385f282ba04af11a03ea68b1a3650ad.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Enhanced Chart Understanding via Visual Language Pre-training on Plot Table Pairs","tldr":"Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language (V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart unde...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77690,"underline_url":"https://underline.io/events/395/posters/15200/poster/77690-enhanced-chart-understanding-via-visual-language-pre-training-on-plot-table-pairs","video_url":null},{"abstract":"Speech-to-speech translation is a typical sequence-to-sequence learning task that naturally has two directions. How to effectively leverage bidirectional supervision signals to produce high-fidelity audio for both directions? Existing approaches either train two separate models or a multitask-learned model with low efficiency and inferior performance. In this paper, we propose a duplex diffusion model that applies diffusion probabilistic models to both sides of a reversible duplex Conformer, so that either end can simultaneously input and output a distinct language's speech. Our model enables reversible speech translation by simply flipping the input and output ends. Experiments show that our model achieves the first success of reversible speech translation with significant improvements of ASR-BLEU scores compared with a list of state-of-the-art baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.509","authors":["Xianchao Wu"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-7_-machine-translation-(virtual-poster)"],"id":"P2728","is_paper":true,"keywords":["speech translation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.509.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77691/poster_document/b756494b02d6fa60e4370ddca1447ffb.pdf","preview_image":"https://assets.underline.io/lecture/77691/poster/6f850d24ea26f1075e242095651fcf1c.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77691/slideshow/6d1789d4d45cca43816d111c29d30427.pdf","title":"Duplex Diffusion Models Improve Speech-to-Speech Translation","tldr":"Speech-to-speech translation is a typical sequence-to-sequence learning task that naturally has two directions. How to effectively leverage bidirectional supervision signals to produce high-fidelity audio for both directions? Existing approaches either train two separate models or a multitask-learne...","track":"Machine Translation","underline_id":77691,"underline_url":"https://underline.io/events/395/posters/15279/poster/77691-duplex-diffusion-models-improve-speech-to-speech-translation","video_url":null},{"abstract":"Semi-supervised learning (SSL) is a popular setting aiming to effectively utilize unlabelled data to improve model performance in downstream natural language processing (NLP) tasks. Currently, there are two popular approaches to make use of the unlabelled data: Self-training (ST) and Task-adaptive pre-training (TAPT). ST uses a teacher model to assign pseudo-labels to the unlabelled data, while TAPT continues pre-training on the unlabelled data before fine-tuning. To the best of our knowledge, the effectiveness of TAPT in SSL tasks has not been systematically studied, and no previous work has directly compared TAPT and ST in terms of their ability to utilize the pool of unlabelled data. In this paper, we provide an extensive empirical study comparing five state-of-the-art ST approaches and TAPT across various NLP tasks and data sizes, including in- and out-of domain settings. Surprisingly, we find that TAPT is a strong and more robust SSL learner, even when using just a few hundred unlabelled samples or in the presence of domain shifts, compared to more sophisticated ST approaches, and tends to bring greater improvements in SSL than in fully-supervised settings. \nOur further analysis demonstrates the risks of using ST approaches when the size of labelled or unlabelled data is small or when domain shifts exist, and highlights TAPT as a potential solution.","anthology_url":"https://aclanthology.org/2023.findings-acl.347","authors":["Zhengxiang Shi","Francesco Tonolini","Nikolaos Aletras","Emine Yilmaz","Gabriella Kazai","Yunlong Jiao"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-4_-large-language-models-(virtual-poster)"],"id":"P273","is_paper":true,"keywords":["continual learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.347.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77288/poster_document/c0a941fcc3182ec9de3ef03d58d23d6c.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Rethinking Semi-supervised Learning with Language Models","tldr":"Semi-supervised learning (SSL) is a popular setting aiming to effectively utilize unlabelled data to improve model performance in downstream natural language processing (NLP) tasks. Currently, there are two popular approaches to make use of the unlabelled data: Self-training (ST) and Task-adaptive p...","track":"Large Language Models","underline_id":77288,"underline_url":"https://underline.io/events/395/posters/15240/poster/77288-rethinking-semi-supervised-learning-with-language-models","video_url":null},{"abstract":"Text classification datasets from specialised or technical domains are in high demand, especially in industrial applications. However, due to the high cost of annotation such datasets are usually expensive to create. While Active Learning (AL) can reduce the labeling cost, required AL strategies are often only tested on general knowledge domains and tend to use information sources that are not consistent across tasks. We propose Reinforced Active Learning (RAL) to train a Reinforcement Learning policy that utilizes many different aspects of the data and the task in order to select the most informative unlabeled subset dynamically over the course of the AL procedure. We demonstrate the superior performance of the proposed RAL framework compared to strong AL baselines across four intricate multi-class, multi-label text classification datasets taken from specialised domains. In addition, we experiment with a unique data augmentation approach to further reduce the number of samples RAL needs to annotate.","anthology_url":"https://aclanthology.org/2023.findings-acl.697","authors":["Lukas Wertz","Jasmina Bogojeska","Katsiaryna Mirylenka","Jonas Kuhn"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P274","is_paper":true,"keywords":["self-supervised learning","reinforcement learning","human-in-the-loop / active learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.697.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77289/poster_document/6f457d4ed4d42120e8010033e0bfbc92.pdf","preview_image":"https://assets.underline.io/lecture/77289/poster/d3da0e3c809daf5270aba83802cc184d.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77289/slideshow/bd2fa63843231733cbf8f23e5827ea59.pdf","title":"Reinforced Active Learning for Low-Resource, Domain-Specific, Multi-Label Text Classification","tldr":"Text classification datasets from specialised or technical domains are in high demand, especially in industrial applications. However, due to the high cost of annotation such datasets are usually expensive to create. While Active Learning (AL) can reduce the labeling cost, required AL strategies are...","track":"Machine Learning for NLP","underline_id":77289,"underline_url":"https://underline.io/events/395/posters/15279/poster/77289-analyzing-text-representations-by-measuring-task-alignment","video_url":null},{"abstract":"Code-switching, or switching between languages, occurs for many reasons and has important linguistic, sociological, and cultural implications. Multilingual speakers code-switch for a variety of communicative functions, such as expressing emotions, borrowing terms, making jokes, introducing a new topic, etc.  The function of code-switching may be quite useful for the analysis of linguists, cognitive scientists, speech therapists, and others, but is not readily apparent. To remedy this situation, we annotate and release a new dataset of functions of code-switching in Spanish-English. We build the first system (to our knowledge) to automatically identify a wide range of functions for which speakers code-switch in everyday speech, achieving an accuracy of 75\\% across all functions.","anthology_url":"https://aclanthology.org/2023.findings-acl.469","authors":["Ritu Madhura Belani","Jeffrey Flanigan"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-1_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2746","is_paper":true,"keywords":["code-switching"],"languages":["spanish"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.469.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77694/poster_document/96c3f7938a241fb086ea0fdc9f9dc759.pdf","preview_image":"https://assets.underline.io/lecture/77694/poster/5482b83b4ebfbb79a2f71acd620178e2.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77694/slideshow/ef1947a41fddb07a6fcd85aba39ef1a7.pdf","title":"Automatic Identification of Code-Switching Functions in Speech Transcripts","tldr":"Code-switching, or switching between languages, occurs for many reasons and has important linguistic, sociological, and cultural implications. Multilingual speakers code-switch for a variety of communicative functions, such as expressing emotions, borrowing terms, making jokes, introducing a new top...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77694,"underline_url":"https://underline.io/events/395/posters/15200/poster/77694-automatic-identification-of-code-switching-functions-in-speech-transcripts","video_url":null},{"abstract":"While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of trainable parameters on each individual task. Inspired by the observation that the over-parameterization nature of PLMs makes it possible to freeze most of the parameters during the fine-tuning stage, in this work, we propose a novel ``quantize before fine-tuning'' framework, PreQuant, that differs from both quantization-aware training and post-training quantization. \\{pasted macro `OUR'\\}\\ is compatible with various quantization strategies, with outlier-aware parameter-efficient fine-tuning incorporated to correct the induced quantization error. We demonstrate the effectiveness of PreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an empirical investigation into the workflow of PreQuant, which sheds light on its efficacy.","anthology_url":"https://aclanthology.org/2023.findings-acl.511","authors":["Zhuocheng Gong","Jiahao Liu","Qifan Wang","Yang Yang","Jingang Wang","Wei Wu","Yunsen Xian","Dongyan Zhao","Rui Yan"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2749","is_paper":true,"keywords":["model compression methods","parameter-efficient finetuning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.511.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models","tldr":"While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision ...","track":"Machine Learning for NLP","underline_id":77695,"underline_url":"https://underline.io/events/395/posters/15200/poster/77695-prequant-a-task-agnostic-quantization-approach-for-pre-trained-language-models","video_url":null},{"abstract":"Most existing intent discovery methods leverage representation learning and clustering to transfer the prior knowledge of known intents to unknown ones. The learned representations are limited to the syntactic forms of sentences, therefore, fall short of recognizing adequate variations under the same meaning of unknown intents. This paper proposes an approach utilizing frame knowledge as conceptual semantic guidance to bridge the gap between known intents representation learning and unknown intents clustering. Specifically, we employ semantic regularization to minimize the bidirectional KL divergence between model predictions for frame-based and sentence-based samples. \nMoreover, we construct a frame-guided data augmenter to capture intent-friendly semantic information and implement contrastive clustering learning for unsupervised sentence embedding. \nExtensive experiments on two benchmark datasets show that our method achieves substantial improvements in accuracy (5\\%+) compared to solid baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.898","authors":["yajing sun","Rui Zhang","Jingyuan Yang","Wei Peng"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2767","is_paper":true,"keywords":["dialogue state tracking"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.898.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Intent Discovery with Frame-guided Semantic Regularization and Augmentation","tldr":"Most existing intent discovery methods leverage representation learning and clustering to transfer the prior knowledge of known intents to unknown ones. The learned representations are limited to the syntactic forms of sentences, therefore, fall short of recognizing adequate variations under the sam...","track":"Dialogue and Interactive Systems","underline_id":77696,"underline_url":"https://underline.io/events/395/posters/15200/poster/77696-intent-discovery-with-frame-guided-semantic-regularization-and-augmentation","video_url":null},{"abstract":"Until now, few studies have been explored on Automated Creative Essay Scoring (ACES), in which a pre-trained model automatically labels an essay as a creative or a non-creative. Since the creativity evaluation of essays is very subjective, each evaluator often has his or her own criteria for creativity. For this reason, quantifying creativity in essays is very challenging. In this work, as one of preliminary studies in developing a novel model for ACES, we deeply investigate the correlation between creative essays and expressiveness. Specifically, we explore how rare tokens affect the evaluation of creativity for essays. For such a journey, we present five distinct methods to extract rare tokens, and conduct a comparative study on the correlation between rare tokens and creative essay evaluation results using BERT. Our experimental results showed clear correlation between rare tokens and creative essays. In all test sets, accuracies of our rare token masking-based BERT (ramBERT) model were improved over the existing BERT model up to 14\\%.","anthology_url":"https://aclanthology.org/2023.findings-acl.639","authors":["Youbin Lee","Deokgi Kim","Byung-Won On","Ingyu Lee"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-1_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2772","is_paper":true,"keywords":["methodology"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.639.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77697/poster_document/25f1627c37269a4c50a04eaa47f12e52.pdf","preview_image":"https://assets.underline.io/lecture/77697/poster/2996c2a183df0f9be7bde3bfbd63871a.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77697/slideshow/bce83cd402f67a5aba63ab94e1856938.pptx","title":"A Comparative Analysis of the Effectiveness of Rare Tokens on Creative Expression using ramBERT","tldr":"Until now, few studies have been explored on Automated Creative Essay Scoring (ACES), in which a pre-trained model automatically labels an essay as a creative or a non-creative. Since the creativity evaluation of essays is very subjective, each evaluator often has his or her own criteria for creativ...","track":"Theme: Reality Check","underline_id":77697,"underline_url":"https://underline.io/events/395/posters/15200/poster/77697-vstar-a-video-grounded-dialogue-dataset-for-situated-semantic-understanding-with-scene-and-topic-transitions","video_url":null},{"abstract":"A recent family of techniques, dubbed lightweight fine-tuning methods, facilitates parameter-efficient transfer by updating only a small set of additional parameters while keeping the parameters of the original model frozen. While proven to be an effective approach, there are no existing studies on if and how such knowledge of the downstream fine-tuning approach calls for complementary measures after pre-training and before fine-tuning. In this work, we show that taking the ultimate choice of fine-tuning into consideration boosts the performance of parameter-efficient fine-tuning. By relying on optimization-based meta-learning using MAML with certain modifications for our distinct purpose, we prime the pre-trained model specifically for parameter-efficient fine-tuning, resulting in gains of up to 4.96 points on cross-lingual NER fine-tuning. Our ablation settings and analyses further reveal that the specific approach we take to meta-learning is crucial for the attained gains.","anthology_url":"https://aclanthology.org/2023.findings-acl.737","authors":["Mozhdeh Gheini","Xuezhe Ma","Jonathan May"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2786","is_paper":true,"keywords":["parameter-efficient finetuning","meta learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.737.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Know Where You're Going: Meta-Learning for Parameter-Efficient Fine-Tuning","tldr":"A recent family of techniques, dubbed lightweight fine-tuning methods, facilitates parameter-efficient transfer by updating only a small set of additional parameters while keeping the parameters of the original model frozen. While proven to be an effective approach, there are no existing studies on ...","track":"Machine Learning for NLP","underline_id":77699,"underline_url":"https://underline.io/events/395/posters/15240/poster/77699-know-where-you-re-going-meta-learning-for-parameter-efficient-fine-tuning","video_url":null},{"abstract":"Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model's representations. However, current methods are restricted to removing only linearly encoded information.\nIn this work, we propose Iterative Gradient-Based Projection (IGBP), a novel method for removing non-linear encoded concepts from neural representations. Our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the  representation on a hypersurface, such that the classifiers become oblivious to the target attribute. We evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. Our results demonstrate that IGBP is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy.","anthology_url":"https://aclanthology.org/2023.findings-acl.369","authors":["Shadi Iskander","Kira Radinsky","Yonatan Belinkov"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P2788","is_paper":true,"keywords":["model bias/unfairness mitigation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.369.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77700/poster_document/177f064cbe4f373b7009e5e2d11f347d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection","tldr":"Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model's representations. However, current methods are restricted to removing only linearly encoded information.\n...","track":"Ethics and NLP","underline_id":77700,"underline_url":"https://underline.io/events/395/posters/15279/poster/77700-shielded-representations-protecting-sensitive-attributes-through-iterative-gradient-based-projection","video_url":null},{"abstract":"Automatic speech translation is sensitive to speech recognition errors, but in a multilingual scenario, the same content may be available in various languages via simultaneous interpreting, dubbing or subtitling. In this paper, we hypothesize that leveraging multiple sources will improve translation quality if the sources complement one another in terms of correct information they contain. To this end, we first show that on a 10-hour ESIC corpus, the ASR errors in the original English speech and its simultaneous interpreting into German and Czech are mutually independent. We then use two sources, English and German, in a multi-source setting for translation into Czech to establish its robustness to ASR errors. Furthermore, we observe this robustness when translating both noisy sources together in a simultaneous translation setting. Our results show that multi-source neural machine translation has the potential to be useful in a real-time simultaneous translation setting, thereby motivating further investigation in this area.","anthology_url":"https://aclanthology.org/2023.findings-acl.228","authors":["Dominik Mach\u00e1\u010dek","Peter Polak","Ond\u0159ej Bojar","Raj Dabre"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-7_-machine-translation-(virtual-poster)"],"id":"P279","is_paper":true,"keywords":["speech translation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.228.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77290/poster_document/2d86649247bb93aa270af27d75d49f09.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Robustness of Multi-Source MT to Transcription Errors","tldr":"Automatic speech translation is sensitive to speech recognition errors, but in a multilingual scenario, the same content may be available in various languages via simultaneous interpreting, dubbing or subtitling. In this paper, we hypothesize that leveraging multiple sources will improve translation...","track":"Machine Translation","underline_id":77290,"underline_url":"https://underline.io/events/395/posters/15279/poster/77290-robustness-of-multi-source-mt-to-transcription-errors","video_url":null},{"abstract":"Despite the recent advances in open-domain dialogue systems, building a reliable evaluation metric is still a challenging problem. Recent studies proposed learnable metrics based on classification models trained to distinguish the correct response. However, neural classifiers are known to make overly confident predictions for examples from unseen distributions. We propose DENSITY, which evaluates a response by utilizing density estimation on the feature space derived from a neural classifier. Our metric measures how likely a response would appear in the distribution of human conversations. Moreover, to improve the performance of DENSITY, we utilize contrastive learning to further compress the feature space. Experiments on multiple response evaluation datasets show that DENSITY correlates better with human evaluations than the existing metrics.","anthology_url":"https://aclanthology.org/2023.findings-acl.896","authors":["ChaeHun Park","Seungil Chad Lee","Daniel Rim","Jaegul Choo"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2796","is_paper":true,"keywords":["evaluation and metrics"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.896.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77701/poster_document/199d75517175c976053a2809c8d22dc8.pdf","preview_image":"https://assets.underline.io/lecture/77701/poster/77db9fb8544f2d28169e858ace07b2ab.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77701/slideshow/f1eedfaf82ba089f963a4723ed16ecff.pdf","title":"DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation","tldr":"Despite the recent advances in open-domain dialogue systems, building a reliable evaluation metric is still a challenging problem. Recent studies proposed learnable metrics based on classification models trained to distinguish the correct response. However, neural classifiers are known to make overl...","track":"Dialogue and Interactive Systems","underline_id":77701,"underline_url":"https://underline.io/events/395/posters/15200/poster/77701-density-open-domain-dialogue-evaluation-metric-using-density-estimation","video_url":null},{"abstract":"Event skeleton generation, aiming to induce an event schema skeleton graph with abstracted event nodes and their temporal relations from a set of event instance graphs, is a critical step in the temporal complex event schema induction task. Existing methods effectively address this task from a graph generation perspective but suffer from noise-sensitive and error accumulation, e.g., the inability to correct errors while generating schema. We, therefore, propose a novel Diffusion Event Graph Model~(DEGM) to address these issues. Our DEGM is the first workable diffusion model for event skeleton generation, where the embedding and rounding techniques with a custom edge-based loss are introduced to transform a discrete event graph into learnable latent representations. Furthermore, we propose a denoising training process to maintain the model's robustness. Consequently, DEGM derives the final schema, where error correction is guaranteed by iteratively refining the latent representations during the schema generation process. Experimental results on three IED bombing datasets demonstrate that our DEGM achieves better results than other state-of-the-art baselines. Our code and data are available at https://github.com/zhufq00/EventSkeletonGeneration.","anthology_url":"https://aclanthology.org/2023.findings-acl.800","authors":["Fangqi Zhu","Lin Zhang","Jun Gao","Bing Qin","Ruifeng Xu","Haiqin Yang"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-1_-information-extraction-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2824","is_paper":true,"keywords":["event extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.800.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77704/poster_document/2862b8d42833e523542065a1593915bb.pdf","preview_image":"https://assets.underline.io/lecture/77704/poster/742fcf9819f2af66db7263de20b89b47.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77704/slideshow/05a4adbd7d8007e75ab270c04a889321.pdf","title":"A Diffusion Model for Event Skeleton Generation","tldr":"Event skeleton generation, aiming to induce an event schema skeleton graph with abstracted event nodes and their temporal relations from a set of event instance graphs, is a critical step in the temporal complex event schema induction task. Existing methods effectively address this task from a graph...","track":"Information Extraction","underline_id":77704,"underline_url":"https://underline.io/events/395/posters/15200/poster/77704-a-diffusion-model-for-event-skeleton-generation","video_url":null},{"abstract":"Emotion Support Conversation (ESC) is an emerging and challenging task with the goal of reducing the emotional distress of people. Previous attempts fail to maintain smooth transitions between utterances in ESC because they ignoring to grasp the fine-grained transition information at each dialogue turn. To solve this problem, we propose to take into account turn-level state Transitions of ESC (TransESC) from three perspectives, including semantics transition, strategy transition and emotion transition, to drive the conversation in a smooth and natural way. Specifically, we construct the state transition graph with a two-step way, named transit-then-interact, to grasp such three types of turn-level transition information. Finally, they are injected into the transition aware decoder to generate more engaging responses. Both automatic and human evaluations on the benchmark dataset demonstrate the superiority of TransESC to generate more smooth and effective supportive responses. {Our source code will be publicly available.","anthology_url":"https://aclanthology.org/2023.findings-acl.420","authors":["Weixiang Zhao","Yanyan Zhao","Shilong Wang","Bing Qin"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2830","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.420.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77706/poster_document/e892837bff61de4417d650c220fdd2ef.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77706/slideshow/b243f576df8a836ec7599a86fe65dcea.pdf","title":"TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition","tldr":"Emotion Support Conversation (ESC) is an emerging and challenging task with the goal of reducing the emotional distress of people. Previous attempts fail to maintain smooth transitions between utterances in ESC because they ignoring to grasp the fine-grained transition information at each dialogue t...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77706,"underline_url":"https://underline.io/events/395/posters/15240/poster/77706-transesc-smoothing-emotional-support-conversation-via-turn-level-state-transition","video_url":null},{"abstract":"Distilling  knowledge from a high-resource task, e.g., machine translation, is an effective way to alleviate the data scarcity problem of end-to-end speech translation.\nHowever, previous works simply use the classical knowledge distillation that does not allow for adequate transfer of knowledge from machine translation.\nIn this paper, we propose a comprehensive  knowledge distillation framework for speech translation, CKDST, which is capable of comprehensively and effectively distilling knowledge from machine translation to speech translation from two perspectives: cross-modal contrastive representation distillation and simultaneous decoupled knowledge distillation. \nIn the former, we leverage a contrastive learning objective to optmize the mutual information between speech and text representations for representation distillation in the encoder. \nIn the later, we decouple the non-target class knowledge from target class knowledge for logits distillation in the decoder.\nExperiments on the MuST-C benchmark dataset demonstrate that our CKDST substantially improves the baseline by 1.2 BLEU on average in all translation directions, and outperforms previous state-of-the-art end-to-end and cascaded speech translation models.","anthology_url":"https://aclanthology.org/2023.findings-acl.195","authors":["Yikun Lei","Zhengshan Xue","Xiaohu Zhao","Haoran Sun","Shaolin Zhu","xiaodong lin","Deyi Xiong"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-7_-machine-translation-(virtual-poster)"],"id":"P2835","is_paper":true,"keywords":["speech translation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.195.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77708/poster_document/45f192cda1a5927aad2ea9f957a3cea5.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"CKDST: Comprehensively and Effectively Distill Knowledge from Machine Translation to End-to-End Speech Translation","tldr":"Distilling  knowledge from a high-resource task, e.g., machine translation, is an effective way to alleviate the data scarcity problem of end-to-end speech translation.\nHowever, previous works simply use the classical knowledge distillation that does not allow for adequate transfer of knowledge from...","track":"Machine Translation","underline_id":77708,"underline_url":"https://underline.io/events/395/posters/15279/poster/77708-ckdst-comprehensively-and-effectively-distill-knowledge-from-machine-translation-to-end-to-end-speech-translation","video_url":null},{"abstract":"Existing multimodal task-oriented dialog data fails to demonstrate the diverse expressions of user subjective preferences and recommendation acts in the real-life shopping scenario. This paper introduces a new dataset SURE (Multimodal Recommendation Dialog with Subjective Preference), which contains 12K shopping dialogs in complex store scenes. The data is built in two phases with human annotations to ensure quality and diversity. SURE is well-annotated with subjective preferences and recommendation acts proposed by sales experts. A comprehensive analysis is given to reveal the distinguishing features of SURE. Three benchmark tasks are then proposed on the data to evaluate the capability of multimodal recommendation agents. Basing on the SURE, we propose a baseline model, powered by a state-of-the-art multimodal model, for these tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.217","authors":["Yuxing Long","Binyuan Hui","Caixia Yuan","Fei Huang","Yongbin Li","Xiaojie WANG"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-4_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P2860","is_paper":true,"keywords":["multi-modal dialogue systems"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.217.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77711/poster_document/7788d8ac6ae4b28b2c39e3448c90060a.pdf","preview_image":"https://assets.underline.io/lecture/77711/poster/be8da3bd03c6d302834e9ee63b1f8598.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77711/slideshow/723ac04cbe4939bf15b45803ab107e3f.pdf","title":"Multimodal Recommendation Dialog with Subjective Preference: A New Challenge and Benchmark","tldr":"Existing multimodal task-oriented dialog data fails to demonstrate the diverse expressions of user subjective preferences and recommendation acts in the real-life shopping scenario. This paper introduces a new dataset SURE (Multimodal Recommendation Dialog with Subjective Preference), which contains...","track":"Dialogue and Interactive Systems","underline_id":77711,"underline_url":"https://underline.io/events/395/posters/15240/poster/77711-multimodal-recommendation-dialog-with-subjective-preference-a-new-challenge-and-benchmark","video_url":null},{"abstract":"Much of the work testing machine translation systems for robustness and sensitivity has been adversarial or tended towards testing noisy input such as spelling errors, or non-standard input such as dialects. In this work, we take a step back to investigate a sensitivity problem that can seem trivial and is often overlooked: punctuation. We perform basic sentence-final insertion and deletion perturbation tests with full stops, exclamation and questions marks across source languages and demonstrate a concerning finding: commercial, production-level machine translation systems are vulnerable to mere single punctuation insertion or deletion, resulting in unreliable translations. Moreover, we demonstrate that both string-based and model-based evaluation metrics also suffer from this vulnerability, producing significantly different scores when translations only differ in a single punctuation, with model-based metrics penalizing each punctuation differently. Our work calls into question the reliability of machine translation systems and their evaluation metrics, particularly for real-world use cases, where inconsistent punctuation is often the most common and the least disruptive noise.","anthology_url":"https://aclanthology.org/2023.findings-acl.381","authors":["Prathyusha Jwalapuram"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-1_-theme_-reality-check-(virtual-poster)"],"id":"P2876","is_paper":true,"keywords":["evaluation","ai hype & expectations"],"languages":["german","japanese","ukrainian"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.381.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77715/poster_document/2d0d72bfc03c023a8df115007e3f44f4.pdf","preview_image":"https://assets.underline.io/lecture/77715/poster/7744a5c34cf3d9129682693d681b01c6.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77715/slideshow/84119d9c528b2a8d3640c5c4c868b0af.pdf","title":"Pulling Out All The Full Stops: Punctuation Sensitivity in Neural Machine Translation and Evaluation","tldr":"Much of the work testing machine translation systems for robustness and sensitivity has been adversarial or tended towards testing noisy input such as spelling errors, or non-standard input such as dialects. In this work, we take a step back to investigate a sensitivity problem that can seem trivial...","track":"Theme: Reality Check","underline_id":77715,"underline_url":"https://underline.io/events/395/posters/15200/poster/77715-pulling-out-all-the-full-stops-punctuation-sensitivity-in-neural-machine-translation-and-evaluation","video_url":null},{"abstract":"The nodes in the commonsense knowledge graph (CSKG) are normally represented by free-form short text (e.g., word or phrase). Different nodes may represent the same concept. This leads to the problems of edge sparsity and node redundancy, which challenges CSKG representation and completion. On the one hand, edge sparsity limits the performance of graph representation learning; On the other hand, node redundancy makes different nodes corresponding to the same concept have inconsistent relations with other nodes. To address the two problems, we propose a new CSKG completion framework based on Contrastive Pretraining and Node Clustering (CPNC). Contrastive Pretraining constructs positive and negative head-tail node pairs on CSKG and utilizes contrastive learning to obtain better semantic node representation. Node Clustering aggregates nodes with the same concept into a latent concept, assisting the task of CSKG completion. We evaluate our CPNC approach on two CSKG completion benchmarks (CN-100K and ATOMIC), where CPNC outperforms the state-of-the-art methods. Extensive experiments demonstrate that both Contrastive Pretraining and Node Clustering can significantly improve the performance of CSKG completion. The source code of CPNC is publicly available on {https://github.com/NUSTM/CPNC}.","anthology_url":"https://aclanthology.org/2023.findings-acl.878","authors":["Siwei Wu","Xiangqing Shen","Rui Xia"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)"],"id":"P2884","is_paper":true,"keywords":["knowledge graphs"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.878.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77717/poster_document/f80ff93cab87b27f48e997a2e6c4b9d6.pdf","preview_image":"https://assets.underline.io/lecture/77717/poster/e507c64fe598830a8580b2d6f6890a81.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77717/slideshow/0f3296dbd27c2cc8d21b0639985cea03.pptx","title":"Commonsense Knowledge Graph Completion Via Contrastive Pretraining and Node Clustering","tldr":"The nodes in the commonsense knowledge graph (CSKG) are normally represented by free-form short text (e.g., word or phrase). Different nodes may represent the same concept. This leads to the problems of edge sparsity and node redundancy, which challenges CSKG representation and completion. On the on...","track":"NLP Applications","underline_id":77717,"underline_url":"https://underline.io/events/395/posters/15240/poster/77717-diaasq-a-benchmark-of-conversational-aspect-based-sentiment-quadruple-analysis","video_url":null},{"abstract":"Aspect Sentiment Triplet Extraction (ASTE) is widely used in various applications. However, existing ASTE datasets are limited in their ability to represent real-world scenarios, hindering the advancement of research in this area. In this paper, we introduce a new dataset, named DMASTE, which is manually annotated to better fit real-world scenarios by providing more diverse and realistic reviews for the task. The dataset includes various lengths, diverse expressions, more aspect types, and more domains than existing datasets. We conduct extensive experiments on DMASTE in multiple settings to evaluate previous ASTE approaches. Empirical results demonstrate that DMASTE is a more challenging ASTE dataset. Further analyses of in-domain and cross-domain settings provide some promising directions for future research.","anthology_url":"https://aclanthology.org/2023.findings-acl.178","authors":["Ting Xu","Huiyun Yang","Zhen Wu","Jiaze Chen","Fei Zhao","Xinyu Dai"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)"],"id":"P2885","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.178.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77718/poster_document/033dd5cba4004523e0eb1c0cdca2d6ba.pdf","preview_image":"https://assets.underline.io/lecture/77718/poster/702f3b6a0cde05e99d90aaa8f7c76bcc.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77718/slideshow/84a81f09090a2aec2f792f9a49a7ced8.pdf","title":"Measuring Your ASTE Models in The Wild: A Diversified Multi-domain Dataset For Aspect Sentiment Triplet Extraction","tldr":"Aspect Sentiment Triplet Extraction (ASTE) is widely used in various applications. However, existing ASTE datasets are limited in their ability to represent real-world scenarios, hindering the advancement of research in this area. In this paper, we introduce a new dataset, named DMASTE, which is man...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77718,"underline_url":"https://underline.io/events/395/posters/15240/poster/77718-measuring-your-aste-models-in-the-wild-a-diversified-multi-domain-dataset-for-aspect-sentiment-triplet-extraction","video_url":null},{"abstract":"Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80\\% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100\\% of the dataset.","anthology_url":"https://aclanthology.org/2023.findings-acl.507","authors":["Cheng-Yu Hsieh","Chun-Liang Li","CHIH-KUAN YEH","Hootan Nakhost","Yasuhisa Fujii","Alex Jason Ratner","Ranjay Krishna","Chen-Yu Lee","Tomas Pfister"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2887","is_paper":true,"keywords":["multi-task learning","data augmentation","model compression methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.507.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77719/poster_document/61b5b3a4507dd59c9c13ee49decc6bd9.pdf","preview_image":"https://assets.underline.io/lecture/77719/poster/dca989c3a4dcaa065e9407f971911163.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77719/slideshow/30c0cf3bd2c6734d37aa17ef7c1d0a17.pdf","title":"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes","tldr":"Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning...","track":"Machine Learning for NLP","underline_id":77719,"underline_url":"https://underline.io/events/395/posters/15240/poster/77719-distilling-step-by-step-outperforming-larger-language-models-with-less-training-data-and-smaller-model-sizes","video_url":null},{"abstract":"We present a new task, speech dialogue translation mediating speakers of different languages. We construct the SpeechBSD dataset for the task and conduct baseline experiments. Furthermore, we consider context to be an important aspect that needs to be addressed in this task and propose two ways of utilizing context, namely monolingual context and bilingual context. We conduct cascaded speech translation experiments using Whisper and mBART, and show that bilingual context performs better in our settings.","anthology_url":"https://aclanthology.org/2023.findings-acl.72","authors":["Shuichiro Shimizu","Chenhui Chu","Sheng Li","Sadao Kurohashi"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-1_-machine-translation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2893","is_paper":true,"keywords":["speech translation"],"languages":["japanese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.72.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77720/poster_document/9dfd19086d51ba9ba7b91a1fd41828e0.pdf","preview_image":"https://assets.underline.io/lecture/77720/poster/5914d89388ae90331b761b617a3936a7.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77720/slideshow/dcac4c69dab5a296cfb8f5c24f51bcfb.pdf","title":"Towards Speech Dialogue Translation Mediating Speakers of Different Languages","tldr":"We present a new task, speech dialogue translation mediating speakers of different languages. We construct the SpeechBSD dataset for the task and conduct baseline experiments. Furthermore, we consider context to be an important aspect that needs to be addressed in this task and propose two ways of u...","track":"Machine Translation","underline_id":77720,"underline_url":"https://underline.io/events/395/posters/15200/poster/77720-epic-multi-perspective-annotation-of-a-corpus-of-irony","video_url":null},{"abstract":"This paper explores the task of Temporal Video Grounding (TVG) where, given an untrimmed video and a query sentence, the goal is to recognize and determine temporal boundaries of action instances in the video described by natural language queries. \nRecent works tackled this task by improving query inputs with large pre-trained language models (PLM), at the cost of more expensive training. However, the effects of this integration are unclear, as these works also propose improvements in the visual inputs.  \nTherefore, this paper studies the role of query sentence representation with PLMs in TVG and assesses the applicability of parameter-efficient training with NLP adapters. \nWe couple popular PLMs with a selection of existing approaches and test different adapters to reduce the impact of the additional parameters. \nOur results on three challenging datasets show that, with the same visual inputs, TVG models greatly benefited from the PLM integration and fine-tuning, stressing the importance of the text query representation in this task.\nFurthermore, adapters were an effective alternative to full fine-tuning, even though they are not tailored to our task, allowing PLM integration in larger TVG models and delivering results comparable to SOTA models. \nFinally, our results shed light on which adapters work best in different scenarios.","anthology_url":"https://aclanthology.org/2023.findings-acl.829","authors":["Erica Kido Shimomoto","Edison Marrese-Taylor","Hiroya Takamura","Ichiro Kobayashi","Hideki Nakayama","Yusuke Miyao"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2907","is_paper":true,"keywords":["cross-modal application"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.829.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77721/poster_document/8aac29261903760618cdcf1cb6946684.pdf","preview_image":"https://assets.underline.io/lecture/77721/poster/34224c17d8565e6ecbf8338bd267dd80.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77721/slideshow/15a58ad3742a07291f3bcc0715b712cc.pdf","title":"Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding","tldr":"This paper explores the task of Temporal Video Grounding (TVG) where, given an untrimmed video and a query sentence, the goal is to recognize and determine temporal boundaries of action instances in the video described by natural language queries. \nRecent works tackled this task by improving query i...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77721,"underline_url":"https://underline.io/events/395/posters/15200/poster/77721-cone-an-efficient-coarse-to-fine-alignment-framework-for-long-video-temporal-grounding","video_url":null},{"abstract":"Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE), a fundamental information extraction task. We hypothesize that instruction-tuning has been unable to elicit strong RE capabilities in LLMs due to RE's low incidence in instruction-tuning datasets, making up less than 1\\% of all tasks (Wang et al. 2022). To address this limitation, we propose QA4RE, a framework that aligns RE with question answering (QA), a predominant task in instruction-tuning datasets. Comprehensive zero-shot RE experiments over four datasets with two series of instruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework consistently improves LLM performance, strongly verifying our hypothesis and enabling LLMs to outperform strong zero-shot baselines by a large margin. Additionally, we provide thorough experiments and discussions to show the robustness, few-shot effectiveness, and strong transferability of our QA4RE framework. This work illustrates a promising way of adapting LLMs to challenging and underrepresented tasks by aligning these tasks with more common instruction-tuning tasks like QA.","anthology_url":"https://aclanthology.org/2023.findings-acl.50","authors":["Kai Zhang","Bernal Jimenez Gutierrez","Yu Su"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-1_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P2957","is_paper":true,"keywords":["prompting","applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.50.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77728/poster_document/4d239ca47ca6ca0da1e686ecd1d6934b.pdf","preview_image":"https://assets.underline.io/lecture/77728/poster/1b0715de192108d0ae4a5ea55f4bb60d.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77728/slideshow/8456c928e0e7145af3ee16894f5a7106.pdf","title":"Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors","tldr":"Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small L...","track":"Large Language Models","underline_id":77728,"underline_url":"https://underline.io/events/395/posters/15200/poster/77728-aligning-instruction-tasks-unlocks-large-language-models-as-zero-shot-relation-extractors","video_url":null},{"abstract":"The impression is crucial for the referring physicians to grasp key information since it is concluded from the findings and reasoning of radiologists. To alleviate the workload of radiologists and reduce repetitive human labor in impression writing, many researchers have focused on automatic impression generation. However, recent works on this task mainly summarize the corresponding findings and pay less attention to the radiology images. In clinical, radiographs can provide more detailed valuable observations to enhance radiologists' impression writing, especially for complicated cases. Besides, each sentence in findings usually focuses on single anatomy, such that they only need to be matched to corresponding anatomical regions instead of the whole image, which is beneficial for textual and visual features alignment. Therefore, we propose a novel anatomy-enhanced multimodal model to promote impression generation. In detail, we first construct a set of rules to extract anatomies and put these prompts into each sentence to highlight anatomy characteristics. Then, two separate encoders are applied to extract features from the radiograph and findings. Afterward, we utilize a contrastive learning module to align these two representations at the overall level and use a co-attention to fuse them at the sentence level with the help of anatomy-enhanced sentence representation. The experimental results on two benchmark datasets confirm the effectiveness of the proposed method, which achieves state-of-the-art results.","anthology_url":"https://aclanthology.org/2023.findings-acl.764","authors":["jinpeng hu","Zhihong Chen","Yang Liu","Xiang Wan","Tsung-Hui Chang"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-4_-summarization-(virtual-poster)"],"id":"P2987","is_paper":true,"keywords":["multimodal summarization"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.764.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77731/poster_document/70e2ed46e9fbb85ca2f58cea959e54bd.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Improving Radiology Summarization with Radiograph and Anatomy Prompts","tldr":"The impression is crucial for the referring physicians to grasp key information since it is concluded from the findings and reasoning of radiologists. To alleviate the workload of radiologists and reduce repetitive human labor in impression writing, many researchers have focused on automatic impress...","track":"Summarization","underline_id":77731,"underline_url":"https://underline.io/events/395/posters/15240/poster/77731-improving-radiology-summarization-with-radiograph-and-anatomy-prompts","video_url":null},{"abstract":"Chinese Named Entity Recognition (CNER) is a widely used technology in various applications. While recent studies have focused on utilizing additional information of the Chinese language and characters to enhance CNER performance, this paper focuses on a specific aspect of CNER known as fine-grained CNER (FG-CNER). FG-CNER involves the use of hierarchical, fine-grained categories (e.g. Person-MovieStar) to label named entities. To promote research in this area, we introduce the FiNE dataset, a dataset for FG-CNER consisting of 30,000 sentences from various domains and containing 67,651 entities in 54 fine-grained flattened hierarchical categories. Additionally, we propose SoftFiNE, a novel approach for FG-CNER that utilizes a custom-designed relevance scoring function based on label structures to learn the potential relevance between different flattened hierarchical labels. Our experimental results demonstrate that the proposed SoftFiNE method outperforms the state-of-the-art baselines on the FiNE dataset. Furthermore, we conduct extensive experiments on three other datasets, including OntoNotes 4.0, Weibo, and Resume, where SoftFiNE achieved state-of-the-art performance on all three datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.211","authors":["Jiuding Yang","Jinwen Luo","Weidong Guo","Di Niu","Yu Xu"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-4_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P300","is_paper":true,"keywords":["nlp datasets"],"languages":["chinese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.211.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77292/poster_document/1da0470c1d2662a3f4413071cff464f3.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77292/slideshow/01a830224b96a5230669b7c6988b22ea.pdf","title":"Exploiting Hierarchically Structured Categories in Fine-grained Chinese Named Entity Recognition","tldr":"Chinese Named Entity Recognition (CNER) is a widely used technology in various applications. While recent studies have focused on utilizing additional information of the Chinese language and characters to enhance CNER performance, this paper focuses on a specific aspect of CNER known as fine-grained...","track":"Resources and Evaluation","underline_id":77292,"underline_url":"https://underline.io/events/395/posters/15240/poster/77292-exploiting-hierarchically-structured-categories-in-fine-grained-chinese-named-entity-recognition","video_url":null},{"abstract":"Speaker diarization is a classic task in speech processing and is crucial in multi-party scenarios such as meetings and conversations. Current mainstream speaker diarization approaches consider acoustic information only, which result in performance degradation when encountering adverse acoustic environment. In this paper, we propose methods to extract speaker-related information from semantic content in multi-party meetings, which, as we will show, can further benefit speaker diarization. \nWe introduce two sub-tasks, Dialogue Detection and Speaker-Turn Detection, in which we effectively extract speaker information from conversational semantics. We also propose a simple yet effective algorithm to jointly model acoustic and semantic information and obtain speaker-identified texts.\nExperiments on both AISHELL-4 and AliMeeting datasets show that our method achieves consistent improvements over acoustic-only speaker diarization systems.","anthology_url":"https://aclanthology.org/2023.findings-acl.884","authors":["Luyao Cheng","Siqi Zheng","Zhang Qinglin","Hui Wang","Yafeng Chen","Qian Chen"],"category":"Findings","demo_url":null,"display_track":"Speech and Multimodality","event_ids":["session-1_-speech-and-multimodality-(virtual-poster)"],"id":"P3005","is_paper":true,"keywords":["speech and vision"],"languages":["mandarin"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.884.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77734/poster_document/667a912648705e420c98a3b13b29f36d.pdf","preview_image":"https://assets.underline.io/lecture/77734/poster/2f7956ada2cfa192407fa22358ebbe4f.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77734/slideshow/6afdb92d64a6fc5a0ac727340b655159.pdf","title":"Exploring Speaker-Related Information in Spoken Language Understanding for Better Speaker Diarization","tldr":"Speaker diarization is a classic task in speech processing and is crucial in multi-party scenarios such as meetings and conversations. Current mainstream speaker diarization approaches consider acoustic information only, which result in performance degradation when encountering adverse acoustic envi...","track":"Speech and Multimodality","underline_id":77734,"underline_url":"https://underline.io/events/395/posters/15200/poster/77734-exploring-speaker-related-information-in-spoken-language-understanding-for-better-speaker-diarization","video_url":null},{"abstract":"Figures of speech help people express abstract concepts and evoke stronger emotions than literal expressions, thereby making texts more creative and engaging. Due to its pervasive and fundamental character, figurative language understanding has been addressed in Natural Language Processing, but it's highly understudied in a multilingual setting and when considering more than one figure of speech at the same time. To bridge this gap, we introduce multilingual multi-figurative language modelling, and provide a benchmark for sentence-level figurative language detection, covering three common figures of speech and seven languages. Specifically, we develop a framework for figurative language detection based on template-based prompt learning. In so doing, we unify multiple detection tasks that are interrelated across multiple figures of speech and languages, without requiring task- or language-specific modules. Experimental results show that our framework outperforms several strong baselines and may serve as a blueprint for the joint modelling of other interrelated tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.589","authors":["Huiyuan Lai","Antonio Toral","Malvina Nissim"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3011","is_paper":true,"keywords":["rhetoric and framing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.589.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77735/poster_document/c0cf8ab5be0134d6090c7f227704be87.pdf","preview_image":"https://assets.underline.io/lecture/77735/poster/54aca24917f594ec8963753e29e8df93.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77735/slideshow/1760e3439314291f9687292193d15d6f.pdf","title":"Multilingual Multi-Figurative Language Detection","tldr":"Figures of speech help people express abstract concepts and evoke stronger emotions than literal expressions, thereby making texts more creative and engaging. Due to its pervasive and fundamental character, figurative language understanding has been addressed in Natural Language Processing, but it's...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77735,"underline_url":"https://underline.io/events/395/posters/15200/poster/77735-multilingual-multi-figurative-language-detection","video_url":null},{"abstract":"How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.\nRecently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation(DUB) to answer two questions (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://anonymous.4open.science/r/DUB/.","anthology_url":"https://aclanthology.org/2023.findings-acl.447","authors":["Dong Zhang","Rong Ye","Tom Ko","Mingxuan Wang","Yaqian Zhou"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-1_-machine-translation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3017","is_paper":true,"keywords":["speech translation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.447.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77736/poster_document/4a8389d05045c39526298cc9f2f62181.pdf","preview_image":"https://assets.underline.io/lecture/77736/poster/49470cf831fb98cb0f2c3197223716c4.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77736/slideshow/87d6696e743012180ea402bba705b4ed.pdf","title":"DUB: Discrete Unit Back-translation for Speech Translation","tldr":"How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.\nRecently, the approach of representing speech with unsupervised discrete units yields a new way ...","track":"Machine Translation","underline_id":77736,"underline_url":"https://underline.io/events/395/posters/15200/poster/77736-dub-discrete-unit-back-translation-for-speech-translation","video_url":null},{"abstract":"Transformer architectures are complex and their use in NLP, while it has engendered many successes, makes their interpretability or explainability challenging.  Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this paper, we present some of their limitations and introduce COCKATIEL, which successfully addresses some of them. COCKATIEL is a novel, post-hoc, concept-based, model-agnostic XAI technique that generates meaningful explanations from the last layer of a neural net model trained on an NLP classification task by using Non-Negative Matrix Factorization (NMF) to discover the concepts the model leverages to make predictions and by exploiting a Sensitivity Analysis to estimate accurately the importance of each of these concepts for the model. It does so without compromising the accuracy of the underlying model or requiring a new one to be trained. \n\nWe conduct experiments in single and multi-aspect sentiment analysis tasks and we show COCKATIEL's superior ability to discover concepts that align with humans' on Transformer models without any supervision, we objectively verify the faithfulness of its explanations through fidelity metrics, and we showcase its ability to provide meaningful explanations in two different datasets.\n\nOur code is freely available: https://github.com/fanny-jourdan/cockatiel","anthology_url":"https://aclanthology.org/2023.findings-acl.317","authors":["Fanny Jourdan","Agustin Martin Picard","Thomas Fel","Laurent Risser","Jean-Michel Loubes","Nicholas Asher"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3035","is_paper":true,"keywords":["hierarchical & concept explanations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.317.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77737/poster_document/37fcb7834f0fb94e9113ad9194299027.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77737/slideshow/89deababfd007f75b4d93eed9b16b1dc.pdf","title":"COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP","tldr":"Transformer architectures are complex and their use in NLP, while it has engendered many successes, makes their interpretability or explainability challenging.  Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this p...","track":"Interpretability and Analysis of Models for NLP","underline_id":77737,"underline_url":"https://underline.io/events/395/posters/15200/poster/77737-cockatiel-continuous-concept-ranked-attribution-with-interpretable-elements-for-explaining-neural-net-classifiers-on-nlp","video_url":null},{"abstract":"Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated examples can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.564","authors":["Sweta Agrawal","Chunting Zhou","Mike Lewis","Luke Zettlemoyer","Marjan Ghazvininejad"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-7_-machine-translation-(virtual-poster)"],"id":"P304","is_paper":true,"keywords":["few-shot/zero-shot mt"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.564.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"In-context Examples Selection for Machine Translation","tldr":"Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the d...","track":"Machine Translation","underline_id":77293,"underline_url":"https://underline.io/events/395/posters/15279/poster/77293-in-context-examples-selection-for-machine-translation","video_url":null},{"abstract":"Without any explicit cross-lingual training data, multilingual language models can achieve cross-lingual transfer. One common way to improve this transfer is to perform realignment steps before fine-tuning, i.e., to train the model to build similar representations for pairs of words from translated sentences. But such realignment methods were found to not always improve results across languages and tasks, which raises the question of whether aligned representations are truly beneficial for cross-lingual transfer. We provide evidence that alignment is actually significantly correlated with cross-lingual transfer across languages, models and random seeds. We show that fine-tuning can have a significant impact on alignment, depending mainly on the downstream task and the model. Finally, we show that realignment can, in some instances, improve cross-lingual transfer, and we identify conditions in which realignment methods provide significant improvements. Namely, we find that realignment works better on tasks for which alignment is correlated with cross-lingual transfer when generalizing to a distant language and with smaller models, as well as when using a bilingual dictionary rather than FastAlign to extract realignment pairs. For example, for POS-tagging, between English and Arabic, realignment can bring a +15.8 accuracy improvement on distilmBERT, even outperforming XLM-R Large by 1.7. We thus advocate for further research on realignment methods for smaller multilingual models as an alternative to scaling.","anthology_url":"https://aclanthology.org/2023.findings-acl.189","authors":["Felix Gaschi","Patricio Cerda","Parisa RASTIN","Yannick Toussaint"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-1_-multilingualism-and-cross-lingual-nlp-(virtual-poster)"],"id":"P3047","is_paper":true,"keywords":["cross-lingual transfer","mutlilingual representations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.189.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77738/poster_document/aa76eba012e421852a6a89f14cd31190.pdf","preview_image":"https://assets.underline.io/lecture/77738/poster/0aa6e3e77b0072c4b562c332638fd539.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77738/slideshow/16da0a3923ba50c16cf6dac6a253ea17.pdf","title":"Exploring the Relationship between Alignment and Cross-lingual Transfer in Multilingual Transformers","tldr":"Without any explicit cross-lingual training data, multilingual language models can achieve cross-lingual transfer. One common way to improve this transfer is to perform realignment steps before fine-tuning, i.e., to train the model to build similar representations for pairs of words from translated ...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77738,"underline_url":"https://underline.io/events/395/posters/15200/poster/77738-tada-task-agnostic-dialect-adapters-for-english","video_url":null},{"abstract":"Disfluencies in user utterances can trigger a chain of errors impacting all the modules of a dialogue system: natural language understanding, dialogue state tracking, and response generation. In this work, we first analyze existing dialogue datasets commonly used in research and show that they only contain a marginal number of disfluent utterances. Due to this relative absence of disfluencies in their training data, dialogue systems may then critically fail when exposed to disfluent utterances. Following this observation, we propose to augment existing datasets with disfluent user utterances by paraphrasing fluent utterances into disfluent ones. Relying on a pre-trained language model, our few-shot disfluent paraphraser guided by a disfluency classifier can generate useful disfluent utterances for training better dialogue systems. We report on improvements for both dialogue state tracking and response generation when the dialogue systems are trained on datasets augmented with our disfluent utterances.","anthology_url":"https://aclanthology.org/2023.findings-acl.728","authors":["Benjamin Marie"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-7_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P3057","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.728.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77739/poster_document/4ec9e7c59db87f99349f9d07929657f6.pdf","preview_image":"https://assets.underline.io/lecture/77739/poster/2afe72c03cfcb39c3d1ef27aebb4ffe0.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77739/slideshow/278fcf8530bd193912f4f051e30e147b.pdf","title":"Disfluency Generation for More Robust Dialogue Systems","tldr":"Disfluencies in user utterances can trigger a chain of errors impacting all the modules of a dialogue system: natural language understanding, dialogue state tracking, and response generation. In this work, we first analyze existing dialogue datasets commonly used in research and show that they only ...","track":"Dialogue and Interactive Systems","underline_id":77739,"underline_url":"https://underline.io/events/395/posters/15279/poster/77739-disfluency-generation-for-more-robust-dialogue-systems","video_url":null},{"abstract":"A major challenge in the field of Text Generation is evaluation: Human evaluations are cost-intensive, and automated metrics often display considerable disagreements with human judgments. In this paper, we propose to apply automated metrics for Text Generation in a preference-based evaluation protocol. The protocol features a statistical model that incorporates various levels of uncertainty to account for the error-proneness of the metrics. We show that existing metrics are generally over-confident in assigning significant differences between systems. As a remedy, the model allows to combine human ratings with automated ratings. We show that it can reduce the required amounts of human ratings to arrive at robust and statistically significant results by more than 50\\%, while yielding the same evaluation outcome as the pure human evaluation in 95\\% of cases. We showcase the benefits of the evaluation protocol for three text generation tasks: dialogue systems, machine translation, and text summarization.","anthology_url":"https://aclanthology.org/2023.findings-acl.404","authors":["Jan Deriu","Pius von D\u00e4niken","Don Tuggener","Mark Cieliebak"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-4_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3059","is_paper":true,"keywords":["evaluation methodologies","evaluation","statistical testing for evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.404.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77740/slideshow/2404fa552780123349799b62be6ea8a5.pdf","title":"Correction of Errors in Preference Ratings from Automated Metrics for Text Generation","tldr":"A major challenge in the field of Text Generation is evaluation: Human evaluations are cost-intensive, and automated metrics often display considerable disagreements with human judgments. In this paper, we propose to apply automated metrics for Text Generation in a preference-based evaluation protoc...","track":"Resources and Evaluation","underline_id":77740,"underline_url":"https://underline.io/events/395/posters/15240/poster/77740-correction-of-errors-in-preference-ratings-from-automated-metrics-for-text-generation","video_url":null},{"abstract":"Transferring information retrieval (IR) models from a high-resource language (typically English) to other languages in a zero-shot fashion has become a widely adopted approach. In this work, we show that the effectiveness of zero-shot rankers diminishes when queries and documents are present in different languages. Motivated by this, we propose to train ranking models on artificially code-switched data instead, which we generate by utilizing bilingual lexicons. To this end, we experiment with lexicons induced from (1) cross-lingual word embeddings and (2) parallel Wikipedia page titles. We use the mMARCO dataset to extensively evaluate reranking models on 36 language pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual IR (MLIR). Our results show that code-switching can yield consistent and substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while maintaining stable performance in MoIR. Encouragingly, the gains are especially pronounced for distant languages (up to 2x absolute gain). We further show that our approach is robust towards the ratio of code-switched tokens and also extends to unseen languages. Our results demonstrate that training on code-switched data is a cheap and effective way of generalizing zero-shot rankers for cross-lingual and multilingual retrieval.","anthology_url":"https://aclanthology.org/2023.findings-acl.193","authors":["Robert Litschko","Ekaterina Artemova","Barbara Plank"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-1_-information-retrieval-and-text-mining-(virtual-poster)"],"id":"P3066","is_paper":true,"keywords":["re-ranking"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.193.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77742/poster_document/005337f774f2f792f094bfb0e5d6779a.pdf","preview_image":"https://assets.underline.io/lecture/77742/poster/3e889cb4317b7f9b1216bd2c92fd01d1.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77742/slideshow/fe4ab3f7c7a862944925eefd4bbb97be.pdf","title":"Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data","tldr":"Transferring information retrieval (IR) models from a high-resource language (typically English) to other languages in a zero-shot fashion has become a widely adopted approach. In this work, we show that the effectiveness of zero-shot rankers diminishes when queries and documents are present in diff...","track":"Information Retrieval and Text Mining","underline_id":77742,"underline_url":"https://underline.io/events/395/posters/15200/poster/77742-managertower-aggregating-the-insights-of-uni-modal-experts-for-vision-language-representation-learning","video_url":null},{"abstract":"Machine Reading Comprehension (MRC) models easily learn spurious correlations from complex contexts such as tabular data. Counterfactual training---using the factual and counterfactual data by augmentation---has become a promising solution. However, it is costly to construct faithful counterfactual examples because it is tricky to maintain the consistency and dependency of the tabular data. In this paper, we take a more efficient fashion to ask \\textbf{hypothetical questions} like \\textit{``in which year would the net profit be larger if the revenue in 2019 were \\$38,298?''}, whose effects on the answers are equivalent to those expensive counterfactual tables. We propose a hypothetical training framework that uses paired examples with different hypothetical questions to supervise the direction of model gradient towards the counterfactual answer change. The superior generalization results on tabular MRC datasets, including a newly constructed stress test and MultiHiertt, validate our effectiveness.","anthology_url":"https://aclanthology.org/2023.findings-acl.79","authors":["Moxin Li","Wenjie Wang","Fuli Feng","Hanwang Zhang","Qifan Wang","Tat-Seng Chua"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-7_-question-answering-(virtual-poster)"],"id":"P3072","is_paper":true,"keywords":["generalization"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.79.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77744/poster_document/8fa2c430db3dfd404ff03b36593bd946.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Hypothetical Training for Robust Machine Reading Comprehension of Tabular Context","tldr":"Machine Reading Comprehension (MRC) models easily learn spurious correlations from complex contexts such as tabular data. Counterfactual training---using the factual and counterfactual data by augmentation---has become a promising solution. However, it is costly to construct faithful counterfactual ...","track":"Question Answering","underline_id":77744,"underline_url":"https://underline.io/events/395/posters/15279/poster/77744-hypothetical-training-for-robust-machine-reading-comprehension-of-tabular-context","video_url":null},{"abstract":"Sequence-to-sequence (seq2seq) models based on the Transformer architecture have become a ubiquitous tool applicable not only to classical text generation tasks such as machine translation and summarization but also to any other task where an answer can be represented in a form of a finite text fragment (e.g., question answering).\n  However, when deploying a model in practice, we need not only high performance but also an ability to determine cases where the model is not applicable. Uncertainty estimation (UE) techniques provide a tool for identifying out-of-domain (OOD) input where the model is susceptible to errors.\n  State-of-the-art UE methods for seq2seq models rely on computationally heavyweight and impractical deep ensembles. In this work, we perform an empirical investigation of various novel UE methods for large pre-trained seq2seq models T5 and BART on three tasks: machine translation, text summarization, and question answering. We apply computationally lightweight density-based UE methods to seq2seq models and show that they often outperform heavyweight deep ensembles on the task of OOD detection.","anthology_url":"https://aclanthology.org/2023.findings-acl.93","authors":["Artem Vazhentsev","Akim Tsvigun","Roman Konstantinovich Vashurin","Sergey Petrakov","Daniil Vasilev","Maxim Panov","Alexander Panchenko","Artem Shelmanov"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-7_-generation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3082","is_paper":true,"keywords":["analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.93.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77745/poster_document/e7a0e33ff81b255a8cc58458d4c7e99e.pdf","preview_image":"https://assets.underline.io/lecture/77745/poster/3dcdbfe90387ad21cbabb5fe1258eeeb.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77745/slideshow/41c6039929da4d1344d0b708cb1e63e8.pdf","title":"Efficient Out-of-Domain Detection for Sequence to Sequence Models","tldr":"Sequence-to-sequence (seq2seq) models based on the Transformer architecture have become a ubiquitous tool applicable not only to classical text generation tasks such as machine translation and summarization but also to any other task where an answer can be represented in a form of a finite text frag...","track":"Generation","underline_id":77745,"underline_url":"https://underline.io/events/395/posters/15279/poster/77745-efficient-out-of-domain-detection-for-sequence-to-sequence-models","video_url":null},{"abstract":"The usefulness of part-of-speech tags for parsing has been heavily questioned due to the success of word-contextualized parsers. Yet, most studies are limited to coarse-grained tags and high quality written content; while we know little about their influence when it comes to models in production that face lexical errors. We expand these setups and design an adversarial attack to verify if the use of morphological information by parsers: (i) contributes to error propagation or (ii) if on the other hand it can play a role to correct mistakes that word-only neural parsers make. The results on 14 diverse UD treebanks show that under such attacks, for transition- and graph-based models their use contributes to degrade the performance even faster, while for the (lower-performing) sequence labeling parsers they are helpful. We also show that if morphological tags were utopically robust against lexical perturbations, they would be able to correct parsing mistakes.","anthology_url":"https://aclanthology.org/2023.findings-acl.459","authors":["Alberto Mu\u00f1oz-Ortiz","David Vilares"],"category":"Findings","demo_url":null,"display_track":"Syntax: Tagging, Chunking, and Parsing","event_ids":["session-1_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)"],"id":"P3091","is_paper":true,"keywords":["dependency parsing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.459.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77746/poster_document/eb23819170f93f867a568900173d8c23.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Another Dead End for Morphological Tags? Perturbed Inputs and Parsing","tldr":"The usefulness of part-of-speech tags for parsing has been heavily questioned due to the success of word-contextualized parsers. Yet, most studies are limited to coarse-grained tags and high quality written content; while we know little about their influence when it comes to models in production tha...","track":"Syntax: Tagging, Chunking, and Parsing","underline_id":77746,"underline_url":"https://underline.io/events/395/posters/15200/poster/77746-another-dead-end-for-morphological-tagsquestion-perturbed-inputs-and-parsing","video_url":null},{"abstract":"Metaphors are highly creative constructs of human language that grow old and eventually die. Popular datasets used for metaphor processing tasks were constructed from dated source texts. In this paper, we propose NewsMet, a large high-quality contemporary dataset of news headlines hand-annotated with metaphorical verbs. The dataset comprises headlines from various sources including political, satirical, reliable and fake. Our dataset serves the purpose of evaluation for the tasks of metaphor interpretation and generation. \nThe experiments reveal several insights and limitations of using LLMs to automate metaphor processing tasks as frequently seen in the recent literature. The dataset is publicly available for research purposes{{https://github.com/AxleBlaze3/NewsMet\\_Metaphor\\_Dataset}}.","anthology_url":"https://aclanthology.org/2023.findings-acl.641","authors":["Rohan Joseph","Timothy Liu","Aik Beng Ng","Simon See","Sunny Rai"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)"],"id":"P3107","is_paper":true,"keywords":["nlp datasets","evaluation","metrics"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.641.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77747/poster_document/4f6b37fa91d1973e05bbcb23528a41d6.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"NewsMet : A 'do it all' Dataset of Contemporary Metaphors in News Headlines","tldr":"Metaphors are highly creative constructs of human language that grow old and eventually die. Popular datasets used for metaphor processing tasks were constructed from dated source texts. In this paper, we propose NewsMet, a large high-quality contemporary dataset of news headlines hand-annotated wit...","track":"Resources and Evaluation","underline_id":77747,"underline_url":"https://underline.io/events/395/posters/15279/poster/77747-metanews-a-do-it-all-dataset-of-contemporary-metaphors-in-news-headlines","video_url":null},{"abstract":"Temporal Knowledge Graph Completion aims to complete missing entities or relations under temporal constraints. Previous tensor decomposition-based models for TKGC only independently consider the combination of one single relation with one single timestamp, ignoring the global nature of the embedding. We propose a Frequency Attention (FA) model to capture the global temporal dependencies between one relation and the entire timestamp. Specifically, we use Discrete Cosine Transform (DCT) to capture the frequency of the timestamp embedding and further compute the frequency attention weight to scale embedding. Meanwhile, the previous temporal tucker decomposition method uses a simple norm regularization to constrain the core tensor, which limits the optimization performance. Thus, we propose Orthogonal Regularization (OR) variants for the core tensor, which can limit the non-superdiagonal elements of the 3-rd core tensor. Experiments on three standard TKGC datasets demonstrate that our method outperforms the state-of-the-art results on several metrics. The results suggest that the direct-current component is not the best feature for TKG representation learning. Additional analysis shows the effectiveness of our FA and OR models, even with smaller embedding dimensions.","anthology_url":"https://aclanthology.org/2023.findings-acl.458","authors":["Likang Xiao","Richong Zhang","Zijie Chen","Junfan Chen"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)"],"id":"P3114","is_paper":true,"keywords":["knowledge graphs"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.458.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77748/poster_document/c0d2779f63b54c53c1571753598e44d2.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Tucker Decomposition with Frequency Attention for Temporal Knowledge Graph Completion","tldr":"Temporal Knowledge Graph Completion aims to complete missing entities or relations under temporal constraints. Previous tensor decomposition-based models for TKGC only independently consider the combination of one single relation with one single timestamp, ignoring the global nature of the embedding...","track":"NLP Applications","underline_id":77748,"underline_url":"https://underline.io/events/395/posters/15240/poster/77748-tucker-decomposition-with-frequency-attention-for-temporal-knowledge-graph-completion","video_url":null},{"abstract":"Multilingual language models have recently gained attention as a promising solution for representing multiple languages in a single model. In this paper, we propose new criteria to evaluate the quality of lexical representation and vocabulary overlap observed in sub-word tokenizers.\nOur findings show that the overlap of vocabulary across languages can be actually detrimental to certain downstream tasks (POS, dependency tree labeling). In contrast, NER and sentence-level tasks (cross-lingual retrieval, NLI) benefit from sharing vocabulary. We also observe that the coverage of the language-specific tokens in the multilingual vocabulary significantly impacts the word-level tasks. Our study offers a deeper understanding of the role of tokenizers in multilingual language models and guidelines for future model developers to choose the most suitable tokenizer for their specific application before undertaking costly model pre-training.","anthology_url":"https://aclanthology.org/2023.findings-acl.350","authors":["Tomasz Limisiewicz","Ji\u0159\u00ed Balhar","David Mare\u010dek"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3121","is_paper":true,"keywords":["mutlilingual representations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.350.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77750/poster_document/606bdc18296fd20fb912176e2d227327.pdf","preview_image":"https://assets.underline.io/lecture/77750/poster/382ba189f967a110c3ece8f382146e78.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77750/slideshow/28833dce63ed4e88b480752fd96cd3ec.pdf","title":"Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages","tldr":"Multilingual language models have recently gained attention as a promising solution for representing multiple languages in a single model. In this paper, we propose new criteria to evaluate the quality of lexical representation and vocabulary overlap observed in sub-word tokenizers.\nOur findings sho...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77750,"underline_url":"https://underline.io/events/395/posters/15279/poster/77750-tokenization-impacts-multilingual-language-modeling-assessing-vocabulary-allocation-and-overlap-across-languages","video_url":null},{"abstract":"Information extraction (IE) systems aim to automatically extract structured information, such as named entities, relations between entities, and events, from unstructured texts. While most existing work addresses a particular IE task, universally modeling various IE tasks with one model has achieved great success recently. Despite their success, they employ a one-stage learning strategy, i.e., directly learning to extract the target structure given the input text, which contradicts the human learning process. In this paper, we propose a unified easy-to-hard learning framework consisting of three stages, i.e., the easy stage, the hard stage, and the main stage, for IE by mimicking the human learning process. By breaking down the learning process into multiple stages, our framework facilitates the model to acquire general IE task knowledge and improve its generalization ability.  Extensive experiments across four IE tasks demonstrate the effectiveness of our framework. We achieve new state-of-the-art results on 13 out of 17 datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.754","authors":["Chang Gao","Wenxuan Zhang","Wai Lam","Lidong Bing"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-4_-information-extraction-(virtual-poster)"],"id":"P3147","is_paper":true,"keywords":["named entity recognition and relation extraction","event extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.754.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77753/poster_document/dc1fc295697d0ef17995e6b01ce948e4.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77753/slideshow/bda047f76aebedd3ab408af1337ce6f6.pdf","title":"Easy-to-Hard Learning for Information Extraction","tldr":"Information extraction (IE) systems aim to automatically extract structured information, such as named entities, relations between entities, and events, from unstructured texts. While most existing work addresses a particular IE task, universally modeling various IE tasks with one model has achieved...","track":"Information Extraction","underline_id":77753,"underline_url":"https://underline.io/events/395/posters/15240/poster/77753-easy-to-hard-learning-for-information-extraction","video_url":null},{"abstract":"Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of diff pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation.","anthology_url":"https://aclanthology.org/2023.findings-acl.386","authors":["Lukas Hauzenberger","Shahed Masoudian","Deepak Kumar","Markus Schedl","Navid Rekabsaz"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)"],"id":"P3150","is_paper":true,"keywords":["model bias/unfairness mitigation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.386.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77754/poster_document/3b0a2a2eb768934c816637e685726b9c.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks","tldr":"Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reac...","track":"Ethics and NLP","underline_id":77754,"underline_url":"https://underline.io/events/395/posters/15279/poster/77754-modular-and-on-demand-bias-mitigation-with-attribute-removal-subnetworks","video_url":null},{"abstract":"Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external variables like the speaker's gender. This work introduces MTCue, a novel neural machine translation (NMT) framework that interprets all context (including discrete variables) as text. MTCue learns an abstract representation of context, enabling transferability across different data settings and leveraging similar attributes in low-resource scenarios. With a focus on a dialogue domain with access to document and metadata context, we extensively evaluate MTCue in four language pairs in both translation directions. Our framework demonstrates significant improvements in translation quality over a parameter-matched non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58). Moreover, MTCue significantly outperforms a \"tagging\" baseline at translating English text. Analysis reveals that the context encoder of MTCue learns a representation space that organises context based on specific attributes, such as formality, enabling effective zero-shot control. Pre-training on context embeddings also improves MTCue's few-shot performance compared to the \"tagging\" baseline. Finally, an ablation study conducted on model components and contextual variables further supports the robustness of MTCue for context-based NMT.","anthology_url":"https://aclanthology.org/2023.findings-acl.521","authors":["Sebastian T. Vincent","Robert James Flynn","Carolina Scarton"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-4_-machine-translation-(virtual-poster)"],"id":"P3152","is_paper":true,"keywords":["few-shot/zero-shot mt","modelling"],"languages":["german","polish","french"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.521.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77755/poster_document/661780138ea9679733cf91e585426cb0.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation","tldr":"Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external vari...","track":"Machine Translation","underline_id":77755,"underline_url":"https://underline.io/events/395/posters/15240/poster/77755-mtcue-learning-zero-shot-control-of-extra-textual-attributes-by-leveraging-unstructured-context-in-neural-machine-translation","video_url":null},{"abstract":"The generative retrieval model depends solely on the information encoded in its model parameters without external memory, its information capacity is limited and fixed. To overcome the limitation, we propose Nonparametric Decoding (Np Decoding) which can be applied to existing generative retrieval models. Np Decoding uses nonparametric contextualized vocab embeddings (external memory) rather than vanilla vocab embeddings as decoder vocab embeddings. By leveraging the contextualized vocab embeddings, the generative retrieval model is able to utilize both the parametric and nonparametric space. Evaluation over 9 datasets (8 single-hop and 1 multi-hop) in the document retrieval task shows that applying Np Decoding to generative retrieval models significantly improves the performance. We also show that Np Decoding is data- and parameter-efficient, and shows high performance in the zero-shot setting.","anthology_url":"https://aclanthology.org/2023.findings-acl.801","authors":["Hyunji Lee","JaeYoung Kim","Hoyeon Chang","hanseok Oh","Sohee Yang","Vladimir Karpukhin","Yi Lu","Minjoon Seo"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-7_-information-retrieval-and-text-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3153","is_paper":true,"keywords":["passage retrieval","document representation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.801.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77756/poster_document/4aa013a262df2580b9438023babeed6e.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Nonparametric Decoding for Generative Retrieval","tldr":"The generative retrieval model depends solely on the information encoded in its model parameters without external memory, its information capacity is limited and fixed. To overcome the limitation, we propose Nonparametric Decoding (Np Decoding) which can be applied to existing generative retrieval m...","track":"Information Retrieval and Text Mining","underline_id":77756,"underline_url":"https://underline.io/events/395/posters/15279/poster/77756-nonparametric-decoding-for-generative-retrieval","video_url":null},{"abstract":"Task-oriented dialogue (ToD) systems have been widely deployed in many industries as they deliver more efficient customer support.  These systems are typically constructed for a single domain or language and do not generalise well beyond this.  To support work on Natural Language Understanding (NLU) in ToD across multiple languages and domains simultaneously, we constructed Multi3NLU++, a multilingual, multi-intent, multi-domain dataset. Multi3NLU++ extends the English-only NLU++ dataset to include manual translations into a range of high, medium, and low resource languages (Spanish, Marathi, Turkish and Amharic), in two domains (banking and hotels). Because of its multi-intent property, Multi3NLU++ represents complex and natural user goals, and therefore allows us to measure the realistic performance of ToD systems in a varied set of the world's languages. We use Multi3NLU++ to benchmark state-of-the-art multilingual models for the NLU tasks of intent detection and slot labeling for ToD systems in the multilingual setting. The results demonstrate the challenging nature of the dataset, particularly in the low-resource language setting, offering ample room for future experimentation in multi-domain multilingual ToD setups.","anthology_url":"https://aclanthology.org/2023.findings-acl.230","authors":["Nikita Moghe","Evgeniia Razumovskaia","Liane K Guillou","Ivan Vuli\u0107","Anna Korhonen","Alexandra Birch"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3165","is_paper":true,"keywords":["task-oriented","multilingual / low resource"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.230.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77759/poster_document/9fbaa5d6953ff9f58915636c80a63154.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77759/slideshow/179bf8bb4ad897c7617456b5e8e4b008.pdf","title":"Multi3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue","tldr":"Task-oriented dialogue (ToD) systems have been widely deployed in many industries as they deliver more efficient customer support.  These systems are typically constructed for a single domain or language and do not generalise well beyond this.  To support work on Natural Language Understanding (NLU)...","track":"Dialogue and Interactive Systems","underline_id":77759,"underline_url":"https://underline.io/events/395/posters/15200/poster/77759-multi3nlu-a-multilingual-multi-intent-multi-domain-dataset-for-natural-language-understanding-in-task-oriented-dialogue","video_url":null},{"abstract":"Work on hate speech has made considering rude and harmful examples in scientific publications inevitable. This situation raises various problems, such as whether or not to obscure profanities. While science must accurately disclose what it does, the unwarranted spread of hate speech can harm readers and increases its internet frequency. While maintaining publications' professional appearance, obfuscating profanities makes it challenging to evaluate the content, especially for non-native speakers.\nSurveying 150 ACL papers, we discovered that obfuscation is usually used for English but not other languages, and even then, quite unevenly.\nWe discuss the problems with obfuscation and suggest a multilingual community resource called PrOf with a Python module to standardize profanity obfuscation processes. We believe PrOf can help scientific publication policies to make hate speech work accessible and comparable, irrespective of language.","anthology_url":"https://aclanthology.org/2023.findings-acl.240","authors":["Debora Nozza","Dirk Hovy"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)"],"id":"P3184","is_paper":true,"keywords":["language resources"],"languages":["french","german","italian","spanish"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.240.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77762/poster_document/1b3f52be2d8a4090626a11898226da98.pdf","preview_image":"https://assets.underline.io/lecture/77762/poster/302f44bef593d79012c96c5093e0a68b.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77762/slideshow/83f301889d8f8e76107d579e5c6a4cb0.pdf","title":"The State of Profanity Obfuscation in Natural Language Processing Scientific Publications","tldr":"Work on hate speech has made considering rude and harmful examples in scientific publications inevitable. This situation raises various problems, such as whether or not to obscure profanities. While science must accurately disclose what it does, the unwarranted spread of hate speech can harm readers...","track":"Resources and Evaluation","underline_id":77762,"underline_url":"https://underline.io/events/395/posters/15279/poster/77762-lego-mt-learning-detachable-models-for-massively-multilingual-machine-translation","video_url":null},{"abstract":"In real-world systems, scaling has been critical for improving the translation quality in autoregressive translation (AT), which however has not been well studied for non-autoregressive translation (NAT). In this work, we bridge the gap by systematically studying the impact of scaling on NAT behaviors. Extensive experiments on six WMT benchmarks over two advanced NAT models show that scaling can alleviate the commonly-cited weaknesses of NAT models, resulting in better translation performance. To reduce the side-effect of scaling on decoding speed, we empirically investigate the impact of NAT encoder and decoder on the translation performance. Experimental results on the large-scale WMT20 En-De show that the asymmetric architecture (e.g. bigger encoder and smaller decoder) can achieve comparable performance with the scaling model, while maintaining the superiority of decoding speed with standard NAT models. To this end, we establish a new benchmark by validating scaled NAT models on the scaled dataset, which can be regarded as a strong baseline for future works. We release code and system outputs at https://github.com/DeepLearnXMU/Scaling4NAT.","anthology_url":"https://aclanthology.org/2023.findings-acl.763","authors":["Zhihao Wang","Longyue Wang","Jinsong Su","Junfeng Yao","Zhaopeng Tu"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-4_-theme_-reality-check-(virtual-poster)"],"id":"P3188","is_paper":true,"keywords":["lessons from deployment","evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.763.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77764/poster_document/b649e8730fa9b0a1e1c8c957e0561e4e.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Revisiting Non-Autoregressive Translation at Scale","tldr":"In real-world systems, scaling has been critical for improving the translation quality in autoregressive translation (AT), which however has not been well studied for non-autoregressive translation (NAT). In this work, we bridge the gap by systematically studying the impact of scaling on NAT behavio...","track":"Theme: Reality Check","underline_id":77764,"underline_url":"https://underline.io/events/395/posters/15240/poster/77764-revisiting-non-autoregressive-translation-at-scale","video_url":null},{"abstract":"In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning. Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using techniques such as few-shot prompting, chain-of-thought (CoT), and external memory. Our human evaluation results show that MPC is on par with fine-tuned chatbot models in open-domain conversations, making it an effective solution for creating consistent and engaging chatbots.","anthology_url":"https://aclanthology.org/2023.findings-acl.277","authors":["Gibbeum Lee","Volker Hartmann","Jongho Park","Dimitris Papailiopoulos","Kangwook Lee"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3190","is_paper":true,"keywords":["evaluation and metrics","conversational modeling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.277.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77765/poster_document/e240588ee70f4c760870a87d52a8646d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77765/slideshow/0e01487fb3579edc3fce9f799aaf36ec.pdf","title":"Prompted LLMs as Chatbot Modules for Long Open-domain Conversation","tldr":"In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning. Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using techniques...","track":"Dialogue and Interactive Systems","underline_id":77765,"underline_url":"https://underline.io/events/395/posters/15200/poster/77765-prompted-llms-as-chatbot-modules-for-long-open-domain-conversation","video_url":null},{"abstract":"Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy makes the evaluation results on the same dataset not directly comparable, but the data preprocessing details are not widely noted and specified in papers. (2) The output space discrepancy of different model paradigms makes different-paradigm EE models lack grounds for comparison and also leads to unclear mapping issues between predictions and annotations. (3) The absence of pipeline evaluation of many EAE-only works makes them hard to be directly compared with EE works and may not well reflect the model performance in real-world pipeline scenarios. We demonstrate the significant influence of these pitfalls through comprehensive meta-analyses of recent papers and empirical experiments. To avoid these pitfalls, we suggest a series of remedies, including specifying data preprocessing, standardizing outputs, and providing pipeline evaluation results. To help implement these remedies, we develop a consistent evaluation framework OmniEvent, which can be obtained from https://github.com/THU-KEG/OmniEvent.","anthology_url":"https://aclanthology.org/2023.findings-acl.586","authors":["Hao Peng","Xiaozhi Wang","Feng Yao","Kaisheng Zeng","Lei Hou","Juanzi Li","Zhiyuan Liu","Weixing Shen"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-7_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3225","is_paper":true,"keywords":["evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.586.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77768/poster_document/049b59f39f4a6e51b7214666b2743940.pdf","preview_image":"https://assets.underline.io/lecture/77768/poster/a8139b58fd305e58452fa59f325cfc10.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77768/slideshow/22dd16fd7ac933a3f340ec519dc5972f.pdf","title":"The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation","tldr":"Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy m...","track":"Theme: Reality Check","underline_id":77768,"underline_url":"https://underline.io/events/395/posters/15279/poster/77768-the-devil-is-in-the-details-on-the-pitfalls-of-event-extraction-evaluation","video_url":null},{"abstract":"A typical product or place often has hundreds of reviews, and summarization of these texts is an important and challenging problem. Recent progress on abstractive summarization in domains such as news has been driven by supervised systems trained on hundreds of thousands of news articles paired with human-written summaries. However for opinion texts, such large scale datasets are rarely available. Unsupervised methods, self-training, and few-shot learning approaches bridge that gap. In this work, we present a novel self-training approach, OpineSum for abstractive opinion summarization. The self-training summaries in this approach are built automatically using a novel application of textual entailment and capture the consensus of opinions across the various reviews for an item. This method can be used to obtain silver-standard summaries on a large scale and train both unsupervised and few-shot abstractive summarization systems. OpineSum outperforms strong peer systems in both settings.","anthology_url":"https://aclanthology.org/2023.findings-acl.686","authors":["Annie Louis","Joshua Maynez"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-7_-summarization-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3234","is_paper":true,"keywords":["abstractive summarisation","multi-document summarization"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.686.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"OpineSum: Entailment-based self-training for abstractive opinion summarization","tldr":"A typical product or place often has hundreds of reviews, and summarization of these texts is an important and challenging problem. Recent progress on abstractive summarization in domains such as news has been driven by supervised systems trained on hundreds of thousands of news articles paired with...","track":"Summarization","underline_id":77770,"underline_url":"https://underline.io/events/395/posters/15279/poster/77770-opinesum-entailment-based-self-training-for-abstractive-opinion-summarization","video_url":null},{"abstract":"Recent advances in QA pair generation (QAG) have raised interest in applying this technique to the educational field. However, the diversity of QA types remains a challenge despite its contributions to comprehensive learning and assessment of children. In this paper, we propose a QAG framework that enhances QA type diversity by producing different interrogative sentences and implicit/explicit answers. Our framework comprises a QFS-based answer generator, an iterative QA generator, and a relevancy-aware ranker. The two generators aim to expand the number of candidates while covering various types. The ranker trained on the in-context negative samples clarifies the top-N outputs based on the ranking score. Extensive evaluations and detailed analyses demonstrate that our approach outperforms previous state-of-the-art results by significant margins, achieving improved diversity and quality. Our task-oriented processes are consistent with real-world demand, which highlights our system's high applicability.","anthology_url":"https://aclanthology.org/2023.findings-acl.380","authors":["Sugyeong Eo","Hyeonseok Moon","Jinsung Kim","Yuna Hur","Jeongwook Kim","SongEun Lee","Changwoo Chun","SUNGSOO PARK","Heuiseok Lim"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3242","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.380.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77771/poster_document/e3a029aa9466fc44a9b79dba2d5466b8.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Towards Diverse and Effective Question-Answer Pair Generation from Children Storybooks","tldr":"Recent advances in QA pair generation (QAG) have raised interest in applying this technique to the educational field. However, the diversity of QA types remains a challenge despite its contributions to comprehensive learning and assessment of children. In this paper, we propose a QAG framework that ...","track":"NLP Applications","underline_id":77771,"underline_url":"https://underline.io/events/395/posters/15240/poster/77771-towards-diverse-and-effective-question-answer-pair-generation-from-children-storybooks","video_url":null},{"abstract":"Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to domain generalization for abstractive summarization. Given a number of source domains, our method first trains a prefix for each one of them. These source prefixes generate summaries for a small number of target domain documents. The similarity of the generated summaries to their corresponding source documents is used for calculating weights required to average source prefixes. In DAPA, prefix tuning allows for lightweight finetuning, and weight averaging allows for the computationally efficient addition of new source domains. When evaluated on four diverse summarization domains, DAPA shows comparable or better performance against the baselines demonstrating the effectiveness of its prefix averaging scheme.","anthology_url":"https://aclanthology.org/2023.findings-acl.288","authors":["Pranav Ajit Nair","Sukomal Pal","Pradeepika Verma"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)"],"id":"P3281","is_paper":true,"keywords":["generalization","parameter-efficient finetuning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.288.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77773/poster_document/8d9c0e3ecea69ea427a3f728cc8b12a8.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77773/slideshow/ba9a79dc1eee891679fca71e742454b8.pdf","title":"Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization","tldr":"Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to do...","track":"Machine Learning for NLP","underline_id":77773,"underline_url":"https://underline.io/events/395/posters/15200/poster/77773-domain-aligned-prefix-averaging-for-domain-generalization-in-abstractive-summarization","video_url":null},{"abstract":"Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM. We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used. Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model.","anthology_url":"https://aclanthology.org/2023.findings-acl.144","authors":["Fran Jeleni\u0107","Josip Juki\u0107","Nina Drobac","Jan Snajder"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)"],"id":"P3287","is_paper":true,"keywords":["human-in-the-loop / active learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.144.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77774/poster_document/7cd18de82f37b848a03b203b3d347d79.pdf","preview_image":"https://assets.underline.io/lecture/77774/poster/ee586656e26a1dd313917a2795507f83.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77774/slideshow/0b001746b9af75fd8e79560c9822dfde.pptx","title":"On Dataset Transferability in Active Learning for Transformers","tldr":"Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one mod...","track":"Machine Learning for NLP","underline_id":77774,"underline_url":"https://underline.io/events/395/posters/15240/poster/77774-on-dataset-transferability-in-active-learning-for-transformers","video_url":null},{"abstract":"Motivated by the question of the extent to which large language models \"understand\" social intelligence, we investigate the ability of such models to generate correct responses to questions involving descriptions of faux pas situations. The faux pas test is a test used in clinical psychology, which is known to be more challenging for children than individual tests of theory-of-mind or social intelligence. Our results demonstrate that, while the models seem to sometimes offer correct responses, they in fact struggle with this task, and that many of the seemingly correct responses can be attributed to over-interpretation by the human reader (\"the ELIZA effect\"). An additional phenomenon observed is the failure of most models to generate a correct response to presupposition questions. Finally, in an experiment in which the models are tasked with generating original faux pas stories, we find that while some models are capable of generating novel faux pas stories, the stories are all explicit, as the models are limited in their abilities to describe situations in an implicit manner.","anthology_url":"https://aclanthology.org/2023.findings-acl.663","authors":["Natalie Shapira","Guy Zwirn","Yoav Goldberg"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-1_-discourse-and-pragmatics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3308","is_paper":true,"keywords":["coherence"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.663.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77777/poster_document/d7dbceaa3d17171903f5830f668bdbf2.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"How Well Do Large Language Models Perform on Faux Pas Tests?","tldr":"Motivated by the question of the extent to which large language models \"understand\" social intelligence, we investigate the ability of such models to generate correct responses to questions involving descriptions of faux pas situations. The faux pas test is a test used in clinical psychology, which ...","track":"Discourse and Pragmatics","underline_id":77777,"underline_url":"https://underline.io/events/395/posters/15200/poster/77777-how-well-do-large-language-models-perform-on-faux-pas-testsquestion","video_url":null},{"abstract":"Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple, versatile and efficient regularization which can be applied to various supervised classification tasks. Conventional LS, however, regardless of the training instance assumes that each non-target class is equally likely.\nIn this work, we present a general framework for training with label regularization, which includes conventional LS but can also model instance-specific variants. Based on this formulation, we propose an efficient way of learning LAbel regularization by devising a Bi-level Optimization (LABO) problem. We derive a deterministic and interpretable solution of the inner loop as the optimal label smoothing without the need to store the parameters or the output of a trained model. \nFinally, we conduct extensive experiments and demonstrate our LABO consistently yields improvement over conventional label regularization on various fields, including seven machine translation and three image classification tasks across various neural network architectures while maintaining training efficiency.","anthology_url":"https://aclanthology.org/2023.findings-acl.356","authors":["Peng Lu","Ahmad Rashid","Ivan Kobyzev","Mehdi Rezagholizadeh","Phillippe Langlais"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P331","is_paper":true,"keywords":["optimization methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.356.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77295/poster_document/d42f72414d9651993d51d352911c4bb1.pdf","preview_image":"https://assets.underline.io/lecture/77295/poster/d6248c44c854da41c9b763c58b236a11.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization","tldr":"Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple, versati...","track":"Machine Learning for NLP","underline_id":77295,"underline_url":"https://underline.io/events/395/posters/15240/poster/77295-labo-towards-learning-optimal-label-regularization-via-bi-level-optimization","video_url":null},{"abstract":"Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language models on a corpus of German Easy Language, a specific style of German. Then, we used these models as decoders in a sequence-to-sequence simplification task. We show that the language models adapt to the style characteristics of Easy Language and output more accessible texts. Moreover, with the style-specific pre-training, we reduced the number of trainable parameters in text simplification models. Hence, less parallel data is sufficient for training. Our results indicate that pre-training on unaligned data can reduce the required parallel data while improving the performance on downstream tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.74","authors":["Miriam Ansch\u00fctz","Joshua Oehms","Thomas Wimmer","Bart\u0142omiej Jezierski","Georg Groh"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-1_-summarization-(virtual-poster)"],"id":"P3313","is_paper":true,"keywords":["abstractive summarisation"],"languages":["german"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.74.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77778/poster_document/1ccaeaf1bfac91066189ae89f05712ce.pdf","preview_image":"https://assets.underline.io/lecture/77778/poster/6ec92edf43f3ad0b7c6d3c956bf67512.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77778/slideshow/02dbd57e638c4b111420b7a5cae52e6d.pdf","title":"Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training","tldr":"Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language model...","track":"Summarization","underline_id":77778,"underline_url":"https://underline.io/events/395/posters/15200/poster/77778-language-models-for-german-text-simplification-overcoming-parallel-data-scarcity-through-style-specific-pre-training","video_url":null},{"abstract":"With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one's argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose ClaimDIff, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In ClaimDiff, we provide human-labeled 2,941 claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19\\% absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided comparison.","anthology_url":"https://aclanthology.org/2023.findings-acl.289","authors":["Miyoung Ko","Ingyu Seong","Hwaran Lee","Joonsuk Park","Minsuk Chang","Minjoon Seo"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-1_-nlp-applications-(virtual-poster)"],"id":"P3326","is_paper":true,"keywords":["fact checking, rumour/misinformation detection"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.289.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77780/poster_document/33d64733c45ef17f3780a686d10624cf.pdf","preview_image":"https://assets.underline.io/lecture/77780/poster/859bbfa427bc8638de5c9cb31cf02932.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77780/slideshow/f51f0bce1a677fac825eef8db9fb5365.pdf","title":"ClaimDiff: Comparing and Contrasting Claims on Contentious Issues","tldr":"With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially...","track":"NLP Applications","underline_id":77780,"underline_url":"https://underline.io/events/395/posters/15200/poster/77780-c-stance-a-large-dataset-for-chinese-zero-shot-stance-detection","video_url":null},{"abstract":"In recent years, pre-trained transformer-based language models (LM) have become a key resource for implementing most NLP tasks. However, pre-training such models demands large text collections not available in most languages. In this paper, we study the use of machine-translated corpora for pre-training LMs. We answer the following research questions: RQ1: Is MT-based data an alternative to real data for learning a LM?; RQ2: Can real data be complemented with translated data and improve the resulting LM? In order to validate these two questions, several BERT models for Basque have been trained, combining real data and synthetic data translated from Spanish.\nThe evaluation carried out on 9 NLU tasks indicates that models trained exclusively on translated data offer competitive results. Furthermore, models trained with real data can be improved with synthetic data, although further research is needed on the matter.","anthology_url":"https://aclanthology.org/2023.findings-acl.235","authors":["Gorka Urbizu","I\u00f1aki San Vicente","Xabier Saralegi","Ander Corral"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3340","is_paper":true,"keywords":["data augmentation"],"languages":["basque"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.235.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77782/slideshow/3d062b7b3cf8bc98056370bb85966bcc.pdf","title":"Not Enough Data to Pre-train Your Language Model? MT to the Rescue!","tldr":"In recent years, pre-trained transformer-based language models (LM) have become a key resource for implementing most NLP tasks. However, pre-training such models demands large text collections not available in most languages. In this paper, we study the use of machine-translated corpora for pre-trai...","track":"Machine Learning for NLP","underline_id":77782,"underline_url":"https://underline.io/events/395/posters/15200/poster/77782-not-enough-data-to-pre-train-your-language-modelquestion-mt-to-the-rescue","video_url":null},{"abstract":"Large language models are very resource intensive, both financially and environmentally, and require an amount of training data which is simply unobtainable for the majority of NLP practitioners. Previous work has researched the scaling laws of such models, but optimal ratios of model parameters, dataset size, and computation costs focused on the large scale. In contrast, we analyze the effect those variables have on the performance of language models in constrained settings, by building three lightweight BERT models (16M/51M/124M parameters) trained over a set of small corpora (5M/25M/125M words).\nWe experiment on four languages of different linguistic characteristics (Basque, Spanish, Swahili and Finnish), and evaluate the models on MLM and several NLU tasks. We conclude that the power laws for parameters, data and compute for low-resource settings differ from the optimal scaling laws previously inferred, and data requirements should be higher. Our insights are consistent across all the languages we study, as well as across the MLM and downstream tasks. Furthermore, we experimentally establish when the cost of using a Transformer-based approach is worth taking, instead of favouring other computationally lighter solutions.","anthology_url":"https://aclanthology.org/2023.findings-acl.492","authors":["Gorka Urbizu","I\u00f1aki San Vicente","Xabier Saralegi","Rodrigo Agerri","Aitor Soroa"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-4_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3356","is_paper":true,"keywords":["scaling"],"languages":["basque","spanish","swahili","finnish"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.492.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77784/slideshow/9e6c2fe5c1a12468d52b731968a48b7a.pdf","title":"Scaling Laws for BERT in Low-Resource Settings","tldr":"Large language models are very resource intensive, both financially and environmentally, and require an amount of training data which is simply unobtainable for the majority of NLP practitioners. Previous work has researched the scaling laws of such models, but optimal ratios of model parameters, da...","track":"Large Language Models","underline_id":77784,"underline_url":"https://underline.io/events/395/posters/15240/poster/77784-scaling-laws-for-bert-in-low-resource-settings","video_url":null},{"abstract":"The task of textual geolocation \u2014 retrieving the coordinates of a place based on a free-form language description \u2014 calls for not only grounding but also natural language understanding and geospatial reasoning. Even though there are quite a few datasets in English used for geolocation, they are currently based on open-source data (Wikipedia and Twitter), where the location of the described place is mostly implicit, such that the location retrieval resolution is limited. Furthermore, there are no datasets available for addressing the problem of textual geolocation in morphologically rich and resource-poor languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to collect literal place descriptions and analyze lingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place descriptions of various place types in three cities in Israel. Qualitative and empirical analysis show that the data exhibits abundant use of geospatial reasoning and requires a novel environmental representation.","anthology_url":"https://aclanthology.org/2023.findings-acl.460","authors":["Tzuf Paz-Argaman","Tal Bauman","Itai Mondshine","Itzhak Omer","Sagi Dalyot","Reut Tsarfaty"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-4_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3371","is_paper":true,"keywords":["corpus creation","language resources","nlp datasets","datasets for low resource languages"],"languages":["hebrew"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.460.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77786/poster_document/620a71dee3477894a3ca401a955181b1.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77786/slideshow/f93e836d7bf081df805c2fe51306d986.pdf","title":"HeGeL: A Novel Dataset for Geo-Location from Hebrew Text","tldr":"The task of textual geolocation \u2014 retrieving the coordinates of a place based on a free-form language description \u2014 calls for not only grounding but also natural language understanding and geospatial reasoning. Even though there are quite a few datasets in English used for geolocation, they are curr...","track":"Resources and Evaluation","underline_id":77786,"underline_url":"https://underline.io/events/395/posters/15240/poster/77786-hegel-a-novel-dataset-for-geo-location-from-hebrew-text","video_url":null},{"abstract":"Recent studies have pointed out that many well-developed Visual Question Answering (VQA) systems suffer from bias problem. Despite the remarkable performance gained on In-Distribution (ID) datasets,  the VQA model might merely capture the superficial correlation from question to answer rather than showing real reasoning abilities. Therefore, when switching to Out-of-Distribution (OOD) dataset, whose test distribution is unknown or even reversed with the training set, significant drops might be demonstrated.\nAlthough efforts have been devoted to easing the negative bias effect brought by language prior and analysing its  inherent cause, they are still limited by the following two aspects. First, most current debiasing methods achieve promising OOD generalization ability with a major sacrifice of the ID performance. Second, existing researches are restricted by exploiting comprehensive biases, since weakening the language bias is mainly focused, while only a few works consider vision bias. In this paper, we investigate a straightforward way to mitigate bias problem for VQA task. Specifically, we reduce bias effect by subtracting bias score from standard VQA base score. Based on such a direct strategy, we design two bias learning branches to detect more bias information, which are combined with a dynamical constraint loss to alleviate the problem of over-correction and insufficient debiasing effect. We evaluate our method on the challenging VQA v2.0 and VQA-CP V2,0 datasets and the proposed method achievessignificant improvement.","anthology_url":"https://aclanthology.org/2023.findings-acl.311","authors":["Yu Li","Bojie Hu","Fengshuo Zhang","Yahan Yu","Jian Liu","Yufeng Chen","Jinan Xu"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)"],"id":"P3397","is_paper":true,"keywords":["multimodal qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.311.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77789/poster_document/0fc008b5e1c801805f67426d364ec12d.pdf","preview_image":"https://assets.underline.io/lecture/77789/poster/abaff1eb73e96a384825aedb9f13fb61.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"A Multi-modal Debiasing Model with Dynamical Constraint for Robust Visual Question Answering","tldr":"Recent studies have pointed out that many well-developed Visual Question Answering (VQA) systems suffer from bias problem. Despite the remarkable performance gained on In-Distribution (ID) datasets,  the VQA model might merely capture the superficial correlation from question to answer rather than s...","track":"Question Answering","underline_id":77789,"underline_url":"https://underline.io/events/395/posters/15240/poster/77789-a-multi-modal-debiasing-model-with-dynamical-constraint-for-robust-visual-question-answering","video_url":null},{"abstract":"Sparsely gated Mixture of Experts (MoE) models have been shown to be a compute-efficient method to scale model capacity for multilingual machine translation. However, for low-resource tasks, MoE models severely over-fit. We show effective regularization strategies, namely dropout techniques for MoE layers in EOM and FOM, Conditional MoE Routing and Curriculum Learning methods that prevent over-fitting and improve the performance of MoE models on low-resource tasks without adversely affecting high-resource tasks. On a massively multilingual machine translation benchmark, our strategies result in about +1 chrF++ improvement in very low resource language pairs. We perform an extensive analysis of the learned MoE routing to better understand the impact of our regularization methods and how we can improve them.","anthology_url":"https://aclanthology.org/2023.findings-acl.897","authors":["Maha Elbayad","Anna Y Sun","Shruti Bhosale"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-7_-machine-translation-(virtual-poster)"],"id":"P34","is_paper":true,"keywords":["modelling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.897.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77256/poster_document/f628f3f336e02cb5668316300b96a95d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual Machine Translation","tldr":"Sparsely gated Mixture of Experts (MoE) models have been shown to be a compute-efficient method to scale model capacity for multilingual machine translation. However, for low-resource tasks, MoE models severely over-fit. We show effective regularization strategies, namely dropout techniques for MoE ...","track":"Machine Translation","underline_id":77256,"underline_url":"https://underline.io/events/395/posters/15279/poster/77256-fixing-moe-over-fitting-on-low-resource-languages-in-multilingual-machine-translation","video_url":null},{"abstract":"In this paper, we propose a novel language model guided captioning approach, LAMOC,  for knowledge-based visual question answering (VQA). Our approach employs the generated captions by a captioning model as the context of an answer prediction model, which is a Pre-Trained Language model (PLM). \nAs the major contribution, we  leverage the guidance and feedback of the prediction model to improve the capability of the captioning model. In this way, the captioning model can become aware of the task goal and information need from the PLM. To develop our approach, we design two specific training stages, where the first stage adapts the captioning model to the prediction model (selecting more suitable caption propositions for training) and the second stage tunes the  captioning model according to the task goal (learning from feedback of the PLM). Extensive experiments demonstrate the effectiveness of the proposed approach on the knowledge-based VQA task. Specifically, on the challenging A-OKVQA dataset, LAMOC outperforms several competitive zero-shot methods and even achieves comparable results to a fine-tuned VLP model. Our code is publicly available at https://github.com/RUCAIBox/LAMOC.","anthology_url":"https://aclanthology.org/2023.findings-acl.590","authors":["Yifan Du","Junyi Li","Tianyi Tang","Wayne Xin Zhao","Ji-Rong Wen"],"category":"Findings","demo_url":null,"display_track":"Speech and Multimodality","event_ids":["session-7_-speech-and-multimodality-(virtual-poster)"],"id":"P3412","is_paper":true,"keywords":["multimodality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.590.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77792/poster_document/bc481eb825493c3dc8333c9a246e3468.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Zero-shot Visual Question Answering with Language Model Feedback","tldr":"In this paper, we propose a novel language model guided captioning approach, LAMOC,  for knowledge-based visual question answering (VQA). Our approach employs the generated captions by a captioning model as the context of an answer prediction model, which is a Pre-Trained Language model (PLM). \nAs t...","track":"Speech and Multimodality","underline_id":77792,"underline_url":"https://underline.io/events/395/posters/15279/poster/77792-zero-shot-visual-question-answering-with-language-model-feedback","video_url":null},{"abstract":"We introduce the task of correcting named entity recognition (NER) errors without re-training model.\nAfter an NER model is trained and deployed in production,\nit makes prediction errors, which usually need to be fixed quickly.\nTo address this problem, we firstly construct a gazetteer containing named entities and corresponding possible entity types.\nAnd then, we propose type enhanced BERT (TyBERT),\na method that integrates the named entity's type information into BERT by an adapter layer.\nWhen errors are identified, we can repair the model by updating the gazetteer.\nIn other words, the gazetteer becomes a trigger to control NER model's output.\nThe experiment results in multiple corpus show the effectiveness of our method, which outperforms strong baselines.x","anthology_url":"https://aclanthology.org/2023.findings-acl.445","authors":["Kuai Li","Chen Chen","Tao Yang","Tianming Du","Peijie Yu","dong du","Feng Zhang"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-4_-information-extraction-(virtual-poster)"],"id":"P3430","is_paper":true,"keywords":["named entity recognition and relation extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.445.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77795/poster_document/65405fe70936af035e42fa70d4edf0da.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Type Enhanced BERT for Correcting NER Errors","tldr":"We introduce the task of correcting named entity recognition (NER) errors without re-training model.\nAfter an NER model is trained and deployed in production,\nit makes prediction errors, which usually need to be fixed quickly.\nTo address this problem, we firstly construct a gazetteer containing name...","track":"Information Extraction","underline_id":77795,"underline_url":"https://underline.io/events/395/posters/15240/poster/77795-type-enhanced-bert-for-correcting-ner-errors","video_url":null},{"abstract":"Keyphrase generation is the task of summarizing the contents of any given article into a few salient phrases (or keyphrases). Existing works for the task mostly rely on large-scale annotated datasets, which are not easy to acquire. Very few works address the problem of keyphrase generation in low-resource settings, but they still rely on a lot of additional unlabeled data for pretraining and on automatic methods for pseudo-annotations. In this paper, we present data augmentation strategies specifically to address keyphrase generation in purely resource-constrained domains. We design techniques that use the full text of the articles to improve both present and absent keyphrase generation. We test our approach comprehensively on three datasets and show that the data augmentation strategies consistently improve the state-of-the-art performance. We release our source code at https://github.com/kgarg8/kpgen-lowres-data-aug.","anthology_url":"https://aclanthology.org/2023.findings-acl.534","authors":["Krishna K Garg","Jishnu Ray Chowdhury","Cornelia Caragea"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-7_-information-extraction-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3447","is_paper":true,"keywords":["document-level extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.534.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77799/slideshow/0838baa1de3f8acf10445a958cbce704.pdf","title":"Data Augmentation for Low-Resource Keyphrase Generation","tldr":"Keyphrase generation is the task of summarizing the contents of any given article into a few salient phrases (or keyphrases). Existing works for the task mostly rely on large-scale annotated datasets, which are not easy to acquire. Very few works address the problem of keyphrase generation in low-re...","track":"Information Extraction","underline_id":77799,"underline_url":"https://underline.io/events/395/posters/15279/poster/77799-data-augmentation-for-low-resource-keyphrase-generation","video_url":null},{"abstract":"CLIP (Contrastive Language\u2013Image Pretraining) is an English multimodal representation model learned from a massive amount of English text-image pairs and has achieved great success in various downstream tasks, including image classification, text-to-image retrieval, and image generation. When extending CLIP to other languages, the major problem is the lack of good-quality text-image pairs. In this work, we present AltCLIP, a simple and low-resource method to build a strong multilingual multimodal representation model. Instead of training a model from scratch on multilingual text-image pairs, we take the original CLIP model trained on English text-image pairs and alter its text encoder with a pre-trained multilingual text encoder (XLM-R). We then align text and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. Our method utilizes the existence of rich parallel text data and pre-trained multilingual language models. We present extensive experimental evaluations to demonstrate the effectiveness of our proposed method. Our model sets new state-of-the-art zero-shot performances on a wide range of tasks in multilingual multimodal benchmarks, including ImageNet-CN/IT/JA/KO serials, Flicker30k-CN, COCO-CN, Multi30k, and XTD. Further, our model outperforms the original CLIP model on zero-shot cross-modal retrieval, Image Classification in the Wild (ICinW) tasks, and CLIP Benchmark. We plan to open-source our code, pre-trained model weights, and evaluation toolkits of multilingual multimodal tasks, to facilitate research on multilingual multimodal representation learning.","anthology_url":"https://aclanthology.org/2023.findings-acl.552","authors":["Zhongzhi Chen","Guang Liu","Bo-Wen Zhang","Qinghong Yang","Ledell Y. Wu"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)"],"id":"P345","is_paper":true,"keywords":["cross-modal pretraining"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.552.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77299/poster_document/d843e6ac16dc838007d3cd5e4e6567f2.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities","tldr":"CLIP (Contrastive Language\u2013Image Pretraining) is an English multimodal representation model learned from a massive amount of English text-image pairs and has achieved great success in various downstream tasks, including image classification, text-to-image retrieval, and image generation. When extend...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77299,"underline_url":"https://underline.io/events/395/posters/15200/poster/77299-altclip-altering-the-language-encoder-in-clip-for-extended-language-capabilities","video_url":null},{"abstract":"Global co-occurrence information is the primary source of structural information on multilingual corpora, and we find that analogical/parallel compound words across languages have similar co-occurrence counts/frequencies (normalized) giving weak but stable self-supervision for cross-lingual transfer. Following the observation, we aim at associating contextualized representations with relevant (contextualized) representations across languages with the help of co-occurrence counts. The result is MLM-GC (MLM with Global Co-occurrence) pre-training that the model learns local bidirectional information from MLM and global co-occurrence information from a log-bilinear regression. Experiments show that MLM-GC pre-training substantially outperforms MLM pre-training for 4 downstream cross-lingual tasks and 1 additional monolingual task, showing the advantages of forming isomorphic spaces across languages.","anthology_url":"https://aclanthology.org/2023.findings-acl.475","authors":["Xi Ai","Bin Fang"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)"],"id":"P3457","is_paper":true,"keywords":["representation learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.475.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77802/poster_document/fd8a339585c05c5aa65214dd925e90b3.pdf","preview_image":"https://assets.underline.io/lecture/77802/poster/7ce100316c3a3a41e36aa158d7d7bb45.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77802/slideshow/51226ee26fb43926b6b498e597071aaf.pdf","title":"Multilingual Pre-training with Self-supervision from Global Co-occurrence Information","tldr":"Global co-occurrence information is the primary source of structural information on multilingual corpora, and we find that analogical/parallel compound words across languages have similar co-occurrence counts/frequencies (normalized) giving weak but stable self-supervision for cross-lingual transfer...","track":"Machine Learning for NLP","underline_id":77802,"underline_url":"https://underline.io/events/395/posters/15279/poster/77802-multilingual-pre-training-with-self-supervision-from-global-co-occurrence-information","video_url":null},{"abstract":"The current generation of intelligent assistants require explicit user requests to perform tasks or services, often leading to lengthy and complex conversations. In contrast, human assistants can infer multiple implicit intents from utterances via their commonsense knowledge, thereby simplifying interactions. To bridge this gap, this paper proposes a framework for multi-domain dialogue systems. This framework automatically infers implicit intents from user utterances, and prompts a large pre-trained language model to suggest suitable task-oriented bots. By leveraging commonsense knowledge, our framework recommends associated bots in a zero-shot manner, enhancing interaction efficiency and effectiveness. This approach substantially reduces interaction complexity, seamlessly integrates various domains and tasks, and represents a significant step towards creating more human-like intelligent assistants that can reason about implicit intents, offering a superior user experience.","anthology_url":"https://aclanthology.org/2023.findings-acl.17","authors":["Hui-Chi Kuo","Yun-Nung Chen"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3473","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.17.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77806/poster_document/d1acf46ddda341727c4bf86c535d1834.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77806/slideshow/58b5e1847b0f3c8b8ec3619530843690.pdf","title":"Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning","tldr":"The current generation of intelligent assistants require explicit user requests to perform tasks or services, often leading to lengthy and complex conversations. In contrast, human assistants can infer multiple implicit intents from utterances via their commonsense knowledge, thereby simplifying int...","track":"Dialogue and Interactive Systems","underline_id":77806,"underline_url":"https://underline.io/events/395/posters/15200/poster/77806-zero-shot-prompting-for-implicit-intent-prediction-and-recommendation-with-commonsense-reasoning","video_url":null},{"abstract":"The increasing sizes of large generative Pre-trained Language Models (PLMs) hinder their deployment\nin real-world applications. To obtain efficient PLMs, previous studies mostly focus on pruning the \nattention heads and  feed-forward networks~(FFNs) of the Transformer. Nevertheless, we find that in generative PLMs, the hidden dimension shared by many other modules (e.g., embedding layer and layer normalization) contains persistent outliers regardless of the network input. This study comprehensively investigates the structured pruning of generative PLMs with all the above compressible components.  To identify redundant network structures, we assign learnable masks over compressible components followed by sparse training. Various sizes of PLMs can be flexibly extracted via different thresholds, and are then task-specifically fine-tuned for further improvement. Extensive experiments on language modeling, summarization and machine translation validate the effectiveness of the proposed method. For example,  the pruned BART brings 1.51x/6.96x inference speedup on GPU/CPU with 67\\% size reduction, and can be further combined with quantization for more than 25$\\times$ compression.","anthology_url":"https://aclanthology.org/2023.findings-acl.692","authors":["Chaofan Tao","Lu Hou","Haoli Bai","Jiansheng Wei","Xin Jiang","Qun Liu","Ping Luo","Ngai Wong"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)"],"id":"P348","is_paper":true,"keywords":["model compression methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.692.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77301/poster_document/c11f707ac22613cdcbe8d319a21d5dd8.pdf","preview_image":"https://assets.underline.io/lecture/77301/poster/de526f412ad7877aad9d30e691dbc818.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77301/slideshow/502fdeea3722e9993e6b75addef1b3da.pdf","title":"Structured Pruning for Efficient Generative Pre-trained Language Models","tldr":"The increasing sizes of large generative Pre-trained Language Models (PLMs) hinder their deployment\nin real-world applications. To obtain efficient PLMs, previous studies mostly focus on pruning the \nattention heads and  feed-forward networks~(FFNs) of the Transformer. Nevertheless, we find that in ...","track":"Machine Learning for NLP","underline_id":77301,"underline_url":"https://underline.io/events/395/posters/15240/poster/77301-commonsense-knowledge-graph-completion-via-contrastive-pretraining-and-node-clustering","video_url":null},{"abstract":"There has been increasing interest in synthesizing data to improve downstream text-to-SQL tasks. In this paper, we examined the existing synthesized datasets and discovered that state-of-the-art text-to-SQL algorithms did not further improve on popular benchmarks when trained with augmented synthetic data. We observed three shortcomings: illogical synthetic SQL queries from independent column sampling, arbitrary table joins, and language gaps between the synthesized SQL and natural language question (NLQ) pair. To address these issues, we propose a novel synthesis framework that imposes strong typing constraints, incorporates key relationships from schema, and conducts schema-distance-weighted column sampling. We also adopt an intermediate representation (IR) for the SQL-to-text task to further improve the quality of the generated NLQ. When existing powerful text-to-SQL parsers are pretrained on our high-quality synthesized data, these models have significant accuracy boosts and achieve new state-of-the-art performance on Spider. We also demonstrate the effectiveness of our techniques with ablation studies","anthology_url":"https://aclanthology.org/2023.findings-acl.86","authors":["Yiqun Hu","Yiyun Zhao","Jiarong Jiang","Wuwei Lan","Henghui Zhu","Anuj Chauhan","Alexander Hanbo Li","Lin Pan","Jun Wang","Chung-Wei Hang","Sheng Zhang","Jiang Guo","Mingwen Dong","Joseph Lilien","Patrick Ng","Zhiguo Wang","Vittorio Castelli","Bing Xiang"],"category":"Findings","demo_url":null,"display_track":"Syntax: Tagging, Chunking, and Parsing","event_ids":["session-7_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)"],"id":"P3482","is_paper":true,"keywords":["semantic parsing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.86.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77807/poster_document/cda171e8e525e5ba7d4a5f996f8cc172.pdf","preview_image":"https://assets.underline.io/lecture/77807/poster/06c2eda0c302d11a17daf9cb95cfa813.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77807/slideshow/7639d162eaad0ed22bf5b37d989354c2.pdf","title":"Importance of Synthesizing High-quality Data for Text-to-SQL Parsing","tldr":"There has been increasing interest in synthesizing data to improve downstream text-to-SQL tasks. In this paper, we examined the existing synthesized datasets and discovered that state-of-the-art text-to-SQL algorithms did not further improve on popular benchmarks when trained with augmented syntheti...","track":"Syntax: Tagging, Chunking, and Parsing","underline_id":77807,"underline_url":"https://underline.io/events/395/posters/15279/poster/77807-end-to-end-task-oriented-dialogue-systems-based-on-schema","video_url":null},{"abstract":"Clinical practice frequently uses medical imaging for diagnosis and treatment. A significant challenge for automatic radiology report generation is that the radiology reports are long narratives consisting of multiple sentences for both abnormal and normal findings. Therefore, applying conventional image captioning approaches to generate the whole report proves to be insufficient, as these are designed to briefly describe images with short sentences. We propose a template-based approach to generate radiology reports from radiographs. Our approach involves the following: i) using a multilabel image classifier, produce the tags for the input radiograph; ii) using a transformer-based model, generate pathological descriptions (a description of abnormal findings seen on radiographs) from the tags generated in step (i); iii) using a BERT-based multi-label text classifier, find the spans in the normal report template to replace with the generated pathological descriptions; and iv) using a rule-based system, replace the identified span with the generated pathological description. We performed experiments with the two most popular radiology report datasets, IU Chest X-ray and MIMIC-CXR and demonstrated that the BLEU-1, ROUGE-L, METEOR, and CIDEr scores are better than the State-of-the-Art models by 25\\%, 36\\%, 44\\% and 48\\% respectively, on the IU X-RAY dataset. To the best of our knowledge, this is the first attempt to generate chest X-ray radiology reports by first creating small sentences for abnormal findings and then replacing them in the normal report template.","anthology_url":"https://aclanthology.org/2023.findings-acl.683","authors":["Kaveri Kale","Pushpak Bhattacharyya","Kshitij Jadhav"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-1_-nlp-applications-(virtual-poster)"],"id":"P3486","is_paper":true,"keywords":["healthcare applications, clincial nlp"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.683.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77810/poster_document/242c8b2d6a55f7f49fca17982aa33db2.pdf","preview_image":"https://assets.underline.io/lecture/77810/poster/68dbe7abb8b28a1ebad7d84de5622091.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77810/slideshow/c5b4c52bb5b303be6e62aa60a56fccc4.pptx","title":"Replace and Report: NLP Assisted Radiology Report Generation","tldr":"Clinical practice frequently uses medical imaging for diagnosis and treatment. A significant challenge for automatic radiology report generation is that the radiology reports are long narratives consisting of multiple sentences for both abnormal and normal findings. Therefore, applying conventional ...","track":"NLP Applications","underline_id":77810,"underline_url":"https://underline.io/events/395/posters/15200/poster/77810-replace-and-report-nlp-assisted-radiology-report-generation","video_url":null},{"abstract":"Data-driven analyses of biases in historical texts can help illuminate the origin and development of biases prevailing in modern society.\n However, digitised historical documents pose a challenge for NLP practitioners as these corpora suffer from errors introduced by optical character recognition (OCR) and are written in an archaic language.\nIn this paper, we investigate the continuities and transformations of bias in historical newspapers published in the Caribbean during the colonial era (18th to 19th centuries). Our analyses are performed along the axes of gender, race, and their intersection. We examine these biases by conducting a temporal study in which we measure the development of lexical associations using distributional semantics models and word embeddings. Further, we evaluate the effectiveness of techniques designed to process OCR-generated data and assess their stability when trained on and applied to the noisy historical newspapers.\nWe find that there is a trade-off between the stability of the word embeddings and their compatibility with the historical dataset. \nWe provide evidence that gender and racial biases are interdependent, and their intersection triggers distinct effects. These findings align with the theory of intersectionality, which stresses that biases affecting people with multiple marginalised identities compound to more than the sum of their constituents.","anthology_url":"https://aclanthology.org/2023.findings-acl.170","authors":["Nadav Borenstein","Karolina Stanczak","Thea Rolskov","Natacha Klein K\u00e4fer","Nat\u00e1lia da Silva Perez","Isabelle Augenstein"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3493","is_paper":true,"keywords":["language/cultural bias analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.170.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77811/poster_document/54d0260d0378b06e839c629cf16d7b90.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77811/slideshow/8af41bf694118cea23fb1d246f6aac7b.pdf","title":"Measuring Intersectional Biases in Historical Documents","tldr":"Data-driven analyses of biases in historical texts can help illuminate the origin and development of biases prevailing in modern society.\n However, digitised historical documents pose a challenge for NLP practitioners as these corpora suffer from errors introduced by optical character recognition (O...","track":"Computational Social Science and Cultural Analytics","underline_id":77811,"underline_url":"https://underline.io/events/395/posters/15279/poster/77811-measuring-intersectional-biases-in-historical-documents","video_url":null},{"abstract":"Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data. This is especially relevant for complex tasks such as the understanding of non-literal meaning. In this work, we probe the performance of three black-box TLMs and two intrinsically transparent white-box models on figurative language classification of sarcasm, similes, idioms, and metaphors. We conduct two studies on the classification results to provide insights into the inner workings of such models. With our first analysis on feature importance, we identify crucial differences in model behavior. With our second analysis using an online experiment with human participants, we inspect different linguistic characteristics of the four figurative language types.","anthology_url":"https://aclanthology.org/2023.findings-acl.622","authors":["Hyewon Jang","Qi Yu","Diego Frassinelli"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3512","is_paper":true,"keywords":["feature attribution"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.622.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77812/poster_document/c0b8fc207f4a15aa248ea80e4d1409b5.pdf","preview_image":"https://assets.underline.io/lecture/77812/poster/6dfa76cd8d4e6a05bfb4db6b4f1ca460.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77812/slideshow/fd82e38984ffe8c00fd4583fd347085a.pdf","title":"Figurative Language Processing: A Linguistically Informed Feature Analysis of the Behavior of Language Models and Humans","tldr":"Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data. This is especially relevant for complex tasks such as the understanding of non-literal meaning. In this work, we probe the performance of three black-b...","track":"Interpretability and Analysis of Models for NLP","underline_id":77812,"underline_url":"https://underline.io/events/395/posters/15240/poster/77812-figurative-language-processing-a-linguistically-informed-feature-analysis-of-the-behavior-of-language-models-and-humans","video_url":null},{"abstract":"Extracting sentiment elements using pre-trained generative models has recently led to large improvements in aspect-based sentiment analysis benchmarks. These models avoid explicit modeling of structure between sentiment elements, which are succinct yet lack desirable properties such as structure well-formedness guarantees or built-in elements alignments. In this study, we propose an opinion tree parsing model, aiming to parse all the sentiment elements from an opinion tree, which can explicitly reveal a more comprehensive and complete aspect-level sentiment structure. In particular, we first introduce a novel context-free opinion grammar to normalize the sentiment structure.  We then employ a neural chart-based opinion tree parser to fully explore the correlations among sentiment elements and parse them in the opinion tree form. Extensive experiments show the superiority of our proposed model and the capacity of the opinion tree parser with the proposed context-free opinion grammar. More importantly, our model is much faster than previous models.","anthology_url":"https://aclanthology.org/2023.findings-acl.505","authors":["Xiaoyi Bao","Xiaotong Jiang","Zhongqing Wang","Yue Zhang","Guodong Zhou"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)"],"id":"P353","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.505.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77302/poster_document/824cdf094c783037931d80035d93f901.pdf","preview_image":"https://assets.underline.io/lecture/77302/poster/7bc1b6e4132347243ff82fc7fdeaa30e.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77302/slideshow/5ddaa24a1e43e7ca85bb5752c93cd632.pptx","title":"Opinion Tree Parsing for Aspect-based Sentiment Analysis","tldr":"Extracting sentiment elements using pre-trained generative models has recently led to large improvements in aspect-based sentiment analysis benchmarks. These models avoid explicit modeling of structure between sentiment elements, which are succinct yet lack desirable properties such as structure wel...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77302,"underline_url":"https://underline.io/events/395/posters/15240/poster/77302-opinion-tree-parsing-for-aspect-based-sentiment-analysis","video_url":null},{"abstract":"The chain-of-though (CoT) prompting methods were successful in various natural language processing (NLP) tasks thanks to their ability to unveil the underlying complex reasoning processes.\nSuch reasoning processes typically exhibit highly structured steps.\nRecent efforts also started investigating methods to encourage more structured reasoning procedures to be captured (cite least to most).\nIn this work, we propose Tab-CoT, a novel tabular-format CoT prompting method, which allows the complex reasoning process to be explicitly modeled in a highly structured manner.\nDespite its simplicity, we show that our approach is capable of performing reasoning across multiple dimensions (i.e., both rows and columns).\nWe demonstrate our approach's strong zero-shot and few-shot capabilities through extensive experiments on a range of reasoning tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.651","authors":["Jin Ziqi","Wei Lu"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)"],"id":"P3540","is_paper":true,"keywords":["logical reasoning","reasoning","math qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.651.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77818/poster_document/87c90bc0b4aed5f3bb8047a146f0f02f.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Tab-CoT: Zero-shot Tabular Chain of Thought","tldr":"The chain-of-though (CoT) prompting methods were successful in various natural language processing (NLP) tasks thanks to their ability to unveil the underlying complex reasoning processes.\nSuch reasoning processes typically exhibit highly structured steps.\nRecent efforts also started investigating m...","track":"Question Answering","underline_id":77818,"underline_url":"https://underline.io/events/395/posters/15240/poster/77818-tab-cot-zero-shot-tabular-chain-of-thought","video_url":null},{"abstract":"Despite recent interest in open domain question answering (ODQA) over tables, many studies still rely on datasets that are not truly optimal for the task with respect to utilizing structural nature of table. These datasets assume answers reside as a single cell value and do not necessitate exploring over multiple cells such as aggregation, comparison, and sorting. Thus, we release Open-WikiTable, the first ODQA dataset that requires complex reasoning over tables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be applicable in the open-domain setting. As each question is coupled with both textual answers and SQL queries, Open-WikiTable opens up a wide range of possibilities for future research, as both reader and parser methods can be applied. The dataset is publicly available.","anthology_url":"https://aclanthology.org/2023.findings-acl.526","authors":["Sunjun Kweon","Yeonsu Kwon","Seonhee Cho","Yohan Jo","Edward Choi"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-1_-resources-and-evaluation-(virtual-poster)"],"id":"P3546","is_paper":true,"keywords":["nlp datasets"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.526.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77819/poster_document/32b221b9f130d7eb8cf954af8250e453.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Open-WikiTable : Dataset for Open Domain Question Answering with Complex Reasoning over Table","tldr":"Despite recent interest in open domain question answering (ODQA) over tables, many studies still rely on datasets that are not truly optimal for the task with respect to utilizing structural nature of table. These datasets assume answers reside as a single cell value and do not necessitate exploring...","track":"Resources and Evaluation","underline_id":77819,"underline_url":"https://underline.io/events/395/posters/15200/poster/77819-open-wikitable-dataset-for-open-domain-question-answering-with-complex-reasoning-over-table","video_url":null},{"abstract":"Text Style Transfer (TST) evaluation is, in practice, inconsistent.\nTherefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field.\nThe meta-analysis reveals a substantial standardization gap in human and automated evaluation.\nIn addition, we also find a validation gap: only few automated metrics have been validated using human experiments.\nTo this end, we thoroughly scrutinize both the standardization and validation gap and reveal the resulting pitfalls.\nThis work also paves the way to close the standardization and validation gap in TST evaluation by calling out requirements to be met by future research.","anthology_url":"https://aclanthology.org/2023.findings-acl.687","authors":["Phil Sidney Ostheimer","Mayank Kumar Nagda","Marius Kloft","Sophie Fellenz"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-7_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3557","is_paper":true,"keywords":["evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.687.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77822/poster_document/2e5340ebf47efc2fb725933378ef2633.pdf","preview_image":"https://assets.underline.io/lecture/77822/poster/9ac54e7b9a9054348783bad7ff5e22fd.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77822/slideshow/0d783fbe7206ecd88a83d4c08d9ee8de.pptx","title":"A Call for Standardization and Validation of Text Style Transfer Evaluation","tldr":"Text Style Transfer (TST) evaluation is, in practice, inconsistent.\nTherefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field.\nThe meta-analysis reveals a substantial standardization gap in human and auto...","track":"Theme: Reality Check","underline_id":77822,"underline_url":"https://underline.io/events/395/posters/15279/poster/77822-banglabook-a-large-scale-bangla-dataset-for-sentiment-analysis-from-book-reviews","video_url":null},{"abstract":"Most text-to-SQL models, even though based on the same grammar decoder, generate the SQL structure first and then fill in the SQL slots with the correct schema items. This second step depends on schema linking:  aligning the entity references in the question with the schema columns or tables. This is generally approached via Exact Match based Schema Linking (EMSL) within a neural network-based schema linking module. EMSL has become standard in text-to-SQL: many state-of-the-art models employ EMSL, with performance dropping significantly when the EMSL component is removed. In this work, however, we show that EMSL reduces robustness, rendering models vulnerable to synonym substitution and typos. Instead of relying on EMSL to make up for deficiencies in question-schema encoding, we show that using a pre-trained language model as an encoder can improve performance without using EMSL, giving a more robust model. We also study the design choice of the schema linking module, finding that a suitable design benefits performance and interoperability. Finally, based on the above study of schema linking, we introduce the grammar linking to help model align grammar references in the question with the SQL keywords.","anthology_url":"https://aclanthology.org/2023.findings-acl.53","authors":["Yujian Gan","Xinyun Chen","Matthew Purver"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)"],"id":"P3593","is_paper":true,"keywords":["table qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.53.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77827/poster_document/8efdba81a97d3c2e80ec48a79823ffff.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77827/slideshow/40977182e91d2507fa37237a62f813df.pdf","title":"Re-appraising the Schema Linking for Text-to-SQL","tldr":"Most text-to-SQL models, even though based on the same grammar decoder, generate the SQL structure first and then fill in the SQL slots with the correct schema items. This second step depends on schema linking:  aligning the entity references in the question with the schema columns or tables. This i...","track":"Question Answering","underline_id":77827,"underline_url":"https://underline.io/events/395/posters/15240/poster/77827-re-appraising-the-schema-linking-for-text-to-sql","video_url":null},{"abstract":"Existing efforts on text synthesis for code-switching mostly require training on code-switched texts in the target language pairs, limiting the deployment of the models to cases lacking code-switched data.\nIn this work, we study the problem of synthesizing code-switched texts for language pairs absent from the training data. We introduce GLOSS, a model built on top of a pre-trained multilingual machine translation model (PMMTM) with an additional code-switching module. This module, either an adapter or extra prefixes, learns code-switching patterns from code-switched data during training, while the primary component of GLOSS, i.e., the PMMTM, is frozen. The design of only adjusting the code-switching module prevents our model from overfitting to the constrained training data for code-switching. Hence, GLOSS exhibits the ability to generalize and synthesize code-switched texts across a broader spectrum of language pairs. Additionally, we develop a self-training algorithm on target language pairs further to enhance the reliability of GLOSS. Automatic evaluations on four language pairs show that GLOSS achieves at least 55\\% relative BLEU and METEOR scores improvements compared to strong baselines. Human evaluations on two language pairs further validate the success of GLOSS.","anthology_url":"https://aclanthology.org/2023.findings-acl.318","authors":["I-Hung Hsu","Avik Ray","Shubham Garg","Nanyun Peng","Jing Huang"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3596","is_paper":true,"keywords":["code-switching"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.318.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77828/poster_document/1a9def9ff195569c662d9dab46ca2e7b.pdf","preview_image":"https://assets.underline.io/lecture/77828/poster/87817e19aa9420a828df8ec475147519.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77828/slideshow/0689ce4eb476ba8667a0fb1f438fbf5e.pdf","title":"Code-Switched Text Synthesis in Unseen Language Pairs","tldr":"Existing efforts on text synthesis for code-switching mostly require training on code-switched texts in the target language pairs, limiting the deployment of the models to cases lacking code-switched data.\nIn this work, we study the problem of synthesizing code-switched texts for language pairs abse...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77828,"underline_url":"https://underline.io/events/395/posters/15279/poster/77828-code-switched-text-synthesis-in-unseen-language-pairs","video_url":null},{"abstract":"With a surge in identifying suicidal risk and its severity in social media posts, we argue that a more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare. The success of computational intelligence techniques for inferring mental illness from social media resources, points to natural language processing as a lens for determining Interpersonal Risk Factors (IRF) in human writings. Motivated with limited availability of datasets for social NLP research community, we construct and release a new annotated dataset with human-labelled explanations and classification of IRF affecting mental disturbance on social media: (i) Thwarted Belongingness (TBe), and (ii) Perceived Burdensomeness (PBu). We establish baseline models on our dataset facilitating future research directions to develop real-time personalized AI models by detecting patterns of TBe and PBu in emotional spectrum of user's historical social media profile.","anthology_url":"https://aclanthology.org/2023.findings-acl.757","authors":["Muskan Garg","Amirmohammad Shahbandegan","Amrit Chadha","Vijay Mago"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-4_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3615","is_paper":true,"keywords":["nlp datasets"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.757.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts","tldr":"With a surge in identifying suicidal risk and its severity in social media posts, we argue that a more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare. The success of computational intelligence techniques for in...","track":"Resources and Evaluation","underline_id":77830,"underline_url":"https://underline.io/events/395/posters/15240/poster/77830-an-annotated-dataset-for-explainable-interpersonal-risk-factors-of-mental-disturbance-in-social-media-posts","video_url":null},{"abstract":"Models trained with empirical risk minimization (ERM) are revealed to easily rely on spurious correlations, resulting in poor generalization. Group distributionally robust optimization (group DRO) can alleviate this problem by minimizing the worst-case loss over pre-defined groups. While promising, in practice factors like expensive annotations and privacy preclude the availability of group labels. More crucially, when taking a closer look at the failure modes of out-of-distribution generalization, the typical procedure of reweighting in group DRO loses efficiency. Hinged on the limitations, in this work, we reformulate the group DRO framework by proposing Q-Diversity. Characterized by an interactive training mode, Q-Diversity relaxes the group identification from annotation into direct parameterization. Furthermore, a novel mixing strategy across groups is presented to diversify the under-represented groups. In a series of experiments on both synthetic and real-world text classification tasks, results demonstrate that Q-Diversity can consistently improve worst-case accuracy under different distributional shifts, outperforming state-of-the-art alternatives.","anthology_url":"https://aclanthology.org/2023.findings-acl.8","authors":["Ting Wu","Rui Zheng","Tao Gui","Qi Zhang","Xuanjing Huang"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)"],"id":"P3629","is_paper":true,"keywords":["model bias/unfairness mitigation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.8.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77833/poster_document/df8c61945eb6425cb1c3339ee8dc8cad.pdf","preview_image":"https://assets.underline.io/lecture/77833/poster/2f6afaf39adec013deaf86b078fa8237.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Modeling the Q-Diversity in a Min-max Play Game for Robust Optimization","tldr":"Models trained with empirical risk minimization (ERM) are revealed to easily rely on spurious correlations, resulting in poor generalization. Group distributionally robust optimization (group DRO) can alleviate this problem by minimizing the worst-case loss over pre-defined groups. While promising, ...","track":"Ethics and NLP","underline_id":77833,"underline_url":"https://underline.io/events/395/posters/15279/poster/77833-modeling-the-q-diversity-in-a-min-max-play-game-for-robust-optimization","video_url":null},{"abstract":"In recent years, research in text summarization has mainly focused on the news domain, where texts are typically short and have strong layout features. The task of full-book summarization presents additional challenges which are hard to tackle with current resources, due to their limited size and availability in English only. To overcome these limitations, we present \"Echoes from Alexandria\u201d, or in shortened form, \"Echoes\", a large resource for multilingual book summarization. Echoes features\nthree novel datasets: i) Echo-Wiki, for multilingual book summarization, ii) Echo-XSum, for extremely-compressive multilingual book summarization, and iii) Echo-FairySum, for extractive book summarization. To the best of our knowledge, Echoes \u2013 with its thousands of books and summaries \u2013 is the largest resource, and the first to be multilingual, featuring 5 languages and 25 language pairs. In addition to Echoes, we also introduce a new extractive-then-abstractive baseline, and, supported by our experimental results and manual analysis of the summaries generated, we argue that this baseline is more suitable for book summarization than purely-abstractive approaches. We release our resource and software at https://github.com/Babelscape/echoes-from-alexandria in the hope of fostering innovative research in multilingual book\nsummarization.","anthology_url":"https://aclanthology.org/2023.findings-acl.54","authors":["Alessandro Scir\u00e8","Simone Conia","Simone Ciciliano","Roberto Navigli"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-1_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3630","is_paper":true,"keywords":["multilingual corpora","nlp datasets"],"languages":["french","german","italian","spanish"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.54.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77834/poster_document/377bd3db8a994e54c98fa49c1c07f78a.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77834/slideshow/e0e1c5fca1ed72ad9a40b222119963de.pdf","title":"Echoes from Alexandria: A Large Resource for Multilingual Book Summarization","tldr":"In recent years, research in text summarization has mainly focused on the news domain, where texts are typically short and have strong layout features. The task of full-book summarization presents additional challenges which are hard to tackle with current resources, due to their limited size and av...","track":"Resources and Evaluation","underline_id":77834,"underline_url":"https://underline.io/events/395/posters/15200/poster/77834-echoes-from-alexandria-a-large-resource-for-multilingual-book-summarization","video_url":null},{"abstract":"Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at [http://www.github.com/sapienzanlp/LeakDistill](http://www.github.com/sapienzanlp/LeakDistill).","anthology_url":"https://aclanthology.org/2023.findings-acl.125","authors":["Pavlo Vasylenko","\u202aPere-Llu\u00eds Huguet Cabot","Abelardo Carlos Mart\u00ednez Lorenzo","Roberto Navigli"],"category":"Findings","demo_url":null,"display_track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","event_ids":["session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3633","is_paper":true,"keywords":["phrase/sentence embedding"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.125.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77835/poster_document/266ea717f8efe385239dbcf3effed272.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77835/slideshow/47dedfd3e000f358299361dbeb75e979.pdf","title":"Incorporating Graph Information in Transformer-based AMR Parsing","tldr":"Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of...","track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","underline_id":77835,"underline_url":"https://underline.io/events/395/posters/15200/poster/77835-incorporating-graph-information-in-transformer-based-amr-parsing","video_url":null},{"abstract":"Concurrent with the rapid progress in neural network-based models in NLP, the need for creating explanations for the predictions of these black-box models has risen steadily. Yet, especially for complex inputs like texts or images, existing interpretability methods still struggle with deriving easily interpretable explanations that also accurately represent the basis for the model's decision. To this end, we propose a new, model-agnostic method to generate extractive explanations for predictions made by neural networks, that is based on masking parts of the input which the model does not consider to be indicative of the respective class. The masking is done using gradient-based optimization combined with a new regularization scheme that enforces sufficiency, comprehensiveness, and compactness of the generated explanation.\nOur method achieves state-of-the-art results in a challenging paragraph-level rationale extraction task, showing that this task can be performed without training a specialized model.\nWe further apply our method to image inputs and obtain high-quality explanations for image classifications, which indicates that the objectives for optimizing explanation masks in text generalize to inputs of other modalities.","anthology_url":"https://aclanthology.org/2023.findings-acl.867","authors":["Marc Felix Brinner","Sina Zarrie\u00df"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3636","is_paper":true,"keywords":["explanation faithfulness","feature attribution"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.867.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77836/poster_document/3fc2f51f5d0036d7ef2febe792aa329c.pdf","preview_image":"https://assets.underline.io/lecture/77836/poster/5be2f7684e273361b58794da115c84f7.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77836/slideshow/3d83c3d4b0dc0bec75060fa541ef3615.pdf","title":"Model Interpretability and Rationale Extraction by Input Mask Optimization","tldr":"Concurrent with the rapid progress in neural network-based models in NLP, the need for creating explanations for the predictions of these black-box models has risen steadily. Yet, especially for complex inputs like texts or images, existing interpretability methods still struggle with deriving easil...","track":"Interpretability and Analysis of Models for NLP","underline_id":77836,"underline_url":"https://underline.io/events/395/posters/15200/poster/77836-model-interpretability-and-rationale-extraction-by-input-mask-optimization","video_url":null},{"abstract":"Medical dialogue systems (MDS) aim to provide patients with medical services, such as diagnosis and prescription. Since most patients cannot precisely describe their symptoms, dialogue understanding is challenging for MDS. Previous studies mainly addressed this by extracting the mentioned medical entities as critical dialogue history information. In this work, we argue that it is also essential to capture the transitions of the medical entities and the doctor's dialogue acts in each turn, as they help the understanding of how the dialogue flows and enhance the prediction of the entities and dialogue acts to be adopted in the following turn. Correspondingly, we propose a Dual Flow enhanced Medical (DFMed) dialogue generation framework. It extracts the medical entities and dialogue acts used in the dialogue history and models their transitions with an entity-centric graph flow and a sequential act flow, respectively. We employ two sequential models to encode them and devise an interweaving component to enhance their interactions. Experiments on two datasets demonstrate that our method exceeds baselines in both automatic and manual evaluations.","anthology_url":"https://aclanthology.org/2023.findings-acl.423","authors":["Kaishuai Xu","Wenjun Hou","Yi Cheng","Jian Wang","Wenjie Li"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P3649","is_paper":true,"keywords":["knowledge augmented","applications","grounded dialog"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.423.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77838/poster_document/203e3ac9e768f851fe705fa4b37b2f51.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Medical Dialogue Generation via Dual Flow Modeling","tldr":"Medical dialogue systems (MDS) aim to provide patients with medical services, such as diagnosis and prescription. Since most patients cannot precisely describe their symptoms, dialogue understanding is challenging for MDS. Previous studies mainly addressed this by extracting the mentioned medical en...","track":"Dialogue and Interactive Systems","underline_id":77838,"underline_url":"https://underline.io/events/395/posters/15200/poster/77838-medical-dialogue-generation-via-dual-flow-modeling","video_url":null},{"abstract":"Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), partly owing to the massive amounts of world knowledge they memorize during pretraining.\nWhile many downstream applications provide the model with an informational context to aid its underlying task, how the model's world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model's memorized knowledge. This enables model predictions to be grounded in the context, which then facilitates updating specific model predictions without frequently retraining the model. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM models (both pretrained and finetuned) could exhibit low controllability and robustness that does not improve with increasing the model size. As a solution, we propose a simple yet effective method \u2013 knowledge aware finetuning (KAFT) \u2013 to strengthen both controllability and robustness by injecting counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes.","anthology_url":"https://aclanthology.org/2023.findings-acl.112","authors":["Daliang Li","Ankit Singh Rawat","Manzil Zaheer","Xin Wang","Michal Lukasik","Andreas Veit","Felix Yu","Sanjiv Kumar"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-4_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3655","is_paper":true,"keywords":["robustness"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.112.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77840/poster_document/9a8bf075504905a95b8686df6d600740.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Large Language Models with Controllable Working Memory","tldr":"Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), partly owing to the massive amounts of world knowledge they memorize during pretraining.\nWhile many downstream applications provide the model with an informational context to aid its underlying t...","track":"Large Language Models","underline_id":77840,"underline_url":"https://underline.io/events/395/posters/15240/poster/77840-large-language-models-with-controllable-working-memory","video_url":null},{"abstract":"Multilingual image captioning has recently been tackled by training with large-scale machine translated data, which is an expensive, noisy, and time-consuming process. Without requiring any multilingual caption data, we propose LMCap, an image-blind few-shot multilingual captioning model that works by prompting a language model with retrieved captions. Specifically, instead of following the standard encoder-decoder paradigm, given an image, LMCap first retrieves the captions of similar images using a multilingual CLIP encoder. These captions are then combined into a prompt for an XGLM decoder, in order to generate captions in the desired language. In other words, the generation model does not directly process the image, instead it processes retrieved captions. Experiments on the XM3600 dataset of geographically diverse images show that our model is competitive with fully-supervised multilingual captioning models, without requiring any supervised training on any captioning data.","anthology_url":"https://aclanthology.org/2023.findings-acl.104","authors":["Rita Parada Ramos","Bruno Martins","Desmond Elliott"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3659","is_paper":true,"keywords":["cross-modal content generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.104.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77841/poster/764f07ebef1d7f2de6b30209a7966fb5.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77841/slideshow/8ddb6fa5cc560ec707087bcc857c3e0e.pdf","title":"LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented Language Model Prompting","tldr":"Multilingual image captioning has recently been tackled by training with large-scale machine translated data, which is an expensive, noisy, and time-consuming process. Without requiring any multilingual caption data, we propose LMCap, an image-blind few-shot multilingual captioning model that works ...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77841,"underline_url":"https://underline.io/events/395/posters/15240/poster/77841-lmcap-few-shot-multilingual-image-captioning-by-retrieval-augmented-language-model-prompting","video_url":null},{"abstract":"Pre-trained abstractive summarization models can generate fluent summaries and achieve high ROUGE scores. Previous research has found that these models often generate summaries that are inconsistent with their context document and contain nonfactual information. To evaluate factuality in document summarization, a document-level Natural Language Inference (NLI) classifier can be used. However, training such a classifier requires large-scale high-quality factual and nonfactual samples. To that end, we introduce NonFactS, a data generation model, to synthesize nonfactual summaries given a context document and a human-annotated (reference) factual summary. Compared to previous methods, our nonfactual samples are more abstractive and more similar to their corresponding factual samples, resulting in state-of-the-art performance on two factuality evaluation benchmarks, FALSESUM and SUMMAC. Our experiments demonstrate that even without human-annotated summaries, NonFactS can use random sentences to generate nonfactual summaries and a classifier trained on these samples generalizes to out-of-domain documents.","anthology_url":"https://aclanthology.org/2023.findings-acl.400","authors":["Amir Soleimani","Christof Monz","marcel worring"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-1_-summarization-(virtual-poster)"],"id":"P3664","is_paper":true,"keywords":["factuality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.400.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77842/poster_document/30219ff3d685b8f7df9c0c0f91c153e0.pdf","preview_image":"https://assets.underline.io/lecture/77842/poster/7ac2f4b33a9cf44ebcbed71e4a5ed3b3.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77842/slideshow/879c0532afb13c979f80e0d260940f5c.pdf","title":"NonFactS: NonFactual Summary Generation for Factuality Evaluation in Document Summarization","tldr":"Pre-trained abstractive summarization models can generate fluent summaries and achieve high ROUGE scores. Previous research has found that these models often generate summaries that are inconsistent with their context document and contain nonfactual information. To evaluate factuality in document su...","track":"Summarization","underline_id":77842,"underline_url":"https://underline.io/events/395/posters/15200/poster/77842-nonfacts-nonfactual-summary-generation-for-factuality-evaluation-in-document-summarization","video_url":null},{"abstract":"Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research.\nWe investigate outlier dimensions and their relationship to anisotropy in multiple pre-trained multilingual language models. We focus on cross-lingual semantic similarity tasks, as these are natural tasks for evaluating multilingual representations. Specifically, we examine sentence representations. \nSentence transformers which are fine-tuned on parallel resources (that are not always available) perform better on this task, and we show that their representations are more isotropic. However, we aim to improve multilingual representations in general. We investigate how much of the performance difference can be made up by only transforming the embedding space without fine-tuning, and visualise the resulting spaces. We test different operations: Removing individual outlier dimensions, cluster-based isotropy enhancement, and ZCA whitening. We publish our code for reproducibility.","anthology_url":"https://aclanthology.org/2023.findings-acl.439","authors":["Katharina Haemmerl","Alina Fastowski","Jind\u0159ich Libovick\u00fd","Alexander Fraser"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3667","is_paper":true,"keywords":["mutlilingual representations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.439.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77843/poster_document/77e787bf210fd46f7c3b29f9c582eecb.pdf","preview_image":"https://assets.underline.io/lecture/77843/poster/9d68318de90c0931e7549de33c070549.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77843/slideshow/6e15c94b8700c1eb8c89cd5a7255cd4e.pdf","title":"Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity","tldr":"Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multiling...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77843,"underline_url":"https://underline.io/events/395/posters/15279/poster/77843-exploring-anisotropy-and-outliers-in-multilingual-language-models-for-cross-lingual-semantic-sentence-similarity","video_url":null},{"abstract":"Intermediate training of pre-trained transformer-based language models on domain-specific data leads to substantial gains for downstream tasks. To increase efficiency and prevent catastrophic forgetting alleviated from full domain-adaptive pre-training, approaches such as adapters have been developed. However, these require additional parameters for each layer, and are criticized for their limited expressiveness. In this work, we introduce TADA, a novel task-agnostic domain adaptation method which is modular, parameter-efficient, and thus, data-efficient. Within TADA, we retrain the embeddings to learn domain-aware input representations and tokenizers for the transformer encoder, while freezing all other parameters of the model. Then, task-specific fine-tuning is performed. We further conduct experiments with meta-embeddings and newly introduced meta-tokenizers, resulting in one model per task in multi-domain use cases. Our broad evaluation in 4 downstream tasks for 14 domains across single- and multi-domain setups and high- and low-resource scenarios reveals that TADA is an effective and efficient alternative to full domain-adaptive pre-training and adapters for domain adaptation, while not introducing additional parameters or complex training steps.","anthology_url":"https://aclanthology.org/2023.findings-acl.31","authors":["Chia-Chien Hung","Lukas Lange","Jannik Str\u00f6tgen"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)"],"id":"P3686","is_paper":true,"keywords":["transfer learning / domain adaptation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.31.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77846/poster_document/1c2cd58e3bc5a134f73f9875b7713bb3.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"TADA: Efficient Task-Agnostic Domain Adaptation for Transformers","tldr":"Intermediate training of pre-trained transformer-based language models on domain-specific data leads to substantial gains for downstream tasks. To increase efficiency and prevent catastrophic forgetting alleviated from full domain-adaptive pre-training, approaches such as adapters have been develope...","track":"Machine Learning for NLP","underline_id":77846,"underline_url":"https://underline.io/events/395/posters/15240/poster/77846-tada-efficient-task-agnostic-domain-adaptation-for-transformers","video_url":null},{"abstract":"This paper introduces a novel aligner for Abstract Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of different languages. Our approach leverages modern Transformer-based parsers, which inherently encode alignment information in their cross-attention weights, allowing us to extract this information during parsing. This eliminates the need for English-specific rules or the Expectation Maximization (EM) algorithm that have been used in previous approaches. In addition, we propose a guided supervised method using alignment to further enhance the performance of our aligner. We achieve state-of-the-art results in the benchmarks for AMR alignment and demonstrate our aligner's ability to obtain them across multiple languages. Our code will be available at [https://www.github.com/babelscape/AMR-alignment](https://www.github.com/babelscape/AMR-alignment).","anthology_url":"https://aclanthology.org/2023.findings-acl.109","authors":["Abelardo Carlos Mart\u00ednez Lorenzo","\u202aPere-Llu\u00eds Huguet Cabot","Roberto Navigli"],"category":"Findings","demo_url":null,"display_track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","event_ids":["session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)"],"id":"P3705","is_paper":true,"keywords":["phrase/sentence embedding","word/phrase alignment"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.109.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77847/poster_document/a6c183d6ffc6632279e4cb62757f07e3.pdf","preview_image":"https://assets.underline.io/lecture/77847/poster/aae7b5213bf0a7ea80033b38242a6efc.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77847/slideshow/bea1864539947caa67caa85b2f8fed4d.pdf","title":"Cross-lingual AMR Aligner: Paying Attention to Cross-Attention","tldr":"This paper introduces a novel aligner for Abstract Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of different languages. Our approach leverages modern Transformer-based parsers, which inherently encode alignment infor...","track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","underline_id":77847,"underline_url":"https://underline.io/events/395/posters/15200/poster/77847-using-contradictions-improves-question-answering-systems","video_url":null},{"abstract":"Automatically scoring student answers is an important task that is usually solved using instance-based supervised learning. Recently, similarity-based scoring has been proposed as an alternative approach yielding similar perfor- mance. It has hypothetical advantages such as a lower need for annotated training data and better zero-shot performance, both of which are properties that would be highly beneficial when applying content scoring in a realistic classroom setting.\n\nIn this paper we take a closer look at these alleged advantages by comparing different instance-based and similarity-based methods on multiple data sets in a number of learning curve experiments. We find that both the demand on data and cross-prompt performance is similar, thus not confirming the former two suggested advantages. The by default more straightforward possibility to give feedback based on a similarity-based approach may thus tip the scales in favor of it, although future work is needed to explore this advantage in practice.","anthology_url":"https://aclanthology.org/2023.findings-acl.119","authors":["Marie Bexte","Andrea Horbach","Torsten Zesch"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-1_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3765","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.119.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77856/poster_document/5c040b42475d79ab5358287f9c198385.pdf","preview_image":"https://assets.underline.io/lecture/77856/poster/e6c83c647f78aa6c51224ecb7618e8f1.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77856/slideshow/19442bc33cfc31ba1fe3657a36c569f0.pdf","title":"Similarity-Based Content Scoring - A more Classroom-Suitable Alternative to Instance-Based Scoring?","tldr":"Automatically scoring student answers is an important task that is usually solved using instance-based supervised learning. Recently, similarity-based scoring has been proposed as an alternative approach yielding similar perfor- mance. It has hypothetical advantages such as a lower need for annotate...","track":"NLP Applications","underline_id":77856,"underline_url":"https://underline.io/events/395/posters/15200/poster/77856-similarity-based-content-scoring-a-more-classroom-suitable-alternative-to-instance-based-scoringquestion","video_url":null},{"abstract":"Relation Extraction (RE) remains a challenging task, especially when considering realistic out-of-domain evaluations. One of the main reasons for this is the limited training size of current RE datasets: obtaining high-quality (manually annotated) data is extremely expensive and cannot realistically be repeated for each new domain. An intermediate training step on data from related tasks has shown to be beneficial across many NLP tasks.However, this setup still requires supplementary annotated data, which is often not available. In this paper, we investigate intermediate pre-training specifically for RE. We exploit the affinity between syntactic structure and semantic RE, and identify the syntactic relations which are closely related to RE by being on the shortest dependency path between two entities. We then take advantage of the high accuracy of current syntactic parsers in order to automatically obtain large amounts of low-cost pre-training data. By pre-training our RE model on the relevant syntactic relations, we are able to outperform the baseline in five out of six cross-domain setups, without any additional annotated data.","anthology_url":"https://aclanthology.org/2023.findings-acl.436","authors":["Elisa Bassignana","Filip Ginter","Sampo Pyysalo","Rob van der Goot","Barbara Plank"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-4_-information-extraction-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3780","is_paper":true,"keywords":["named entity recognition and relation extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.436.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77858/poster_document/6604818366d5473eb382bc6d1c3773cb.pdf","preview_image":"https://assets.underline.io/lecture/77858/poster/c009c60a1a6940f0942c07179011e115.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77858/slideshow/ce6244cb0d47169c3f855559a98a25ff.pdf","title":"Silver Syntax Pre-training for Cross-Domain Relation Extraction","tldr":"Relation Extraction (RE) remains a challenging task, especially when considering realistic out-of-domain evaluations. One of the main reasons for this is the limited training size of current RE datasets: obtaining high-quality (manually annotated) data is extremely expensive and cannot realistically...","track":"Information Extraction","underline_id":77858,"underline_url":"https://underline.io/events/395/posters/15240/poster/77858-silver-syntax-pre-training-for-cross-domain-relation-extraction","video_url":null},{"abstract":"Compositionality is a hallmark of human language that not only enables linguistic generalization, but also potentially facilitates acquisition. When simulating language emergence with neural networks, compositionality has been shown to improve communication performance; however, its impact on imitation learning has yet to be investigated. Our work explores the link between compositionality and imitation in a Lewis game played by deep neural agents. Our contributions are twofold: first, we show that the learning algorithm used to imitate is crucial: supervised learning tends to produce more average languages, while reinforcement learning introduces a selection pressure toward more compositional languages. \nSecond, our study reveals that compositional languages are easier to imitate, which may induce the pressure toward compositional languages in RL imitation settings.","anthology_url":"https://aclanthology.org/2023.findings-acl.787","authors":["Emily Cheng","Mathieu Rita","Thierry Poibeau"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-4_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P3788","is_paper":true,"keywords":["embodied agents"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.787.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77859/poster_document/5a0c5aa6bd7ef3c8465b06f49d7d4f14.pdf","preview_image":"https://assets.underline.io/lecture/77859/poster/721671b96d7a87a9f6710ca0c4a06afd.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77859/slideshow/426620d270ddf244964901c5853a2907.pdf","title":"On the Correspondence between Compositionality and Imitation in Emergent Neural Communication","tldr":"Compositionality is a hallmark of human language that not only enables linguistic generalization, but also potentially facilitates acquisition. When simulating language emergence with neural networks, compositionality has been shown to improve communication performance; however, its impact on imitat...","track":"Dialogue and Interactive Systems","underline_id":77859,"underline_url":"https://underline.io/events/395/posters/15240/poster/77859-on-the-correspondence-between-compositionality-and-imitation-in-emergent-neural-communication","video_url":null},{"abstract":"Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. \nHowever, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. \nIn this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. \n\nIn this work, we propose an alternative reasoning scheme, Socratic CoT that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. \nWe use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver.\nIn practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems.\nOn multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70\\% compared to the baselines. \nFinally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available: https://github.com/kumar-shridhar/Distiiling-LM.","anthology_url":"https://aclanthology.org/2023.findings-acl.441","authors":["Kumar Shridhar","Alessandro Stolfo","Mrinmaya Sachan"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-7_-generation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3794","is_paper":true,"keywords":["efficient models"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.441.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Distilling Reasoning Capabilities into Smaller Language Models","tldr":"Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. \nHowever, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get Co...","track":"Generation","underline_id":77860,"underline_url":"https://underline.io/events/395/posters/15279/poster/77860-distilling-reasoning-capabilities-into-smaller-language-model","video_url":null},{"abstract":"Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations -- even without ground-truth labels -- and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL's performance consistently improves with more demonstrations in context. Our findings unravel two different forces behind ICL and we advocate for discriminating them in future ICL research due to their distinct nature.","anthology_url":"https://aclanthology.org/2023.findings-acl.527","authors":["Jane Pan","Tianyu Gao","Howard Chen","Danqi Chen"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3796","is_paper":true,"keywords":["prompting","scaling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.527.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning","tldr":"Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning ove...","track":"Spotlight - Metropolitan Centre","underline_id":77861,"underline_url":null,"video_url":null},{"abstract":"AI-driven chatbots have become an emerging solution to address psychological distress. Due to the lack of psychotherapeutic data, researchers use dialogues scraped from online peer support forums to train them. But since the responses in such platforms are not given by professionals, they contain both conforming and non-conforming responses. In this work, we attempt to recognize these conforming and non-conforming response types present in online distress-support dialogues using labels adapted from a well-established behavioral coding scheme named  Motivational Interviewing Treatment Integrity (MITI) code and show how some response types could be rephrased into a more MI adherent form that can, in turn, enable chatbot responses to be more compliant with the MI strategy. As a proof of concept, we build several rephrasers by fine-tuning Blender and GPT3 to rephrase MI non-adherent Advise without permission responses into Advise with permission. We show how this can be achieved with the construction of pseudo-parallel corpora avoiding costs for human labor. Through automatic and human evaluation we show that in the presence of less training data, techniques such as prompting and data augmentation can be used to produce substantially good rephrasings that reflect the intended style and preserve the content of the original text.","anthology_url":"https://aclanthology.org/2023.findings-acl.334","authors":["Anuradha Welivita","Pearl Pu"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P382","is_paper":true,"keywords":["applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.334.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77306/poster_document/c270ab5e7d60365b27ade5d85205ae15.pdf","preview_image":"https://assets.underline.io/lecture/77306/poster/c48a5f2b0fe9502d6d3648d050968ded.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77306/slideshow/35005ea2362fe6c8df216b7940dffa30.pdf","title":"Boosting Distress Support Dialogue Responses with Motivational Interviewing Strategy","tldr":"AI-driven chatbots have become an emerging solution to address psychological distress. Due to the lack of psychotherapeutic data, researchers use dialogues scraped from online peer support forums to train them. But since the responses in such platforms are not given by professionals, they contain bo...","track":"Dialogue and Interactive Systems","underline_id":77306,"underline_url":"https://underline.io/events/395/posters/15200/poster/77306-boosting-distress-support-dialogue-responses-with-motivational-interviewing-strategy","video_url":null},{"abstract":"This paper presents \"HaVQA\", the first multimodal dataset for visual question answering (VQA) tasks in the Hausa language. The dataset was created by manually translating 6,022 English question-answer pairs, which are associated with 1,555 unique images from the Visual Genome dataset. As a result, the dataset provides 12,044 gold standard English-Hausa parallel sentences that were translated in a fashion that guarantees their semantic match with the corresponding visual information. We conducted several baseline experiments on the dataset, including visual question answering, visual question elicitation, text-only and multimodal machine translation.","anthology_url":"https://aclanthology.org/2023.findings-acl.646","authors":["Shantipriya Parida","Idris Abdulmumin","Shamsuddeen Hassan Muhammad","Aneesh Bose","Guneet Singh Kohli","Ibrahim Said Ahmad","Ketan Kotwal","Sayan Deb Sarkar","Ond\u0159ej Bojar","Habeebah Adamu Kakudi"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)"],"id":"P3824","is_paper":true,"keywords":["corpus creation","benchmarking","language resources","multilingual corpora","nlp datasets","datasets for low resource languages"],"languages":["hausa"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.646.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77862/poster_document/0de8860cbf0302cbcab1f750dc7d402d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77862/slideshow/137925e3083bebe13c3cd004522977a4.pdf","title":"HaVQA: A Dataset for Visual Question Answering and Multimodal Research in Hausa Language","tldr":"This paper presents \"HaVQA\", the first multimodal dataset for visual question answering (VQA) tasks in the Hausa language. The dataset was created by manually translating 6,022 English question-answer pairs, which are associated with 1,555 unique images from the Visual Genome dataset. As a result, t...","track":"Resources and Evaluation","underline_id":77862,"underline_url":"https://underline.io/events/395/posters/15279/poster/77862-havqa-a-dataset-for-visual-question-answering-and-multimodal-research-in-hausa-language","video_url":null},{"abstract":"Grammatical error correction (GEC) is a promising task aimed at correcting errors in a text. Many methods have been proposed to facilitate this task with remarkable results. However, most of them only focus on enhancing textual feature extraction without exploring the usage of other modalities' information (e.g., speech), which can also provide valuable knowledge to help the model detect grammatical errors. To shore up this deficiency, we propose a novel framework that integrates both speech and text features to enhance GEC. In detail, we create new multimodal GEC datasets for English and German by generating audio from text using the advanced text-to-speech models. Subsequently, we extract acoustic and textual representations by a multimodal encoder that consists of a speech and a text encoder. A mixture-of-experts (MoE) layer is employed to selectively align representations from the two modalities, and then a dot attention mechanism is used to fuse them as final multimodal representations. Experimental results on CoNLL14, BEA19 English, and Falko-MERLIN German show that our multimodal GEC models achieve significant improvements over strong baselines and achieve a new state-of-the-art result on the Falko-MERLIN test set.","anthology_url":"https://aclanthology.org/2023.findings-acl.594","authors":["Tao Fang","jinpeng hu","Derek F. Wong","Xiang Wan","Lidia S. Chao","Tsung-Hui Chang"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)"],"id":"P3836","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.594.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77863/poster_document/fa6afbea5fa1b2a2c76a0217e4e64879.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Improving Grammatical Error Correction with Multimodal Feature Integration","tldr":"Grammatical error correction (GEC) is a promising task aimed at correcting errors in a text. Many methods have been proposed to facilitate this task with remarkable results. However, most of them only focus on enhancing textual feature extraction without exploring the usage of other modalities' info...","track":"NLP Applications","underline_id":77863,"underline_url":"https://underline.io/events/395/posters/15279/poster/77863-improving-grammatical-error-correction-with-multimodal-feature-integration","video_url":null},{"abstract":"This paper presents an investigation into the differences between processing monolingual input and code-switching (CSW) input in the context of machine translation (MT). Specifically, we compare the performance of three MT systems (Google, mBART-50 and M2M-100-big) in terms of their ability to translate monolingual Vietnamese, a low-resource language, and Vietnamese-English CSW respectively. To our knowledge, this is the first study to systematically analyse what might happen when multilingual MT systems are exposed to CSW data using both automatic and human metrics. We find that state-of-the-art neural translation systems not only achieve higher scores on automatic metrics when processing CSW input (compared to monolingual input), but also produce translations that are consistently rated as more semantically faithful by humans. We further suggest that automatic evaluation alone is insufficient for evaluating the translation of CSW input. Our findings establish a new benchmark that offers insights into the relationship between MT and CSW.","anthology_url":"https://aclanthology.org/2023.findings-acl.893","authors":["Li Nguyen","Christopher Bryant","Oliver Mayeux","Zheng Yuan"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-4_-machine-translation-(virtual-poster)"],"id":"P3841","is_paper":true,"keywords":["switch-code translation"],"languages":["vietnamese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.893.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77864/poster_document/d0de194f36370f26edd4fd196018ae22.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"How effective is machine translation on low-resource code-switching? A case study comparing human and automatic metrics","tldr":"This paper presents an investigation into the differences between processing monolingual input and code-switching (CSW) input in the context of machine translation (MT). Specifically, we compare the performance of three MT systems (Google, mBART-50 and M2M-100-big) in terms of their ability to trans...","track":"Machine Translation","underline_id":77864,"underline_url":"https://underline.io/events/395/posters/15240/poster/77864-how-effective-is-machine-translation-on-low-resource-code-switchingquestion-a-case-study-comparing-human-and-automatic-metrics","video_url":null},{"abstract":"Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection  by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three  tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature.  Experimental results show that our optimized version of mark-then-translate, which we call EasyProject,  is easily applied to many languages and works surprisingly well, outperforming the more complex word alignment-based methods. We analyze several key factors that affect the end-task performance, and show  EasyProject works well because it can accurately preserve label span boundaries after translation. We will publicly release all our code and data.","anthology_url":"https://aclanthology.org/2023.findings-acl.357","authors":["Yang Chen","Chao Jiang","Alan Ritter","Wei Xu"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3845","is_paper":true,"keywords":["cross-lingual transfer"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.357.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77865/poster_document/2a32b6c18a6ce121a985789b9c2dab08.pdf","preview_image":"https://assets.underline.io/lecture/77865/poster/d3da57c302a6999d7fde65e95020fa5f.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77865/slideshow/cc73215a09a241e10155f47279ab92c8.pdf","title":"Frustratingly Easy Label Projection for Cross-lingual Transfer","tldr":"Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77865,"underline_url":"https://underline.io/events/395/posters/15279/poster/77865-wukong-reader-multi-modal-pre-training-for-fine-grained-visual-document-understanding","video_url":null},{"abstract":"State-of-the-art natural language processing models have been shown to achieve remarkable performance in 'closed-world' settings where all the labels in the evaluation set are known at training time. However, in real-world settings, 'novel' instances that do not belong to any known class are often observed. This renders the ability to deal with novelties crucial. To initiate a systematic research in this important area of 'dealing with novelties', we introduce NoveltyTask, a multi-stage task to evaluate a system's performance on pipelined novelty 'detection' and 'accommodation' tasks. We provide mathematical formulation of NoveltyTask and instantiate it with the authorship attribution task that pertains to identifying the correct author of a given text. We use amazon reviews corpus and compile a large dataset (consisting of 250k instances across 200 authors/labels) for NoveltyTask. We conduct comprehensive experiments and explore several baseline methods for the task. Our results show that the methods achieve considerably low performance making the task challenging and leaving sufficient room for improvement. Finally, we believe our work will encourage research in this underexplored area of dealing with novelties, an important step en route to developing robust systems.","anthology_url":"https://aclanthology.org/2023.findings-acl.113","authors":["Neeraj Varshney","Himanshu Gupta","Eric Robertson","Bing Liu","Chitta Baral"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3868","is_paper":true,"keywords":["benchmarking","evaluation methodologies"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.113.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77869/poster_document/cc70f64b9fd4d5238a23620319319cbf.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"A Unified Evaluation Framework for Novelty Detection and Accommodation in NLP with an Instantiation in Authorship Attribution","tldr":"State-of-the-art natural language processing models have been shown to achieve remarkable performance in 'closed-world' settings where all the labels in the evaluation set are known at training time. However, in real-world settings, 'novel' instances that do not belong to any known class are often o...","track":"Resources and Evaluation","underline_id":77869,"underline_url":"https://underline.io/events/395/posters/15279/poster/77869-a-unified-evaluation-framework-for-novelty-detection-and-accommodation-in-nlp-with-an-instantiation-in-authorship-attribution","video_url":null},{"abstract":"We apply reinforcement learning techniques to topic modeling by replacing the variational autoencoder in ProdLDA with a continuous action space reinforcement learning policy. We train the system with a policy gradient algorithm REINFORCE. Additionally, we introduced several modifications: modernize the neural network architecture, weight the ELBO loss, use contextual embeddings, and monitor the learning process via computing topic diversity and coherence for each training step. Experiments are\nperformed on 11 data sets. Our unsupervised model outperforms all other unsupervised models and performs on par with or better than most models using supervised labeling. Our model is outperformed on certain data sets by a model using supervised labeling and contrastive learning. We have also conducted an ablation study to provide empirical evidence of performance improvements from changes we made to ProdLDA and found that the reinforcement learning formulation boosts performance. We open-source our code implementation.","anthology_url":"https://aclanthology.org/2023.findings-acl.265","authors":["Jeremy J Costello","Marek Z Reformat"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P3878","is_paper":true,"keywords":["topic modeling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.265.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77870/poster_document/bbd1ba64e0d1bb3018162881d3c829db.pdf","preview_image":"https://assets.underline.io/lecture/77870/poster/1b33a8f04b5c39527f8b8a704a55e0c5.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77870/slideshow/61fb8255fdc0e85a37153ceebc838c40.pdf","title":"Reinforcement Learning for Topic Models","tldr":"We apply reinforcement learning techniques to topic modeling by replacing the variational autoencoder in ProdLDA with a continuous action space reinforcement learning policy. We train the system with a policy gradient algorithm REINFORCE. Additionally, we introduced several modifications: modernize ...","track":"Interpretability and Analysis of Models for NLP","underline_id":77870,"underline_url":"https://underline.io/events/395/posters/15279/poster/77870-xsim-an-improved-proxy-to-bitext-mining-performance-for-low-resource-languages","video_url":null},{"abstract":"Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions. This approach has two major shortcomings. First, we need expensive human-provided reference questions. Second, it penalises valid questions that may not have high lexical or semantic similarity to the reference questions. In this paper, we propose a new metric, RQUGE, based on the answerability of the candidate question given the context. The metric consists of a question-answering and a span scorer modules, using pre-trained models from existing literature, thus it can be used without any further training. We demonstrate that RQUGE has a higher correlation with human judgment without relying on the reference question. Additionally, RQUGE is shown to be more robust to several adversarial corruptions. Furthermore, we illustrate that we can significantly improve the performance of QA models on out-of-domain datasets by fine-tuning on synthetic data generated by a question generation model and reranked by RQUGE.","anthology_url":"https://aclanthology.org/2023.findings-acl.428","authors":["Alireza Mohammadshahi","Thomas Scialom","Majid Yazdani","Pouya Yanki","Angela Fan","James Henderson","Marzieh Saeidi"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-4_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3880","is_paper":true,"keywords":["metrics"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.428.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77871/poster_document/f591ab57a78ff6173c2bf90eada081e1.pdf","preview_image":"https://assets.underline.io/lecture/77871/poster/cd07d12a52b7221633045923cabca5cc.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77871/slideshow/4bdb105ca645d55da2df72b2087a7e0b.pdf","title":"RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question","tldr":"Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the referenc...","track":"Resources and Evaluation","underline_id":77871,"underline_url":"https://underline.io/events/395/posters/15240/poster/77871-rquge-reference-free-metric-for-evaluating-question-generation-by-answering-the-question","video_url":null},{"abstract":"In this work, we examine the vulnerability of language models to universal adversarial triggers (UATs). We propose a new white-box approach to the construction of layerwise UATs (LUATs), which searches the triggers by perturbing hidden layers of a network. On the example of three transformer models and three datasets from the GLUE benchmark, we demonstrate that our method provides better transferability in a model-to-model setting with an average gain of 9.3\\% in the fooling rate over the baseline. Moreover, we investigate triggers transferability in the task-to-task setting. Using small subsets from the datasets similar to the target tasks for choosing a perturbed layer, we show that LUATs are more efficient than vanilla UATs by 7.1\\% in the fooling rate.","anthology_url":"https://aclanthology.org/2023.findings-acl.10","authors":["Olga Tsymboi","Danil Malaev","Andrei Petrovskii","Ivan Oseledets"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3907","is_paper":true,"keywords":["adversarial attacks/examples/training"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.10.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77875/poster_document/b23dd2638e9802f245e62b2d1e5340a1.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77875/slideshow/c1fdd0abb9a1aca53b4ea9d31a7c9e57.pdf","title":"Layerwise universal adversarial attack on NLP models","tldr":"In this work, we examine the vulnerability of language models to universal adversarial triggers (UATs). We propose a new white-box approach to the construction of layerwise UATs (LUATs), which searches the triggers by perturbing hidden layers of a network. On the example of three transformer models ...","track":"Interpretability and Analysis of Models for NLP","underline_id":77875,"underline_url":"https://underline.io/events/395/posters/15200/poster/77875-layerwise-universal-adversarial-attack-on-nlp-models","video_url":null},{"abstract":"We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator's point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex education videos on TikTok. Children's exposure to sexually suggestive videos has been shown to have adversarial effects on their development (Collins et al. 2017). Meanwhile, virtual sex education, especially on subjects that are more relevant to the LGBTQIA+ community, is very valuable (Mitchell et al. 2014). The platform's current system removes/punishes some of both types of videos, even though they serve different purposes. Our dataset contains video URLs, and it is also audio transcribed. To validate its importance, we explore two transformer-based models for classifying the videos. Our preliminary results suggest that the task of distinguishing between these types of videos is learnable but challenging. These experiments suggest that this dataset is meaningful and invites further study on the subject.","anthology_url":"https://aclanthology.org/2023.findings-acl.365","authors":["Enfa Rose George","Mihai Surdeanu"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3917","is_paper":true,"keywords":["nlp tools for social analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.365.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77876/poster_document/6211eafaf662a22b3692b9c6893cb5b1.pdf","preview_image":"https://assets.underline.io/lecture/77876/poster/58d2edaa7ca39792e941df9c2c859201.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"It's not Sexually Suggestive; It's Educative | Separating Sex Education from Suggestive Content on TikTok videos","tldr":"We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator's point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex ed...","track":"Computational Social Science and Cultural Analytics","underline_id":77876,"underline_url":"https://underline.io/events/395/posters/15279/poster/77876-which-spurious-correlations-impact-reasoning-in-nli-modelsquestion-a-visual-interactive-diagnosis-through-data-constrained-counterfactuals","video_url":null},{"abstract":"Location information can support social media analyses by providing geographic context. Some of the most accurate and popular Twitter geolocation systems rely on rule-based methods that examine the user-provided profile location, which fail to handle informal or noisy location names. We propose Geo-Seq2seq, a sequence-to-sequence (seq2seq) model for Twitter user geolocation that rewrites noisy, multilingual user-provided location strings into structured English location names. We train our system on tens of millions of multilingual location string and geotagged-tweet pairs. Compared to leading methods, our model vastly increases coverage (i.e., the number of users we can geolocate) while achieving comparable or superior accuracy. Our error analysis reveals that constrained decoding helps the model produce valid locations according to a location database. Finally, we measure biases across language, country of origin, and time to evaluate fairness, and find that while our model can generalize well to unseen temporal data, performance does vary by language and country.","anthology_url":"https://aclanthology.org/2023.findings-acl.294","authors":["Jingyu Zhang","Alexandra DeLucia","Chenyu Zhang","Mark Dredze"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)"],"id":"P3923","is_paper":true,"keywords":["nlp tools for social analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.294.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77879/poster_document/920f1376d0c26faaa055b04732fd932e.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning","tldr":"Location information can support social media analyses by providing geographic context. Some of the most accurate and popular Twitter geolocation systems rely on rule-based methods that examine the user-provided profile location, which fail to handle informal or noisy location names. We propose Geo-...","track":"Computational Social Science and Cultural Analytics","underline_id":77879,"underline_url":"https://underline.io/events/395/posters/15279/poster/77879-geo-seq2seq-twitter-user-geolocation-on-noisy-data-through-sequence-to-sequence-learning","video_url":null},{"abstract":"Bias research in NLP seeks to analyse models for social biases, thus helping NLP practitioners uncover, measure, and mitigate social harms. \nWe analyse the body of work that uses prompts and templates to assess bias in language models. We draw on a measurement modelling framework to create a taxonomy of attributes that capture what a bias test aims to measure and how that measurement is carried out. \nBy applying this taxonomy to 90 bias tests, we illustrate qualitatively and quantitatively that core aspects of bias test conceptualisations and operationalisations are frequently unstated or ambiguous, carry implicit assumptions, or be mismatched.\nOur analysis illuminates the scope of possible bias types the field is able to measure, and reveals types that are as yet under-researched.\nWe offer guidance to enable the community to explore a wider section of the possible bias space, and to better close the gap between desired outcomes and experimental design, both for bias and for evaluating language models more broadly.","anthology_url":"https://aclanthology.org/2023.findings-acl.139","authors":["Seraphina Goldfarb-Tarrant","Eddie L. Ungless","Esma Balkir","Su Lin Blodgett"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-7_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3926","is_paper":true,"keywords":["evaluation","methodology","ai hype & expectations","lessons from other fields"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.139.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77880/poster_document/ff62d56c821951964a53813552bfb9c1.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"This prompt is measuring <mask>: evaluating bias evaluation in language models","tldr":"Bias research in NLP seeks to analyse models for social biases, thus helping NLP practitioners uncover, measure, and mitigate social harms. \nWe analyse the body of work that uses prompts and templates to assess bias in language models. We draw on a measurement modelling framework to create a taxonom...","track":"Theme: Reality Check","underline_id":77880,"underline_url":"https://underline.io/events/395/posters/15279/poster/77880-this-prompt-is-measuring-lessmaskgreater-evaluating-bias-evaluation-in-language-models","video_url":null},{"abstract":"Detecting named entities in text has long been a core NLP task. However, not much work has gone into distinguishing whether an entity mention is addressing the entity vs. referring to the entity; e.g., \\textit{John, would you turn the light off?} vs. \\textit{John turned the light off}. While this distinction is marked by a \\textit{vocative case} marker in some languages, many modern Indo-European languages such as English do not use such explicit vocative markers, and the distinction is left to be interpreted in context. In this paper, we present a new annotated dataset that captures the \\textit{address} vs. \\textit{reference} distinction in English, an automatic tagger that performs at 85\\% accuracy in making this distinction, and demonstrate how this distinction is important in NLP and computational social science applications in English language.","anthology_url":"https://aclanthology.org/2023.findings-acl.425","authors":["Vinodkumar Prabhakaran","Aida Mostafazadeh Davani","Melissa Ferguson","Stav Atir"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-4_-discourse-and-pragmatics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3940","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.425.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Distinguishing Address vs. Reference Mentions of Personal Names in Text","tldr":"Detecting named entities in text has long been a core NLP task. However, not much work has gone into distinguishing whether an entity mention is addressing the entity vs. referring to the entity; e.g., \\textit{John, would you turn the light off?} vs. \\textit{John turned the light off}. While this di...","track":"Discourse and Pragmatics","underline_id":77882,"underline_url":"https://underline.io/events/395/posters/15240/poster/77882-distinguishing-address-vs-reference-mentions-of-personal-names-in-text","video_url":null},{"abstract":"Dialogue participants may have varying levels of knowledge about the topic under discussion. In such cases, it is essential for speakers to adapt their utterances by taking their audience into account. Yet, it is an open question how such adaptation can be modelled in computational agents. In this paper, we model a visually grounded referential game between a knowledgeable speaker and a listener with more limited visual and linguistic experience. Inspired by psycholinguistic theories, we endow our speaker with the ability to adapt its referring expressions via a simulation module that monitors the effectiveness of planned utterances from the listener's perspective. We propose an adaptation mechanism building on plug-and-play approaches to controlled language generation, where utterance generation is steered on the fly by the simulator without finetuning the speaker's underlying language model. Our results and analyses show that our approach is effective: the speaker's utterances become closer to the listener's domain of expertise, which leads to higher communicative success.","anthology_url":"https://aclanthology.org/2023.findings-acl.258","authors":["Ece Takmaz","Nicolo' Brandizzi","Mario Giulianelli","Sandro Pezzelle","Raquel Fernandez"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3948","is_paper":true,"keywords":["image text matching"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.258.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77883/poster_document/a9d381b3114680625f3c0237ab2d4a85.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77883/slideshow/93c020131895de98bf1c8ff4aba24bcc.pdf","title":"Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind","tldr":"Dialogue participants may have varying levels of knowledge about the topic under discussion. In such cases, it is essential for speakers to adapt their utterances by taking their audience into account. Yet, it is an open question how such adaptation can be modelled in computational agents. In this p...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77883,"underline_url":"https://underline.io/events/395/posters/15200/poster/77883-speaking-the-language-of-your-listener-audience-aware-adaptation-via-plug-and-play-theory-of-mind","video_url":null},{"abstract":"Neural information retrieval (IR) systems have progressed rapidly in recent years, in large part due to the release of publicly available benchmarking tasks. Unfortunately, some dimensions of this progress are illusory: the majority of the popular IR benchmarks today focus exclusively on downstream task accuracy and thus conceal the costs incurred by systems that trade away efficiency for quality. Latency, hardware cost, and other efficiency considerations are paramount to the deployment of IR systems in user-facing settings. We propose that IR benchmarks structure their evaluation methodology to include not only metrics of accuracy, but also efficiency considerations such as a query latency and the corresponding cost budget for a reproducible hardware setting. For the popular IR benchmarks MS MARCO and XOR-TyDi, we show how the best choice of IR system varies according to how these efficiency considerations are chosen and weighed. We hope that future benchmarks will adopt these guidelines toward more holistic IR evaluation.","anthology_url":"https://aclanthology.org/2023.findings-acl.738","authors":["Keshav Santhanam","Jon Saad-Falcon","Martin Franz","Omar Khattab","Avi Sil","Radu Florian","Md Arafat Sultan","Salim Roukos","Matei Zaharia","Christopher Potts"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-7_-theme_-reality-check-(virtual-poster)"],"id":"P395","is_paper":true,"keywords":["evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.738.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77307/poster/7190516c2fe24a3b017782d64e0f1e5d.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77307/slideshow/100b3964844ec6a4b24ba627b946c087.pdf","title":"Moving Beyond Downstream Task Accuracy for Information Retrieval Benchmarking","tldr":"Neural information retrieval (IR) systems have progressed rapidly in recent years, in large part due to the release of publicly available benchmarking tasks. Unfortunately, some dimensions of this progress are illusory: the majority of the popular IR benchmarks today focus exclusively on downstream ...","track":"Theme: Reality Check","underline_id":77307,"underline_url":"https://underline.io/events/395/posters/15279/poster/77307-moving-beyond-downstream-task-accuracy-for-information-retrieval-benchmarking","video_url":null},{"abstract":"Comprehensive multilingual evaluations have been encouraged by emerging cross-lingual benchmarks and constrained by existing parallel datasets. To partially mitigate this limitation, we extended the Cross-lingual Natural Language Inference (XNLI) corpus with Croatian. The development and test sets were translated by a professional translator, and we show that Croatian is consistent with other XNLI dubs. The train set is translated using Facebook's 1.2B parameter m2m\\_100 model. We thoroughly analyze the Croatian train set and compare its quality with the existing machine-translated German set. The comparison is based on 2000 manually scored sentences per language using a variant of the Direct Assessment (DA) score commonly used at the Conference on Machine Translation (WMT). Our findings reveal that a less-resourced language like Croatian is still lacking in translation quality of longer sentences compared to German. However, both sets have a substantial amount of poor quality translations, which should be considered in translation-based training or evaluation setups.","anthology_url":"https://aclanthology.org/2023.findings-acl.142","authors":["Leo Obadi\u0107","Andrej Jertec","Marko Rajnovi\u0107","Branimir Dropulji\u0107"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-4_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P3954","is_paper":true,"keywords":["corpus creation","language resources","multilingual corpora","nlp datasets","evaluation","datasets for low resource languages"],"languages":["croatian"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.142.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77884/poster_document/14b7a4be3fa7813ad75fba8b77213d75.pdf","preview_image":"https://assets.underline.io/lecture/77884/poster/cc3018e9ba722b0cf12c8da4c37e0daa.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77884/slideshow/aaeee1914ef1bf5b53e792e1bed71c58.pdf","title":"C-XNLI: Croatian Extension of XNLI Dataset","tldr":"Comprehensive multilingual evaluations have been encouraged by emerging cross-lingual benchmarks and constrained by existing parallel datasets. To partially mitigate this limitation, we extended the Cross-lingual Natural Language Inference (XNLI) corpus with Croatian. The development and test sets w...","track":"Resources and Evaluation","underline_id":77884,"underline_url":"https://underline.io/events/395/posters/15240/poster/77884-c-xnli-croatian-extension-of-xnli-dataset","video_url":null},{"abstract":"Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively keeps or removes each weight to produce a subnetwork. They prevent forgetting as the network weights are not being updated. Although there is no forgetting, the performance of SupSup is sub-optimal because fixed weights restrict its representational power. Furthermore, there is no accumulation or transfer of knowledge inside the model when new tasks are learned. Hence, we propose ExSSNeT (Exclusive Supermask SubNetwork Training), that performs exclusive and non-overlapping subnetwork weight training. This avoids conflicting updates to the shared weights by subsequent tasks to improve performance while still preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge Transfer (KKT) module that utilizes previously acquired knowledge to learn new tasks better and faster. We demonstrate that ExSSNeT outperforms strong previous methods on both NLP and Vision domains while preventing forgetting. Moreover, ExSSNeT is particularly advantageous for sparse masks that activate 2-10\\% of the model parameters, resulting in an average improvement of 8.3\\% over SupSup. Furthermore, ExSSNeT scales to a large number of tasks (100).","anthology_url":"https://aclanthology.org/2023.findings-acl.36","authors":["Prateek Yadav","Mohit Bansal"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3961","is_paper":true,"keywords":["generalization","continual learning","meta learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.36.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77887/poster_document/e973523b11d1c1de8ede0fe825e11b43.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77887/slideshow/0bd969dec71f5a4c42816121f75f022f.pdf","title":"Exclusive Supermask Subnetwork Training for Continual Learning","tldr":"Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively keep...","track":"Machine Learning for NLP","underline_id":77887,"underline_url":"https://underline.io/events/395/posters/15240/poster/77887-exclusive-supermask-subnetwork-training-for-continual-learning","video_url":null},{"abstract":"Semantic tasks are rarely formally defined, and the exact relationship between them is an open question. We introduce a taxonomy that elucidates the connection between several problems in lexical semantics, including monolingual and cross-lingual variants. Our theoretical framework is based on the hypothesis of the equivalence of concept and meaning distinctions. Using algorithmic problem reductions, we demonstrate that all problems in the taxonomy can be reduced to word sense disambiguation (WSD), and that WSD itself can be reduced to some problems, making them theoretically equivalent. In addition, we carry out experiments that strongly support the soundness of the concept-meaning hypothesis, and the correctness of our reductions.","anthology_url":"https://aclanthology.org/2023.findings-acl.623","authors":["Bradley M Hauer","Grzegorz Kondrak"],"category":"Findings","demo_url":null,"display_track":"Semantics: Lexical","event_ids":["session-7_-semantics_-lexical-(virtual-poster)"],"id":"P3970","is_paper":true,"keywords":["multilinguality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.623.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77888/poster_document/bcd9eee94d8905ef2976433863e92b0d.pdf","preview_image":"https://assets.underline.io/lecture/77888/poster/34fab6030d4a1007abf174b36bb7c48c.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77888/slideshow/7a3a481cfde142cde9d4e6980c10788d.pdf","title":"Taxonomy of Problems in Lexical Semantics","tldr":"Semantic tasks are rarely formally defined, and the exact relationship between them is an open question. We introduce a taxonomy that elucidates the connection between several problems in lexical semantics, including monolingual and cross-lingual variants. Our theoretical framework is based on the h...","track":"Semantics: Lexical","underline_id":77888,"underline_url":"https://underline.io/events/395/posters/15279/poster/77888-taxonomy-of-problems-in-lexical-semantics","video_url":null},{"abstract":"As NLP systems are increasingly deployed at scale, concerns about their potential negative impacts have attracted the attention of the research community, yet discussions of risk have mostly been at an abstract level and focused on generic AI or NLP applications.  We argue that clearer assessments of risks and harms to users---and concrete strategies to mitigate them---will be possible when we specialize the analysis to more concrete applications and their plausible users.  As an illustration, this paper is grounded in cooking recipe procedural document question answering (ProcDocQA), where there are well-defined risks to users such as injuries or allergic reactions.  Our case study shows that an existing language model, applied in ``zero-shot'' mode, quantitatively answers real-world questions about recipes as well or better than the humans who have answered the questions on the web.  Using a novel questionnaire informed by theoretical work on AI risk, we conduct a risk-oriented error analysis that could then inform the design of a future system to be deployed with lower risk of harm and better performance.","anthology_url":"https://aclanthology.org/2023.findings-acl.81","authors":["Nikita Haduong","Alice Gao","Noah A. Smith"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-4_-theme_-reality-check-(virtual-poster)"],"id":"P3982","is_paper":true,"keywords":["right for the wrong reasons","ai hype & expectations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.81.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77889/poster_document/6cc28f8709df8a522d7fc4441007ac2b.pdf","preview_image":"https://assets.underline.io/lecture/77889/poster/e2a39a8d09f7b7399c831ed188eb9434.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Risks and NLP Design:  A Case Study on Procedural Document QA","tldr":"As NLP systems are increasingly deployed at scale, concerns about their potential negative impacts have attracted the attention of the research community, yet discussions of risk have mostly been at an abstract level and focused on generic AI or NLP applications.  We argue that clearer assessments o...","track":"Theme: Reality Check","underline_id":77889,"underline_url":"https://underline.io/events/395/posters/15240/poster/77889-risks-and-nlp-design-a-case-study-on-procedural-document-qa","video_url":null},{"abstract":"Cutting-edge image generation has been praised for producing high-quality images, suggesting a ubiquitous future in a variety of applications. However, initial studies have pointed to the potential for harm due to predictive bias, reflecting and potentially reinforcing cultural stereotypes. In this work, we are the first to investigate how multimodal models handle diverse gender identities. Concretely, we conduct a thorough analysis in which we compare the output of three image generation models for prompts containing cisgender vs. non-cisgender identity terms. Our findings demonstrate that certain non-cisgender identities are consistently (mis)represented as less human, more stereotyped and more sexualised. We complement our experimental analysis with (a) a survey among non-cisgender individuals and (b) a series of interviews, to establish which harms affected individuals anticipate, and how they would like to be represented. We find respondents are particularly concerned about misrepresentation, and the potential to drive harmful behaviours and beliefs. Simple heuristics to limit offensive content are widely rejected, and instead respondents call for community involvement, curated training data and the ability to customise. These improvements could pave the way for a future where change is led by the affected community, and technology is used to positively ''[portray] queerness in ways that we haven't even thought of''' rather than reproducing stale, offensive stereotypes.","anthology_url":"https://aclanthology.org/2023.findings-acl.502","authors":["Eddie L. Ungless","Bjorn Ross","Anne Lauscher"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)"],"id":"P3983","is_paper":true,"keywords":["ethical considerations in nlp applications","reflections and critiques"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.502.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77890/poster_document/32b618bf51c0fcf93420e9b490757dbf.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77890/slideshow/68cf0e68c5fdc1ec51f903cdc9a7125c.pdf","title":"Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models","tldr":"Cutting-edge image generation has been praised for producing high-quality images, suggesting a ubiquitous future in a variety of applications. However, initial studies have pointed to the potential for harm due to predictive bias, reflecting and potentially reinforcing cultural stereotypes. In this ...","track":"Ethics and NLP","underline_id":77890,"underline_url":"https://underline.io/events/395/posters/15279/poster/77890-stereotypes-and-smut-the-mis-representation-of-non-cisgender-identities-by-text-to-image-models","video_url":null},{"abstract":"Pre-trained language models (PLMs) have achieved great success in NLP and have recently been used for tasks in computational semantics.  However, these tasks do not fully benefit from PLMs since meaning representations are not explicitly included. We introduce multilingual pre-trained language-meaning models based on Discourse Representation Structures (DRSs), including meaning representations besides natural language texts in the same model, and design a new strategy to reduce the gap between the pre-training and fine-tuning objectives.  Since DRSs are language neutral, cross-lingual transfer learning is adopted to further improve the performance of non-English tasks. Automatic evaluation results show that our approach achieves the best performance on both the multilingual DRS parsing and DRS-to-text generation tasks. Correlation analysis between automatic metrics and human judgements on the generation task further validates the effectiveness of our model. Human inspection reveals that out-of-vocabulary tokens are the main cause of erroneous results.","anthology_url":"https://aclanthology.org/2023.findings-acl.345","authors":["Chunliu Wang","Huiyuan Lai","Malvina Nissim","Johan Bos"],"category":"Findings","demo_url":null,"display_track":"Syntax: Tagging, Chunking, and Parsing","event_ids":["session-7_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P3985","is_paper":true,"keywords":["semantic parsing","multi-task approaches (large definition)","low-resources languages pos tagging, parsing and related tasks"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.345.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77891/poster_document/0656474fb1fe71e5e837f4b4e304b53d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77891/slideshow/f6fb0c1d1221240e11670e1f3f7fe327.pdf","title":"Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation","tldr":"Pre-trained language models (PLMs) have achieved great success in NLP and have recently been used for tasks in computational semantics.  However, these tasks do not fully benefit from PLMs since meaning representations are not explicitly included. We introduce multilingual pre-trained language-meani...","track":"Syntax: Tagging, Chunking, and Parsing","underline_id":77891,"underline_url":"https://underline.io/events/395/posters/15279/poster/77891-pre-trained-language-meaning-models-for-multilingual-parsing-and-generation","video_url":null},{"abstract":"Language tasks involving character-level manipulations (e.g., spelling corrections, arithmetic operations, word games) are challenging for models operating on subword units. To address this, we develop a causal intervention framework to learn robust and interpretable character representations inside subword-based language models. Our method treats each character as a typed variable in a causal model and learns such causal structures by adapting the interchange intervention training method of Geiger et al. (2021). We additionally introduce a suite of character-level tasks that systematically vary in their dependence on meaning and sequence-level context. While character-level models still perform best on purely form-based tasks like string reversal, our method outperforms character-level models on more complex tasks that blend form, meaning, and context, such as spelling correction in context and word search games. Compared with standard subword-based models, our approach also significantly improves robustness on unseen token sequences and leads to human-interpretable internal representations of characters.","anthology_url":"https://aclanthology.org/2023.findings-acl.770","authors":["Jing Huang","Zhengxuan Wu","Kyle Mahowald","Christopher Potts"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P399","is_paper":true,"keywords":["hierarchical & concept explanations","robustness"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.770.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77308/poster_document/346390b03cd23b56f779ec2f0ddefb8d.pdf","preview_image":"https://assets.underline.io/lecture/77308/poster/ae3c2e00526da7cc6eaf369abbd4da5d.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77308/slideshow/2904c0984d918c7b3fa2a902f5397b4c.pdf","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training","tldr":"Language tasks involving character-level manipulations (e.g., spelling corrections, arithmetic operations, word games) are challenging for models operating on subword units. To address this, we develop a causal intervention framework to learn robust and interpretable character representations inside...","track":"Interpretability and Analysis of Models for NLP","underline_id":77308,"underline_url":"https://underline.io/events/395/posters/15200/poster/77308-inducing-character-level-structure-in-subword-based-language-models-with-type-level-interchange-intervention-training","video_url":null},{"abstract":"Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in inputs. We introduce SITUATIONSUPERVISION, a family of approaches for improving coherence in LMs by training them to construct and condition on explicit representations of entities and their states. SITUATIONSUPERVISION has two components: an *auxiliary situation modeling* task that trains models to predict entity state representations in context, and a *latent state inference* procedure that imputes these states from partially annotated training data. SITUATIONSUPERVISION can be applied via fine-tuning (by supervising LMs to encode state variables in their hidden representations) and prompting (by inducing LMs to interleave textual descriptions of entity states with output text). In both cases, it requires only a small number of state annotations to produce substantial coherence improvements (up to an 16\\% reduction in errors), showing that standard LMs can be efficiently adapted to explicitly model language and aspects of its meaning.","anthology_url":"https://aclanthology.org/2023.findings-acl.795","authors":["Belinda Z. Li","Maxwell Nye","Jacob Andreas"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-1_-generation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P3993","is_paper":true,"keywords":["text-to-text generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.795.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77892/poster/df08ef1fd9d0409faf7d8453394f9398.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77892/slideshow/c6bd85e774cc83e8f73e050434cec496.pdf","title":"Language Modeling with Latent Situations","tldr":"Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in inputs. We introduce SITUATIONSUPERVISION, a family of approaches for improving coherence in LMs by training them to construct and condition ...","track":"Generation","underline_id":77892,"underline_url":"https://underline.io/events/395/posters/15200/poster/77892-language-modeling-with-latent-situations","video_url":null},{"abstract":"Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.","anthology_url":"https://aclanthology.org/2023.findings-acl.264","authors":["Victoria Lin","Louis-Philippe Morency"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4012","is_paper":true,"keywords":["human-subject application-grounded evaluations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.264.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77893/poster/90e682a01aeff3a6535e50cfac967917.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77893/slideshow/6f70cd6f69daa53efa2904ba4ad06a75.pdf","title":"SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations","tldr":"Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language...","track":"Interpretability and Analysis of Models for NLP","underline_id":77893,"underline_url":"https://underline.io/events/395/posters/15279/poster/77893-sentecon-leveraging-lexicons-to-learn-human-interpretable-language-representations","video_url":null},{"abstract":"Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space. Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.","anthology_url":"https://aclanthology.org/2023.findings-acl.218","authors":["Thilini Wijesiriwardene","Ruwan Wickramarachchi","Bimal Gajera","Shreeyash Mukul Gowaikar","Chandan Gupta","Aman Chadha","Aishwarya Naresh Reganti","Amit Sheth","Amitava Das"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-4_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4021","is_paper":true,"keywords":["benchmarking"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.218.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77894/poster_document/80b8445e83bfe9deb5419344e3fd470c.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77894/slideshow/eb06df558a10899b1a7fcafcc90077d8.pdf","title":"ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models","tldr":"Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benc...","track":"Resources and Evaluation","underline_id":77894,"underline_url":"https://underline.io/events/395/posters/15240/poster/77894-analogical-a-novel-benchmark-for-long-text-analogy-evaluation-in-large-language-models","video_url":null},{"abstract":"Collaboration increasingly happens online. This is especially true for large groups working on global tasks, with collaborators all around the globe. The size and distributed nature of such groups makes decision-making challenging. This paper proposes a set of dialog acts for the study of decision-making mechanisms in such groups, and provides a new annotated dataset based on real-world data from the public mail-archives of one such organisation -- the Internet Engineering Task Force (IETF). We provide an initial data analysis showing that this dataset can be used to better understand decision-making in such organisations. Finally, we experiment with a preliminary transformer-based dialog act tagging model.","anthology_url":"https://aclanthology.org/2023.findings-acl.378","authors":["Mladen Karan","Prashant Khare","Ravi Shekhar","Stephen McQuistin","Ignacio Castro","Gareth Tyson","Colin Perkins","Patrick G.T. Healey","Matthew Purver"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4028","is_paper":true,"keywords":["corpus creation","language resources","nlp datasets","evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.378.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77895/poster_document/0d040fe2acf90f339b32e8c64ad32625.pdf","preview_image":"https://assets.underline.io/lecture/77895/poster/512c4888f0d4e58e2b9b114b84d17900.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77895/slideshow/4ac64bd27257955f160c807b998814bb.pdf","title":"LEDA: a Large-Organization Email-Based Decision-Dialogue-Act Analysis Dataset","tldr":"Collaboration increasingly happens online. This is especially true for large groups working on global tasks, with collaborators all around the globe. The size and distributed nature of such groups makes decision-making challenging. This paper proposes a set of dialog acts for the study of decision-m...","track":"Resources and Evaluation","underline_id":77895,"underline_url":"https://underline.io/events/395/posters/15279/poster/77895-leda-a-large-organization-email-based-decision-dialogue-act-analysis-dataset","video_url":null},{"abstract":"In this paper, we describe our work on social bias detection in a low-resource multilingual setting in which the languages are from two very divergent families- Indo-European (English, Hindi, and Italian) and Altaic (Korean). Currently, the majority of the social bias datasets available are in English and this inhibits progress on social bias detection in low-resource languages. To address this problem, we introduce a new dataset for social bias detection in Hindi and investigate multilingual transfer learning using publicly available English, Italian, and Korean datasets. The Hindi dataset contains ~ 9k social media posts annotated for (i) binary bias labels (bias/neutral), (ii) binary labels for sentiment (positive/negative), (iii) target groups for each bias category, and (iv) rationale for annotated bias labels (a short piece of text). We benchmark our Hindi dataset using different multilingual models, with XLM-R achieving the best performance of 80.8 macro-F1 score. Our results show that the detection of social biases in resource-constrained languages such as Hindi and Korean may be improved with the use of a similar dataset in English. We also show that translating all datasets into English does not work effectively for detecting social bias, since the nuances of source language are lost in translation. All the scripts and datasets utilized in this study will be publicly available.","anthology_url":"https://aclanthology.org/2023.findings-acl.842","authors":["Nihar Ranjan Sahoo","Niteesh Kumar Reddy Mallela","Pushpak Bhattacharyya"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-1_-ethics-and-nlp-(virtual-poster)"],"id":"P4032","is_paper":true,"keywords":["data ethics","human factors in nlp","ethical considerations in nlp applications"],"languages":["hindi"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.842.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77896/slideshow/0fa6f10bc16078d3c64374d76452074b.pdf","title":"With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages","tldr":"In this paper, we describe our work on social bias detection in a low-resource multilingual setting in which the languages are from two very divergent families- Indo-European (English, Hindi, and Italian) and Altaic (Korean). Currently, the majority of the social bias datasets available are in Engli...","track":"Ethics and NLP","underline_id":77896,"underline_url":"https://underline.io/events/395/posters/15200/poster/77896-with-prejudice-to-none-a-few-shot-multilingual-transfer-learning-approach-to-detect-social-bias-in-low-resource-languages","video_url":null},{"abstract":"Rhetorical Structure Theory implies no single discourse interpretation of a text, and the limitations of RST parsers further exacerbate inconsistent parsing of similar structures. Therefore, it is important to take into account that the same argumentative structure can be found in semantically similar texts with varying rhetorical structures. In this work, the differences between paraphrases within the same argument scheme are evaluated from a rhetorical perspective. The study proposes a deep dependency parsing model to assess the connection between rhetorical and argument structures. The model utilizes rhetorical relations; RST structures of paraphrases serve as training data augmentations. The method allows for end-to-end argumentation analysis using a rhetorical tree instead of a word sequence. It is evaluated on the bilingual Microtexts corpus, and the first results on fully-fledged argument parsing for the Russian version of the corpus are reported. The results suggest that argument mining can benefit from multiple variants of discourse structure.","anthology_url":"https://aclanthology.org/2023.findings-acl.209","authors":["Elena Chistova"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-4_-discourse-and-pragmatics-(virtual-poster)"],"id":"P4037","is_paper":true,"keywords":["argument mining"],"languages":["russian"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.209.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77897/poster_document/0180e9733ce203c69c1f4921d7cc47e8.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77897/slideshow/65153bb1f6f54f4c0e83eeee81a8cfc2.pdf","title":"End-to-End Argument Mining over Varying Rhetorical Structures","tldr":"Rhetorical Structure Theory implies no single discourse interpretation of a text, and the limitations of RST parsers further exacerbate inconsistent parsing of similar structures. Therefore, it is important to take into account that the same argumentative structure can be found in semantically simil...","track":"Discourse and Pragmatics","underline_id":77897,"underline_url":"https://underline.io/events/395/posters/15240/poster/77897-end-to-end-argument-mining-over-varying-rhetorical-structures","video_url":null},{"abstract":"Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding or schema linking, it is more challenging to design a unified model for diverse modality configurations. To bring various modality configurations together, we constructed a benchmark for diverse-modal EL (DMEL) from existing EL datasets, covering all three modalities including text, image and table. To approach the DMEL task, we proposed a generative diverse-modal model (GDMM) following a multimodal-encoder-decoder paradigm. Pre-training GDMM with rich corpora builds a solid foundation for DMEL without storing the entire KB for inference. Fine-tuning GDMM builds a stronger DMEL baseline, outperforming state-of-the-art task-specific EL models by 8.51 F1 score on average. Additionally, extensive error analyses are conducted to highlight the challenge of DMEL, facilitating future researches on this task.","anthology_url":"https://aclanthology.org/2023.findings-acl.497","authors":["Sijia Wang","Alexander Hanbo Li","Henghui Zhu","Sheng Zhang","Pramuditha Perera","Chung-Wei Hang","Jie Ma","William Yang Wang","Zhiguo Wang","Vittorio Castelli","Bing Xiang","Patrick Ng"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-1_-information-extraction-(virtual-poster)"],"id":"P4049","is_paper":true,"keywords":["entity linking/disambiguation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.497.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77900/poster_document/dee4bc3543b74b83c673655835a45e80.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Benchmarking Diverse-Modal Entity Linking with Generative Models","tldr":"Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding or schema linking, it is more challenging to design a unified mode...","track":"Information Extraction","underline_id":77900,"underline_url":"https://underline.io/events/395/posters/15200/poster/77900-benchmarking-diverse-modal-entity-linking-with-generative-models","video_url":null},{"abstract":"Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies in a collection of documents. However, traditional HTMs often produce hierarchies where lower-level topics are unrelated and not specific enough to their higher-level topics. Additionally, these methods can be computationally expensive. We present HyHTM - a Hyperbolic geometry-based Hierarchical Topic Model - that addresses these limitations by incorporating hierarchical information from hyperbolic geometry to explicitly model hierarchies in topic models. Experimental results with four baselines show that HyHTM can better attend to parent-child relationships among topics. HyHTM produces coherent topic hierarchies that specialize in granularity from generic higher-level topics to specific lower-level topics. Further, our model is significantly faster and leaves a much smaller memory footprint than our best-performing baseline. We have made the source code for our algorithm publicly accessible.","anthology_url":"https://aclanthology.org/2023.findings-acl.742","authors":["Simra Shahid","Tanay Anand","Nikitha Srikanth","Sumit Bhatia","Balaji Krishnamurthy","nikaash puri"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4051","is_paper":true,"keywords":["topic modeling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.742.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77901/poster_document/b1942454e74317478e3faac57d4c1653.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77901/slideshow/db25430b4bce5750caf869f1ecee6a08.pptx","title":"HyHTM: Hyperbolic Geometry-based Hierarchical Topic Model","tldr":"Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies in a collection of documents. However, traditional HTMs often produce hierarchies where lower-level topics are unrelated and not specific enough to their higher-level topics. Additionally, these methods can be computationa...","track":"Interpretability and Analysis of Models for NLP","underline_id":77901,"underline_url":"https://underline.io/events/395/posters/15200/poster/77901-hyhtm-hyperbolic-geometry-based-hierarchical-topic-model","video_url":null},{"abstract":"Recent progress in representation and contrastive learning in NLP has not widely considered the class of sociopragmatic meaning (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate speech, humor, sarcasm). Our framework outperforms other contrastive learning frameworks for both in-domain and out-of-domain data, across both the general and few-shot settings. For example, compared to two popular pre-trained language models, our model obtains an improvement of 11.66 average F1 on 16 datasets when fine-tuned on only 20 training samples per dataset. We also show that our framework improves uniformity and preserves the semantic structure of representations. Our code is available at: https://github.com/UBC-NLP/infodcl","anthology_url":"https://aclanthology.org/2023.findings-acl.152","authors":["Chiyu Zhang","Muhammad Abdul-Mageed","Ganesh Jawahar"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4053","is_paper":true,"keywords":["emoji prediction and analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.152.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77902/poster_document/1b8099d50b832ab42d8aab52e485dd14.pdf","preview_image":"https://assets.underline.io/lecture/77902/poster/a58ce491648f28f5dcdc68f417665bb3.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77902/slideshow/9dabf5aef998c144c72f23d2324cba09.pdf","title":"Contrastive Learning of Sociopragmatic Meaning in Social Media","tldr":"Recent progress in representation and contrastive learning in NLP has not widely considered the class of sociopragmatic meaning (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferab...","track":"Computational Social Science and Cultural Analytics","underline_id":77902,"underline_url":"https://underline.io/events/395/posters/15279/poster/77902-contrastive-learning-of-sociopragmatic-meaning-in-social-media","video_url":null},{"abstract":"Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.","anthology_url":"https://aclanthology.org/2023.findings-acl.188","authors":["Xianming Li","Zongxi Li","Xiaotian Luo","Haoran Xie","Xing Lee","Yingbin Zhao","Fu Lee Wang","Qing Li"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-7_-information-retrieval-and-text-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P406","is_paper":true,"keywords":["document representation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.188.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77310/poster_document/3614fd0e25b780b4ce7309efc88986cc.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77310/slideshow/c111e64fd2d754ab3ac7cd7978941cc0.pdf","title":"Recurrent Attention Networks for Long-text Modeling","tldr":"Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention bac...","track":"Information Retrieval and Text Mining","underline_id":77310,"underline_url":"https://underline.io/events/395/posters/15279/poster/77310-recurrent-attention-networks-for-long-text-modeling","video_url":null},{"abstract":"Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that 4<5) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMscorrespond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absence of the neural circuitry that directly supports these representations in the human brain. This research shows the utility of understanding LLMs using behavioral benchmarks and points the way to future work on the number of representations of LLMs and their cognitive plausibility.","anthology_url":"https://aclanthology.org/2023.findings-acl.383","authors":["Raj Sanjay Shah","Vijay Marupudi","Reba Koenen","Khushi Bhardwaj","Sashank Varma"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-7_-theme_-reality-check-(virtual-poster)"],"id":"P4064","is_paper":true,"keywords":["lessons from other fields"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.383.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77904/poster/c979e01afe3e73dd5c95733300fc2fa1.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77904/slideshow/b8f94131a1bd77a0053339036d7ad810.pptx","title":"Numeric Magnitude Comparison Effects in Large Language Models","tldr":"Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that...","track":"Theme: Reality Check","underline_id":77904,"underline_url":"https://underline.io/events/395/posters/15279/poster/77904-numeric-magnitude-comparison-effects-in-large-language-models","video_url":null},{"abstract":"Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE). Prior work addresses this using task specific data or synthetic data augmentation, both of which require intervention for each dialect and task pair. This poses a scalability issue that prevents the broad adoption of robust dialectal English NLP. We introduce a simple yet effective method for task-agnostic dialect adaptation by aligning non-SAE dialects using adapters and composing them with task-specific adapters from SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on 4 dialectal variants of the GLUE benchmark without task-specific supervision.","anthology_url":"https://aclanthology.org/2023.findings-acl.51","authors":["William Held","Caleb Ziems","Diyi Yang"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-1_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4065","is_paper":true,"keywords":["linguistic variation","multilingual pre-training","dialects and language varieties"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.51.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77905/poster_document/1149cb153d8806c4ef2e9d4465af9ca0.pdf","preview_image":"https://assets.underline.io/lecture/77905/poster/7d66d869263b4fdffbcb3f36870a676b.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77905/slideshow/77743afaf28af71fb836e5bf309350ad.pdf","title":"TADA : Task Agnostic Dialect Adapters for English","tldr":"Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE). Prior work addresses this using task specific data or synthetic data augmentation, both of which r...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77905,"underline_url":"https://underline.io/events/395/posters/15200/poster/77905-tada-task-agnostic-dialect-adapters-for-english","video_url":null},{"abstract":"Pragmatic reasoning about another speaker's unspoken intent and state of mind is crucial to efficient and effective human communication. It is virtually omnipresent in conversations between humans, e.g., when someone asks \"do you have a minute?\", instead of interpreting it literally as a query about your schedule, you understand that the speaker might have requests that take time, and respond accordingly. In this paper, we present PragmatiCQA, the first large-scale open-domain question answering (QA) dataset featuring 6873 QA pairs that explores pragmatic reasoning in conversations over a diverse set of topics. We designed innovative crowdsourcing mechanisms for interest-based and task-driven data collection to address the common issue of incentive misalignment between crowdworkers and potential users. To compare computational models' capability at pragmatic reasoning, we also propose several quantitative metrics to evaluate question answering systems on PragmatiCQA. We find that state-of-the-art systems still struggle to perform human-like pragmatic reasoning, and highlight their limitations for future research.","anthology_url":"https://aclanthology.org/2023.findings-acl.385","authors":["Peng Qi","Nina Du","Christopher D. Manning","Jing Huang"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-1_-discourse-and-pragmatics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4069","is_paper":true,"keywords":["dialogue","conversation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.385.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77906/poster_document/0f47e644aba203722517a800dd3e10bd.pdf","preview_image":"https://assets.underline.io/lecture/77906/poster/3553af4b1fafd4179300c5dd564402ae.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77906/slideshow/567d2ad8c83f7f02a1dbb931252abf88.pdf","title":"PragmatiCQA: A Dataset for Pragmatic Question Answering in Conversations","tldr":"Pragmatic reasoning about another speaker's unspoken intent and state of mind is crucial to efficient and effective human communication. It is virtually omnipresent in conversations between humans, e.g., when someone asks \"do you have a minute?\", instead of interpreting it literally as a query about...","track":"Discourse and Pragmatics","underline_id":77906,"underline_url":"https://underline.io/events/395/posters/15200/poster/77906-pragmaticqa-a-dataset-for-pragmatic-question-answering-in-conversations","video_url":null},{"abstract":"State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of various factors towards robustness (or the lack of it), we evaluate prompt-based FSL methods against fully fine-tuned models for aspects such as the use of unlabeled data, multiple prompts, number of few-shot examples, model size and type. Our results on six GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL methods lead to a notable relative drop in task performance (i.e., are less robust) in the face of adversarial perturbations. However, using (i) unlabeled data for prompt-based FSL and (ii) multiple prompts flip the trend \u2013 the few-shot learning approaches demonstrate a lesser drop in task performance than fully fine-tuned models. We further demonstrate that increasing the number of few-shot examples and model size lead to increased adversarial robustness of vanilla FSL methods. Broadly, our work sheds light on the adversarial robustness evaluation of prompt-based FSL methods for NLU tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.138","authors":["Venkata Prabhakara Sarath Nookala","Gaurav Verma","Subhabrata Mukherjee","Srijan Kumar"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4074","is_paper":true,"keywords":["adversarial attacks/examples/training","robustness"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.138.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77907/poster_document/6d9caeb91bcaf15e2560fedc7c66720e.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77907/slideshow/b529308c97a26d4b77a2b5137819cfe4.pdf","title":"Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding","tldr":"State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robu...","track":"Interpretability and Analysis of Models for NLP","underline_id":77907,"underline_url":"https://underline.io/events/395/posters/15279/poster/77907-adversarial-robustness-of-prompt-based-few-shot-learning-for-natural-language-understanding","video_url":null},{"abstract":"Hyperbole and metaphor are common in day-to-day communication (e.g., \"I am in deep trouble\": how does trouble have depth?), which makes their detection important, especially in a conversational AI setting. Existing approaches to automatically detect metaphor and hyperbole have studied these language phenomena independently, but their relationship has hardly, if ever, been explored computationally. In this paper, we propose a multi-task deep learning framework to detect hyperbole and metaphor simultaneously. We hypothesize that metaphors help in hyperbole detection, and vice-versa. To test this hypothesis, we annotate two hyperbole datasets- HYPO and HYPO-L- with metaphor labels. Simultaneously, we annotate two metaphor datasets- TroFi and LCC- with hyperbole labels. Experiments using these datasets give an improvement of the state of the art of hyperbole detection by 12\\%. Additionally,  our multi-task learning (MTL) approach shows an improvement of up to 17\\% over single-task learning (STL) for both hyperbole and metaphor detection, supporting our hypothesis. To the best of our knowledge, ours is the first demonstration of computational leveraging of linguistic intimacy between metaphor and hyperbole, leading to showing the superiority of MTL over STL  for hyperbole and metaphor detection.","anthology_url":"https://aclanthology.org/2023.findings-acl.26","authors":["Naveen Badathala","Abisek Rajakumar Kalarani","Tejpalsingh Siledar","Pushpak Bhattacharyya"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-1_-discourse-and-pragmatics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4086","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.26.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77909/poster_document/25148d68da70fcd10c8c11260286cf08.pdf","preview_image":"https://assets.underline.io/lecture/77909/poster/3855c25299be3aeac96b6604878efce9.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77909/slideshow/736e46ad1e79c5d1ae6a77dd6adbb226.pdf","title":"A Match Made in Heaven: A Multi-task Framework for Hyperbole and Metaphor Detection","tldr":"Hyperbole and metaphor are common in day-to-day communication (e.g., \"I am in deep trouble\": how does trouble have depth?), which makes their detection important, especially in a conversational AI setting. Existing approaches to automatically detect metaphor and hyperbole have studied these language...","track":"Discourse and Pragmatics","underline_id":77909,"underline_url":"https://underline.io/events/395/posters/15200/poster/77909-a-match-made-in-heaven-a-multi-task-framework-for-hyperbole-and-metaphor-detection","video_url":null},{"abstract":"We present Varta, a large-scale multilingual dataset for headline generation in Indic languages. This dataset includes more than 41 million pairs of headlines and articles in 14 different Indic languages (and English), which come from a variety of high-quality news sources. To the best of our knowledge, this is the largest collection of curated news articles for Indic languages currently available. We use the collected data in a series of experiments to answer important questions related to Indic NLP and multilinguality research in general. We show that the dataset is challenging even for state-of-the-art abstractive models and that they perform only slightly better than extractive baselines. Owing to its size, we also show that the dataset can be used to pre-train strong language models that outperform competitive baselines in both NLU and NLG benchmarks.","anthology_url":"https://aclanthology.org/2023.findings-acl.215","authors":["Rahul Aralikatte","Ziling Cheng","Sumanth Doddapaneni","Jackie Chi Kit Cheung"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-1_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4087","is_paper":true,"keywords":["corpus creation","multilingual corpora","nlp datasets","datasets for low resource languages"],"languages":["assamese","bhojpuri","bengali","gujarati","hindi","kannada","malayalam","marathi","nepali","oriya","punjabi","tamil","telugu","urdu"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.215.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77910/poster_document/89c5ed25ef2867aa46ef9997d95db329.pdf","preview_image":"https://assets.underline.io/lecture/77910/poster/d454fb532d3aa90eb973b2d23b08559f.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77910/slideshow/29f0cf72a46e32a495d41d87689671b7.pdf","title":"Varta: A Large-Scale Headline-Generation Dataset for Indic Languages","tldr":"We present Varta, a large-scale multilingual dataset for headline generation in Indic languages. This dataset includes more than 41 million pairs of headlines and articles in 14 different Indic languages (and English), which come from a variety of high-quality news sources. To the best of our knowle...","track":"Resources and Evaluation","underline_id":77910,"underline_url":"https://underline.io/events/395/posters/15200/poster/77910-unsupervised-paraphrasing-of-multiword-expressions","video_url":null},{"abstract":"Pre-training models with large crawled corpora can lead to issues such as toxicity and bias, as well as copyright and privacy concerns. A promising way of alleviating such concerns is to conduct pre-training with synthetic tasks and data, since no real-world information is ingested by the model.\nOur goal in this paper is to understand the factors that contribute to the effectiveness of pre-training models when using synthetic resources, particularly in the context of neural machine translation. We propose several novel approaches to pre-training translation models that involve different levels of lexical and structural knowledge, including: 1) generating obfuscated data from a large parallel corpus 2) concatenating phrase pairs extracted from a small word-aligned corpus, and 3) generating synthetic parallel data without real human language corpora. Our experiments on multiple language pairs reveal that pre-training benefits can be realized even with high levels of obfuscation or purely synthetic parallel data. We hope the findings from our comprehensive empirical analysis will shed light on understanding what matters for NMT pre-training, as well as pave the way for the development of more efficient and less toxic models.","anthology_url":"https://aclanthology.org/2023.findings-acl.512","authors":["Zexue He","Graeme Blackwood","Rameswar Panda","Julian McAuley","Rogerio Feris"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-1_-machine-translation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4124","is_paper":true,"keywords":["biases","efficient mt training","pre-training for mt"],"languages":["german","burmese","indonesian","turkish","tagalog"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.512.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77912/poster_document/3f91311bcbf645546b5e45591500eec5.pdf","preview_image":"https://assets.underline.io/lecture/77912/poster/cf281af4d85454993087c36ec6852d48.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77912/slideshow/012212a7c24b6d58ea0a8075da6375f1.pdf","title":"Synthetic Pre-Training Tasks for Neural Machine Translation","tldr":"Pre-training models with large crawled corpora can lead to issues such as toxicity and bias, as well as copyright and privacy concerns. A promising way of alleviating such concerns is to conduct pre-training with synthetic tasks and data, since no real-world information is ingested by the model.\nOur...","track":"Machine Translation","underline_id":77912,"underline_url":"https://underline.io/events/395/posters/15200/poster/77912-synthetic-pre-training-tasks-for-neural-machine-translation","video_url":null},{"abstract":"Pre-trained encoder-only and sequence-to-sequence (seq2seq) models each have advantages, however training both model types from scratch is computationally expensive. We explore recipes to improve pre-training efficiency by initializing one model from the other. (1) Extracting the encoder from a seq2seq model, we show it under-performs a Masked Language Modeling (MLM) encoder, particularly on sequence labeling tasks. Variations of masking during seq2seq training, reducing the decoder size, and continuing with a small amount of MLM training do not close the gap. (2) Conversely, using an encoder to warm-start seq2seq training, we show that by unfreezing the encoder partway through training, we can match task performance of a from-scratch seq2seq model. Overall, this two-stage approach is an efficient recipe to obtain both a multilingual encoder and a seq2seq model, matching the performance of training each model from scratch while reducing the total compute cost by 27\\%.","anthology_url":"https://aclanthology.org/2023.findings-acl.598","authors":["Saleh Soltan","Andy Rosenbaum","Tobias Falke","Qin Lu","Anna Rumshisky","Wael Hamza"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-7_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4132","is_paper":true,"keywords":["pre-training"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.598.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77914/poster_document/7493cc3a94d2f472dc782909c803e25e.pdf","preview_image":"https://assets.underline.io/lecture/77914/poster/946e09ee7aa5b784ba005bcd567d02be.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77914/slideshow/af93ea414239eee66dec3a7547250b36.pdf","title":"Recipes for Sequential Pre-training of Multilingual Encoder and Seq2Seq Models","tldr":"Pre-trained encoder-only and sequence-to-sequence (seq2seq) models each have advantages, however training both model types from scratch is computationally expensive. We explore recipes to improve pre-training efficiency by initializing one model from the other. (1) Extracting the encoder from a seq2...","track":"Large Language Models","underline_id":77914,"underline_url":"https://underline.io/events/395/posters/15279/poster/77914-recipes-for-sequential-pre-training-of-multilingual-encoder-and-seq2seq-models","video_url":null},{"abstract":"Scholarly text is often laden with jargon, or specialized language that can facilitate efficient in-group communication within fields but hinder understanding for out-groups. In this work, we develop and validate an interpretable approach for measuring scholarly jargon from text. Expanding the scope of prior work which focuses on word types, we use word sense induction to also identify words that are widespread but overloaded with different meanings across fields. We then estimate the prevalence of these discipline-specific words and senses across hundreds of subfields, and show that word senses provide a complementary, yet unique view of jargon alongside word types. We demonstrate the utility of our metrics for science of science and computational sociolinguistics by highlighting two key social implications. First, though most fields reduce their use of jargon when writing for general-purpose venues, and some fields (e.g., biological sciences) do so less than others. Second, the direction of correlation between jargon and citation rates varies among fields, but jargon is nearly always negatively correlated with interdisciplinary impact. Broadly, our findings suggest that though multidisciplinary venues intend to cater to more general audiences, some fields' writing norms may act as barriers rather than bridges, and thus impede the dispersion of scholarly ideas.","anthology_url":"https://aclanthology.org/2023.findings-acl.433","authors":["Li Lucy","Jesse Dodge","David Bamman","Katherine A Keith"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-4_-computational-social-science-and-cultural-analytics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4135","is_paper":true,"keywords":["sociolinguistics","nlp tools for social analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.433.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77915/poster_document/34cbdacb1723659f89ae918651e08356.pdf","preview_image":"https://assets.underline.io/lecture/77915/poster/4777e9ea1ea1440106a9df7b4e9704c7.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77915/slideshow/73ec24b6981540b8d91d8d31afb52c95.pdf","title":"Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications","tldr":"Scholarly text is often laden with jargon, or specialized language that can facilitate efficient in-group communication within fields but hinder understanding for out-groups. In this work, we develop and validate an interpretable approach for measuring scholarly jargon from text. Expanding the scope...","track":"Computational Social Science and Cultural Analytics","underline_id":77915,"underline_url":"https://underline.io/events/395/posters/15240/poster/77915-words-as-gatekeepers-measuring-discipline-specific-terms-and-meanings-in-scholarly-publications","video_url":null},{"abstract":"Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection. However, image-caption pretraining is still a hard problem -- it requires multiple concepts (nouns) from captions to be aligned to several objects in images. To tackle this problem, we go to the roots -- the best learner, children. We take inspiration from cognitive science studies dealing with children's language learning to propose a curriculum learning framework. The learning begins with easy-to-align image caption pairs containing one concept per caption. The difficulty is progressively increased with each new phase by adding one more concept per caption. Correspondingly, the knowledge acquired in each learning phase is utilized in subsequent phases to effectively constrain the learning problem to aligning one new concept-object pair in each phase. We show that this learning strategy improves over vanilla image-caption training in various settings -- pretraining from scratch, using a pretrained image or/and pretrained text encoder, low data regime etc.","anthology_url":"https://aclanthology.org/2023.findings-acl.846","authors":["Hammad Ayyubi","Rahul Lokesh","Alireza Zareian","Bo Wu","Shih-Fu Chang"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)"],"id":"P4140","is_paper":true,"keywords":["cross-modal pretraining","image text matching","cross-modal application"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.846.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77916/poster_document/0893cbe1e15fc143d86465011c2404f6.pdf","preview_image":"https://assets.underline.io/lecture/77916/poster/8ec72251de7eadb0205f723dff529650.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77916/slideshow/06480504108c9c7dc4f270648b136ecb.pptx","title":"Learning from Children: Improving Image-Caption Pretraining via Curriculum","tldr":"Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection. However, image-caption pretraining is still a hard problem -- it requires multiple concepts (nouns) from captions to be aligned to several objects in image...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77916,"underline_url":"https://underline.io/events/395/posters/15240/poster/77916-learning-from-children-improving-image-caption-pretraining-via-curriculum","video_url":null},{"abstract":"Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.","anthology_url":"https://aclanthology.org/2023.findings-acl.551","authors":["Zhangyue Yin","Qiushi Sun","Qipeng Guo","Jiawen Wu","Xipeng Qiu","Xuanjing Huang"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-4_-large-language-models-(virtual-poster)"],"id":"P415","is_paper":true,"keywords":["interpretability/analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.551.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77312/poster_document/a776188c2c6aece225452040c7ba3f0e.pdf","preview_image":"https://assets.underline.io/lecture/77312/poster/4d2ee299245d91390d728ef5c68d7dfd.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77312/slideshow/36713a5c06793660dd50295689cf454e.pdf","title":"Do Large Language Models Know What They Don't Know?","tldr":"Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of informa...","track":"Large Language Models","underline_id":77312,"underline_url":"https://underline.io/events/395/posters/15240/poster/77312-do-large-language-models-know-what-they-don-t-knowquestion","video_url":null},{"abstract":"Deverbal nouns are nominal forms of verbs commonly used in written English texts to describe events or actions, as well as their arguments. However, many NLP systems, and in particular pattern-based ones, neglect to handle such nominalized constructions. The solutions that do exist for handling arguments of nominalized constructions are based on semantic annotation and require semantic ontologies, making their applications restricted to a small set of nouns. We propose to adopt instead a more syntactic approach, which maps the arguments of deverbal nouns to the universal-dependency relations of the corresponding verbal construction. We present an unsupervised mechanism---based on contextualized word representations---which allows to enrich universal-dependency trees with dependency arcs denoting arguments of deverbal nouns, using the same labels as the corresponding verbal cases. By sharing the same label set as in the verbal case, patterns that were developed for verbs can be applied without modification but with high accuracy also to the nominal constructions.","anthology_url":"https://aclanthology.org/2023.findings-acl.184","authors":["Aviv Weinstein","Yoav Goldberg"],"category":"Findings","demo_url":null,"display_track":"Syntax: Tagging, Chunking, and Parsing","event_ids":["session-1_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4152","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.184.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77917/poster_document/a153253ef18379df3460768e459ae22c.pdf","preview_image":"https://assets.underline.io/lecture/77917/poster/18903805910a856e0cb90aff33ab934d.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77917/slideshow/8767205893f75534c8d2c91f31da1ef5.pdf","title":"Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding Verbal Labels","tldr":"Deverbal nouns are nominal forms of verbs commonly used in written English texts to describe events or actions, as well as their arguments. However, many NLP systems, and in particular pattern-based ones, neglect to handle such nominalized constructions. The solutions that do exist for handling argu...","track":"Syntax: Tagging, Chunking, and Parsing","underline_id":77917,"underline_url":"https://underline.io/events/395/posters/15200/poster/77917-unsupervised-mapping-of-arguments-of-deverbal-nouns-to-their-corresponding-verbal-labels","video_url":null},{"abstract":"Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost.  Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute.  In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks.  We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation.  Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedup on the encoder forward pass with nearly identical translation quality when compared with the state-of-the-art approach that employs both left context and memory banks.","anthology_url":"https://aclanthology.org/2023.findings-acl.816","authors":["Matthew Raffel","Lizhong Chen"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-7_-machine-translation-(virtual-poster)"],"id":"P4153","is_paper":true,"keywords":["speech translation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.816.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77918/poster_document/6cfa2467f187900c3d7009502f1c5b4d.pdf","preview_image":"https://assets.underline.io/lecture/77918/poster/d1631fcba8edfc4a7527afe63091763b.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77918/slideshow/e47e7f717772bbc9ad46fb31e6b76df0.pdf","title":"Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation","tldr":"Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art ...","track":"Machine Translation","underline_id":77918,"underline_url":"https://underline.io/events/395/posters/15279/poster/77918-implicit-memory-transformer-for-computationally-efficient-simultaneous-speech-translation","video_url":null},{"abstract":"Local coherence is essential for long-form text generation models. We identify two important aspects of local coherence within the visual storytelling task: (1) the model needs to represent re-occurrences of characters within the image sequence in order to mention them correctly in the story; (2) character representations should enable us to find instances of the same characters and distinguish different characters. In this paper, we propose a loss function inspired by a linguistic theory of coherence for self-supervised learning for image sequence representations. We further propose combining features from an object and a face detector to construct stronger character features. To evaluate input-output relevance that current reference-based metrics don't measure, we propose a character matching metric to check whether the models generate referring expressions correctly for characters in input image sequences. Experiments on a visual story generation dataset show that our proposed features and loss function are effective for generating more coherent and visually grounded stories.","anthology_url":"https://aclanthology.org/2023.findings-acl.603","authors":["Xudong Hong","Vera Demberg","Asad Sayeed","Qiankun Zheng","Bernt Schiele"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4158","is_paper":true,"keywords":["cross-modal content generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.603.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77919/poster_document/40d25476ef88f869d2496209acb2ca3a.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77919/slideshow/3f23a12bd4dd17114da999a3092a0d4d.pdf","title":"Visual Coherence Loss for Coherent and Visually Grounded Story Generation","tldr":"Local coherence is essential for long-form text generation models. We identify two important aspects of local coherence within the visual storytelling task: (1) the model needs to represent re-occurrences of characters within the image sequence in order to mention them correctly in the story; (2) ch...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77919,"underline_url":"https://underline.io/events/395/posters/15200/poster/77919-visual-coherence-loss-for-coherent-and-visually-grounded-story-generation","video_url":null},{"abstract":"Knowledge graph embeddings (KGE) have been extensively studied to embed large-scale relational data for many real-world applications. Existing methods have long ignored the fact many KGs contain two fundamentally different views: high-level ontology-view concepts and fine-grained instance-view entities. They usually embed all nodes as vectors in one latent space. However, a single geometric representation fails to capture the structural differences between two views and lacks probabilistic semantics towards concepts' granularity. We propose Concept2Box, a novel approach that jointly embeds the two views of a KG using dual geometric representations. We model concepts with box embeddings, which learn the hierarchy structure and complex relations such as overlap and disjoint among them. Box volumes can be interpreted as concepts' granularity. Different from concepts, we model entities as vectors. To bridge the gap between concept box embeddings and entity vector embeddings, we propose a novel vector-to-box distance metric and learn both embeddings jointly. Experiments on both the public DBpedia KG and a newly-created industrial KG showed the effectiveness of Concept2Box.","anthology_url":"https://aclanthology.org/2023.findings-acl.642","authors":["Zijie Huang","Daheng Wang","Binxuan Huang","Chenwei Zhang","Jingbo Shang","Yan Liang","Zhengyang Wang","Xian Li","Christos Faloutsos","Yizhou Sun","Wei Wang"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-1_-information-extraction-(virtual-poster)"],"id":"P4210","is_paper":true,"keywords":["knowledge base construction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.642.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77923/poster_document/d45cc113b0a1225b857cefa55b84e5ec.pdf","preview_image":"https://assets.underline.io/lecture/77923/poster/54b7c3f70ff48a47d6e6a31b3e384bf3.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77923/slideshow/e86a283229abf98a766359d4731987fe.pdf","title":"Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs","tldr":"Knowledge graph embeddings (KGE) have been extensively studied to embed large-scale relational data for many real-world applications. Existing methods have long ignored the fact many KGs contain two fundamentally different views: high-level ontology-view concepts and fine-grained instance-view entit...","track":"Information Extraction","underline_id":77923,"underline_url":"https://underline.io/events/395/posters/15200/poster/77923-concept2box-joint-geometric-embeddings-for-learning-two-view-knowledge-graphs","video_url":null},{"abstract":"We  propose a simple approach for the abstractive summarization of long legal opinions that takes into account the argument structure of the document. Legal opinions often contain  complex and nuanced argumentation, making it challenging to generate a concise summary that accurately captures the main points of the legal opinion. Our approach involves using argument role information to generate multiple candidate summaries, then reranking these candidates based on alignment with the document's argument structure. We demonstrate the effectiveness of our approach on a dataset of long legal opinions and show that it outperforms several  strong baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.481","authors":["Mohamed Elaraby","Yang Zhong","Diane Litman"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-4_-summarization-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4225","is_paper":true,"keywords":["abstractive summarisation","long-form summarization"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.481.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77924/poster_document/da74c896490a713271eaa81f158badfa.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77924/slideshow/53f188ad701ecdace69d4bd9a79b1e71.pptx","title":"Towards Argument-Aware Abstractive Summarization of Long Legal Opinions with Summary Reranking","tldr":"We  propose a simple approach for the abstractive summarization of long legal opinions that takes into account the argument structure of the document. Legal opinions often contain  complex and nuanced argumentation, making it challenging to generate a concise summary that accurately captures the mai...","track":"Summarization","underline_id":77924,"underline_url":"https://underline.io/events/395/posters/15240/poster/77924-towards-argument-aware-abstractive-summarization-of-long-legal-opinions-with-summary-reranking","video_url":null},{"abstract":"Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained language models and innovative techniques to prompt them. However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications. In this paper, we consolidate previous work on categorizing and representing math story problems and develop MathWorld, which is a graph-based semantic formalism specific for the domain of math story problems. With MathWorld, we can assign world models to math story problems which represent the situations and actions introduced in the text and their mathematical relationships. We combine math story problems from several existing datasets and annotate a corpus of 1,019 problems and 3,204 logical forms with MathWorld. Using this data, we demonstrate the following use cases of MathWorld: (1) prompting language models with synthetically generated question-answer pairs to probe their reasoning and world modeling abilities, and (2) generating new problems by using the world models as a design space.","anthology_url":"https://aclanthology.org/2023.findings-acl.579","authors":["Andreas Opedal","Niklas Stoehr","Abulhair Saparov","Mrinmaya Sachan"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4228","is_paper":true,"keywords":["semantic parsing","interpretability","reasoning","math qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.579.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77925/poster_document/b10858317a459bb63a1dc177e3f39695.pdf","preview_image":"https://assets.underline.io/lecture/77925/poster/f8e0f46015be2e991b9812f2f29255c9.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77925/slideshow/682980f2442f9c5042c8ba97910f3843.pdf","title":"World Models for Math Story Problems","tldr":"Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained lang...","track":"Question Answering","underline_id":77925,"underline_url":"https://underline.io/events/395/posters/15240/poster/77925-world-models-for-math-story-problems","video_url":null},{"abstract":"Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning-based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.","anthology_url":"https://aclanthology.org/2023.findings-acl.537","authors":["Sameer Jain","Vaishakh Keshava","Swarnashree Mysore Sathyendra","Patrick Fernandes","Pengfei Liu","Graham Neubig","Chunting Zhou"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-4_-summarization-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4230","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.537.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77926/poster_document/99fd816b0c4e9d16d27905ebcb702b19.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77926/slideshow/5646aa26ef1f509500c27996473e1c7a.pdf","title":"Multi-Dimensional Evaluation of Text Summarization with In-Context Learning","tldr":"Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or syntheticall...","track":"Summarization","underline_id":77926,"underline_url":"https://underline.io/events/395/posters/15240/poster/77926-multi-dimensional-evaluation-of-text-summarization-with-in-context-learning","video_url":null},{"abstract":"Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5\\%-6\\% of total parameters.","anthology_url":"https://aclanthology.org/2023.findings-acl.150","authors":["Runcheng Liu","Ahmad Rashid","Ivan Kobyzev","Mehdi Rezagholizadeh","Pascal Poupart"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-1_-large-language-models-(virtual-poster)"],"id":"P4242","is_paper":true,"keywords":["prompting"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.150.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77928/poster_document/a6c5f1505eed7006088d4e4e88a71ee3.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Attribute Controlled Dialogue Prompting","tldr":"Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary great...","track":"Large Language Models","underline_id":77928,"underline_url":"https://underline.io/events/395/posters/15200/poster/77928-attribute-controlled-dialogue-prompting","video_url":null},{"abstract":"Pragmatic reference enables efficient interpersonal communication. Prior work uses simple reference games to test models of pragmatic reasoning, often with unidentified speakers and listeners. In practice, however, speakers' sociocultural background shapes their pragmatic assumptions. For example, readers of this paper assume NLP refers to Natural Language Processing, and not \"Neuro-linguistic Programming.\" This work introduces the Cultural Codes dataset, which operationalizes sociocultural pragmatic inference in a simple word reference game. \n\nCultural Codes is based on the multi-turn collaborative two-player game, Codenames Duet. Our dataset consists of 794 games with 7,703 turns, distributed across 153 unique players. Alongside gameplay, we collect information about players' personalities, values, and demographics. Utilizing theories of communication and pragmatics, we predict each player's actions via joint modeling of their sociocultural priors and the game context. Our experiments show that accounting for background characteristics significantly improves model performance for tasks related to both clue-giving and guessing, indicating that sociocultural priors play a vital role in gameplay decisions.","anthology_url":"https://aclanthology.org/2023.findings-acl.410","authors":["Omar Shaikh","Caleb Ziems","William Held","Aryan J Pariani","Fred Morstatter","Diyi Yang"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4252","is_paper":true,"keywords":["human behavior analysis","sociolinguistics"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.410.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77930/poster_document/ef0a1185e85f3368683d0c13dfe4dd38.pdf","preview_image":"https://assets.underline.io/lecture/77930/poster/123e4db5618faeb523924a606cc361e6.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77930/slideshow/aeefe11b9908c170bba4e398eb763b61.pdf","title":"Modeling Cross-Cultural Pragmatic Inference with Codenames Duet","tldr":"Pragmatic reference enables efficient interpersonal communication. Prior work uses simple reference games to test models of pragmatic reasoning, often with unidentified speakers and listeners. In practice, however, speakers' sociocultural background shapes their pragmatic assumptions. For example, r...","track":"Computational Social Science and Cultural Analytics","underline_id":77930,"underline_url":"https://underline.io/events/395/posters/15200/poster/77930-modeling-cross-cultural-pragmatic-inference-with-codenames-duet","video_url":null},{"abstract":"The ability to conduct retrospective analyses of attacks on human rights defenders over time and by location is important for humanitarian organizations to better understand historical or ongoing human rights violations and thus better manage the global impact of such events. We hypothesize that NLP can support such efforts by quickly processing large collections of news articles to detect and summarize the characteristics of attacks on human rights defenders. To that end, we propose a new dataset for detecting Attacks on Human Rights Defenders (HRDsAttack) consisting of crowdsourced annotations on 500 online news articles. The annotations include fine-grained information about the type and location of the attacks, as well as information about the victim(s). We demonstrate the usefulness of the dataset by using it to train and evaluate baseline models on several sub-tasks to predict the annotated characteristics.","anthology_url":"https://aclanthology.org/2023.findings-acl.443","authors":["Shihao Ran","Di Lu","Aoife Cahill","Joel Tetreault","Alejandro Jaimes"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)"],"id":"P4253","is_paper":true,"keywords":["nlp datasets"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.443.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77931/poster_document/1c2cedffcdb8690bd8af94bae91f911f.pdf","preview_image":"https://assets.underline.io/lecture/77931/poster/0225477eff65ad7b1106a325225ee829.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77931/slideshow/5765fa9a6e4d24587384af31de73e1dd.pdf","title":"A New Task and Dataset on Detecting Attacks on Human Rights Defenders","tldr":"The ability to conduct retrospective analyses of attacks on human rights defenders over time and by location is important for humanitarian organizations to better understand historical or ongoing human rights violations and thus better manage the global impact of such events. We hypothesize that NLP...","track":"Resources and Evaluation","underline_id":77931,"underline_url":"https://underline.io/events/395/posters/15279/poster/77931-do-conll-2003-named-entity-taggers-still-work-well-in-2023question","video_url":null},{"abstract":"The opaqueness of deep NLP models has motivated efforts to explain how deep models predict. Recently, work has introduced hierarchical attribution explanations, which calculate attribution scores for compositional text hierarchically to capture compositional semantics. Existing work on hierarchical attributions tends to limit the text groups to a continuous text span, which we call the connecting rule. While easy for humans to read, limiting the attribution unit to a continuous span might lose important long-distance feature interactions for reflecting model predictions. In this work, we introduce a novel strategy for capturing feature interactions and employ it to build hierarchical explanations without the connecting rule. The proposed method can convert ubiquitous non-hierarchical explanations (e.g., LIME) into their corresponding hierarchical versions. Experimental results show the effectiveness of our approach in building high-quality hierarchical explanations.","anthology_url":"https://aclanthology.org/2023.findings-acl.798","authors":["Yiming Ju","Yuanzhe Zhang","Kang Liu","Jun Zhao"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P426","is_paper":true,"keywords":["feature attribution"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.798.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77314/poster_document/0789d1da2fc06510eb4f19d15e4cdaec.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"A Hierarchical Explanation Generation Method Based on Feature Interaction Detection","tldr":"The opaqueness of deep NLP models has motivated efforts to explain how deep models predict. Recently, work has introduced hierarchical attribution explanations, which calculate attribution scores for compositional text hierarchically to capture compositional semantics. Existing work on hierarchical ...","track":"Interpretability and Analysis of Models for NLP","underline_id":77314,"underline_url":"https://underline.io/events/395/posters/15279/poster/77314-a-hierarchical-explanation-generation-method-based-on-feature-interaction-detection","video_url":null},{"abstract":"Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, which further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO (\u03c1) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG (Moon et al., 2019) show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54\\% in FeQA (Durmus et al., 2020)).","anthology_url":"https://aclanthology.org/2023.findings-acl.275","authors":["Ziwei Ji","Zihan Liu","Nayeon Lee","Tiezheng Yu","Bryan Wilie","Min Zeng","Pascale Fung"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-7_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P427","is_paper":true,"keywords":["factuality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.275.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77315/poster_document/ada0048541dcb6b58e197f3e1becde91.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77315/slideshow/eacf42360c019a7fecba6ecb394c733f.pdf","title":"RHO (\u03c1): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding","tldr":"Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between ex...","track":"Dialogue and Interactive Systems","underline_id":77315,"underline_url":"https://underline.io/events/395/posters/15279/poster/77315-rho-reducing-hallucination-in-open-domain-dialogues-with-knowledge-grounding","video_url":null},{"abstract":"Longitudinal user modeling can provide a strong signal for various downstream tasks. Despite the rapid progress in representation learning, dynamic aspects of  modelling individuals' language have only been sparsely addressed. We present a novel extension of neural sequential models using the notion of path signatures from rough path theory, which constitute graduated summaries of continuous paths and have the ability to capture non-linearities in trajectories. By combining path signatures of users' history with contextual neural representations and recursive neural networks we can produce compact time-sensitive user representations. Given the magnitude of mental health conditions with symptoms manifesting in language, we show the applicability of our approach on the task of identifying changes in individuals' mood by analysing their online textual content. By directly integrating signature transforms of users' history in the model architecture we jointly address the two most important aspects of the task, namely sequentiality and temporality. \nOur approach achieves state-of-the-art performance on macro-average F1 score on the two available datasets for the task, outperforming or performing on-par with state-of-the-art models utilising only historical posts and even outperforming prior models which also have access to future posts of users.","anthology_url":"https://aclanthology.org/2023.findings-acl.310","authors":["Talia Tseriotou","Adam Tsakalidis","Peter Foster","Terence J Lyons","Maria Liakata"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4277","is_paper":true,"keywords":["healthcare applications, clincial nlp"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.310.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77933/poster_document/3909a46dab1de4764b079bca1836bc80.pdf","preview_image":"https://assets.underline.io/lecture/77933/poster/d8b00d1ae1c97dfbf5aad74d3657cae4.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77933/slideshow/a88875d4ddde0f5ec01ca47dfd270f34.pdf","title":"Sequential Path Signature Networks for Personalised Longitudinal Language Modeling","tldr":"Longitudinal user modeling can provide a strong signal for various downstream tasks. Despite the rapid progress in representation learning, dynamic aspects of  modelling individuals' language have only been sparsely addressed. We present a novel extension of neural sequential models using the notion...","track":"NLP Applications","underline_id":77933,"underline_url":"https://underline.io/events/395/posters/15240/poster/77933-sequential-path-signature-networks-for-personalised-longitudinal-language-modeling","video_url":null},{"abstract":"We demonstrate that coreference resolution in procedural texts is significantly improved when performing transformation-based entity linking prior to coreference relation identification. When events in the text introduce changes to the state of participating entities, it is often impossible to accurately link entities in anaphoric and coreference relations without an understanding of the transformations those entities undergo. We show how adding event semantics helps to better model entity coreference. We argue that all transformation predicates, not just creation verbs, introduce a new entity into the discourse, as a kind of generalized Result Role, which is typically not textually mentioned. This allows us to model procedural texts as process graphs and to compute the coreference type for any two entities in the recipe. We present our annotation methodology and the corpus generated as well as describe experiments on coreference resolution of entity mentions under a process-oriented model of events.","anthology_url":"https://aclanthology.org/2023.findings-acl.788","authors":["Kyeongmin Rim","Jingxuan Tu","Bingyang Ye","Marc Verhagen","Eben Holderness","James Pustejovsky"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-1_-discourse-and-pragmatics-(virtual-poster)"],"id":"P4295","is_paper":true,"keywords":["anaphora resolution","coreference resolution","bridging resolution"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.788.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77934/poster_document/3a46f8d2367709e6e540a617e055b831.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"The Coreference under Transformation Labeling Dataset: Entity Tracking in Procedural Texts Using Event Models","tldr":"We demonstrate that coreference resolution in procedural texts is significantly improved when performing transformation-based entity linking prior to coreference relation identification. When events in the text introduce changes to the state of participating entities, it is often impossible to accur...","track":"Discourse and Pragmatics","underline_id":77934,"underline_url":"https://underline.io/events/395/posters/15200/poster/77934-the-coreference-under-transformation-labeling-dataset-entity-tracking-in-procedural-texts-using-event-models","video_url":null},{"abstract":"Changing speaker names consistently throughout a dialogue should not affect its meaning and corresponding outputs for text generation from dialogues. However, pre-trained language models, serving as the backbone for dialogue-processing tasks, have shown to be sensitive to nuances. This may result in unfairness in real-world applications. No comprehensive analysis of this problem has been done in the past. In this work, we propose to quantitatively measure a model's sensitivity on speaker names, and comprehensively evaluate a number of known methods for reducing speaker name sensitivity, including a novel approach of our own. Extensive experiments on multiple datasets provide a benchmark for this problem and show the favorable performance of our approach in sensitivity reduction and quality of generation.","anthology_url":"https://aclanthology.org/2023.findings-acl.129","authors":["Qi Jia","Haifeng Tang","Kenny Q. Zhu"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-4_-generation-(virtual-poster)"],"id":"P432","is_paper":true,"keywords":["text-to-text generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.129.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77316/poster_document/50f43e2e8b10b0d53d6b02cb69f4bf93.pdf","preview_image":"https://assets.underline.io/lecture/77316/poster/1f254446ad61292bb4f20aaf13e55bcd.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Reducing Sensitivity on Speaker Names for Text Generation from Dialogues","tldr":"Changing speaker names consistently throughout a dialogue should not affect its meaning and corresponding outputs for text generation from dialogues. However, pre-trained language models, serving as the backbone for dialogue-processing tasks, have shown to be sensitive to nuances. This may result in...","track":"Generation","underline_id":77316,"underline_url":"https://underline.io/events/395/posters/15240/poster/77316-reducing-sensitivity-on-speaker-names-for-text-generation-from-dialogues","video_url":null},{"abstract":"We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments.\nNusaCrowd's data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.","anthology_url":"https://aclanthology.org/2023.findings-acl.868","authors":["Samuel Cahyawijaya","Holy Lovenia","Alham Fikri Aji","Genta Indra Winata","Bryan Wilie","Fajri Koto","Rahmad Mahendra","Christian Wibisono","Ade Romadhony","Karissa Vincentio","Jennifer Santoso","David Moeljadi","Cahya Wirawan","Frederikus Hudi","Muhammad Satrio Wicaksono","Ivan Halim Parmonangan","Ika Alfina","Ilham Firdausi Putra","Samsul Rahmadani","Yulianti Oenang","Ali Akbar Septiandri","James Jaya","Kaustubh Dhole","Arie Suryani","Rifki Afina Putri","Dan Su","Keith David Stevens","Made Nindyatama Nityasya","Muhammad Farid Adilazuarda","Ryan Ignatius Hadiwijaya","Ryandito Diandaru","Tiezheng Yu","Vito Ghifari","Wenliang Dai","Yan Xu","Dyah Inastra Damapuspita","Haryo Akbarianto Wibowo","Cuk Tho","Ichwanul Muslim Karo Karo","Tirana Noor Fatyanosa","Ziwei Ji","Graham Neubig","Timothy Baldwin","Sebastian Ruder","Pascale Fung","Herry Sujaini","Sakriani Sakti","Ayu Purwarianti"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P433","is_paper":true,"keywords":["benchmarking","language resources","multilingual corpora","nlp datasets","automatic evaluation of datasets","evaluation","reproducibility"],"languages":["indonesian","banjar","batak toba","madura","ngaju","lampung nyo","batak karo","tok pisin","tetun dili","dayak","khek","tiociu","minangkabau","acehnese","buginese","sundanese","javenese","balinese","batak"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.868.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77317/poster_document/91f20e755c349c77a9d11fcd9f4ec4e3.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77317/slideshow/f6996b9a5acc82f230226345fb1fc2bd.pdf","title":"NusaCrowd: Open Source Initiative for Indonesian NLP Resources","tldr":"We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the dataset...","track":"Resources and Evaluation","underline_id":77317,"underline_url":"https://underline.io/events/395/posters/15279/poster/77317-nusacrowd-open-source-initiative-for-indonesian-nlp-resources","video_url":null},{"abstract":"Due to the crucial role pretrained language models play in modern NLP, several benchmarks have been proposed to evaluate their performance. In spite of these efforts, no public benchmark of diverse nature currently exists for evaluating Arabic NLU. This makes it challenging to measure progress for both Arabic and multilingual language models. This challenge is compounded by the fact that any benchmark targeting Arabic needs to take into account the fact that Arabic is not a single language but rather a collection of languages and language varieties. In this work, we introduce a publicly available benchmark for Arabic language understanding evaluation dubbed ORCA. It is carefully constructed to cover diverse Arabic varieties and a wide range of challenging Arabic understanding tasks exploiting 60 different datasets (across seven NLU task clusters). To measure current progress in Arabic NLU, we use ORCA to offer a comprehensive comparison between 18 multilingual and Arabic language models. We also provide a public leaderboard with a unified single-number evaluation metric (ORCA score) to facilitate future research.","anthology_url":"https://aclanthology.org/2023.findings-acl.609","authors":["AbdelRahim Elmadany","ElMoatez Billah Nagoudi","Muhammad Abdul-Mageed"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-4_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4336","is_paper":true,"keywords":["benchmarking"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.609.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77938/poster_document/167b66c3cc31bb2c653605c751c24c55.pdf","preview_image":"https://assets.underline.io/lecture/77938/poster/3377f9783d3fb45b9267bad1f46c7b37.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77938/slideshow/1732a3f9c849b1bd92c9fa51c6768dd2.pdf","title":"ORCA: A Challenging Benchmark for Arabic Language Understanding","tldr":"Due to the crucial role pretrained language models play in modern NLP, several benchmarks have been proposed to evaluate their performance. In spite of these efforts, no public benchmark of diverse nature currently exists for evaluating Arabic NLU. This makes it challenging to measure progress for b...","track":"Resources and Evaluation","underline_id":77938,"underline_url":"https://underline.io/events/395/posters/15240/poster/77938-orca-a-challenging-benchmark-for-arabic-language-understanding","video_url":null},{"abstract":"We investigate semi-structured document classification in a zero-shot setting. Classification of semi-structured documents is more challenging than that of standard unstructured documents, as positional, layout, and style information play a vital role in interpreting such documents. The standard classification setting where categories are fixed during both training and testing falls short in dynamic environments where new classification categories could potentially emerge. We focus exclusively on the zero-shot learning setting where inference is done on new unseen classes. To address this task, we propose a matching-based approach that relies on a pairwise contrastive objective for both pretraining and fine-tuning. \nOur results show a significant boost in Macro F1 from the proposed pretraining step and comparable performance of the contrastive fine-tuning to a standard prediction objective in both supervised and unsupervised zero-shot settings.","anthology_url":"https://aclanthology.org/2023.findings-acl.473","authors":["Muhammad Khalifa","Yogarshi Vyas","Shuai Wang","Graham Horwood","Sunil Mallya","Miguel Ballesteros"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4341","is_paper":true,"keywords":["multimodal applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.473.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77939/poster/90fd990cef191251e0be2189054d6e06.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77939/slideshow/a71f2e0c1c9645fe73619b1041ebc99d.pdf","title":"Contrastive Training Improves Zero-Shot Classification of Semi-structured Documents","tldr":"We investigate semi-structured document classification in a zero-shot setting. Classification of semi-structured documents is more challenging than that of standard unstructured documents, as positional, layout, and style information play a vital role in interpreting such documents. The standard cla...","track":"NLP Applications","underline_id":77939,"underline_url":"https://underline.io/events/395/posters/15279/poster/77939-towards-speech-dialogue-translation-mediating-speakers-of-different-languages","video_url":null},{"abstract":"We propose CHRT (Control Hidden\nRepresentation Transformation) \u2013 a con-\ntrolled language generation framework that\nsteers large language models to generate\ntext pertaining to certain attributes (such as\ntoxicity). CHRT gains attribute control by\nmodifying the hidden representation of the\nbase model through learned transformations.\nWe employ a contrastive-learning framework\nto learn these transformations that can be\ncombined to gain multi-attribute control. The\neffectiveness of CHRT is experimentally\nshown by comparing it with seven baselines\nover three attributes. CHRT outperforms all the\nbaselines in the task of detoxification, positive\nsentiment steering, and text simplification\nwhile minimizing the loss in linguistic qualities.\nFurther, our approach has the lowest inference\nlatency of only 0.01 seconds more than the\nbase model, making it the most suitable for\nhigh-performance production environments.\nWe open-source our code and release two novel\ndatasets to further propel controlled language\ngeneration research","anthology_url":"https://aclanthology.org/2023.findings-acl.602","authors":["Vaibhav Kumar","Hana Koorehdavoudi","Masud Moshtaghi","Amita Misra","Ankit R Chadha","Emilio Ferrara"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-1_-generation-(virtual-poster)"],"id":"P4348","is_paper":true,"keywords":["domain adaptation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.602.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77940/poster_document/2e114b12dbb9413ef9576d2c95b6bf3d.pdf","preview_image":"https://assets.underline.io/lecture/77940/poster/840c41851bfa8365acf48815f386116a.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77940/slideshow/80b9de072946cd4f917eb753110b44fb.pdf","title":"Controlled Text Generation with Hidden Representation Transformations","tldr":"We propose CHRT (Control Hidden\nRepresentation Transformation) \u2013 a con-\ntrolled language generation framework that\nsteers large language models to generate\ntext pertaining to certain attributes (such as\ntoxicity). CHRT gains attribute control by\nmodifying the hidden representation of the\nbase model ...","track":"Generation","underline_id":77940,"underline_url":"https://underline.io/events/395/posters/15200/poster/77940-a-method-for-studying-semantic-construal-in-grammatical-constructions-with-interpretable-contextual-embedding-spaces","video_url":null},{"abstract":"Temporal knowledge graph (TKG) completion models typically rely on having access to the entire graph during training. However, in real-world scenarios, TKG data is often received incrementally as events unfold, leading to a dynamic non-stationary data distribution over time. While one could incorporate fine-tuning to existing methods to allow them to adapt to evolving TKG data, this can lead to forgetting previously learned patterns. Alternatively, retraining the model with the entire updated TKG can mitigate forgetting but is computationally burdensome. To address these challenges, we propose a general continual training framework that is applicable to any TKG completion method, and leverages two key ideas: (i) a temporal regularization that encourages repurposing of less important model parameters for learning new knowledge, and (ii) a clustering-based experience replay that reinforces the past knowledge by selectively preserving only a small portion of the past data. Our experimental results on widely used event-centric TKG datasets demonstrate the effectiveness of our proposed continual training framework in adapting to new events while reducing catastrophic forgetting. Further, we perform ablation studies to show the effectiveness of each component of our proposed framework. Finally, we investigate the relation between the memory dedicated to experience replay and the benefit gained from our clustering-based sampling strategy.","anthology_url":"https://aclanthology.org/2023.findings-acl.490","authors":["Mehrnoosh Mirtaheri","Mohammad Rostami","Aram Galstyan"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-4_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4371","is_paper":true,"keywords":["graph-based methods","continual learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.490.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77941/poster_document/581f7b59438ad64a870c3fb9c973c830.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77941/slideshow/0fe8f28396e1bad862b1196a11a1051b.pptx","title":"History repeats: Overcoming catastrophic forgetting for event-centric temporal knowledge graph completion","tldr":"Temporal knowledge graph (TKG) completion models typically rely on having access to the entire graph during training. However, in real-world scenarios, TKG data is often received incrementally as events unfold, leading to a dynamic non-stationary data distribution over time. While one could incorpor...","track":"Machine Learning for NLP","underline_id":77941,"underline_url":"https://underline.io/events/395/posters/15240/poster/77941-history-repeats-overcoming-catastrophic-forgetting-for-event-centric-temporal-knowledge-graph-completion","video_url":null},{"abstract":"Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute <0.1\\% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters. In this work, we introduce Residual Prompt Tuning - a simple and efficient method that significantly improves the performance and stability of prompt tuning. We propose to reparameterize soft prompt embeddings using a shallow network with a residual connection. Our experiments show that Residual Prompt Tuning significantly outperforms prompt tuning across T5-Large, T5-Base and BERT-Base models. Notably, our method reaches +7 points improvement over prompt tuning on SuperGLUE benchmark with T5-Base model and allows to reduce the prompt length by 10 times without hurting performance. In addition, we show that our approach is robust to the choice of learning rate and prompt initialization, and is effective in few-shot settings.","anthology_url":"https://aclanthology.org/2023.findings-acl.421","authors":["Anastasiia Razdaibiedina","Yuning Mao","Madian Khabsa","Mike Lewis","Rui Hou","Jimmy Ba","Amjad Almahairi"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-7_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4384","is_paper":true,"keywords":["prompting"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.421.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77943/poster_document/59c7152ee8edf8326a038dae74b90df8.pdf","preview_image":"https://assets.underline.io/lecture/77943/poster/3df47c5f4a9c43409b2eb934222c22c8.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77943/slideshow/cb114aa57406331611e87bb397dfda7e.pdf","title":"Residual Prompt Tuning: improving prompt tuning with residual reparameterization","tldr":"Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute <0.1\\% of total parameters), it typically performs worse than other efficient tuning methods and is quit...","track":"Large Language Models","underline_id":77943,"underline_url":"https://underline.io/events/395/posters/15279/poster/77943-residual-prompt-tuning-improving-prompt-tuning-with-residual-reparameterization","video_url":null},{"abstract":"Syntactically controlled paraphrase generation requires language models to generate paraphrases for sentences according to specific syntactic structures. Existing fine-tuning methods on this task is costly, as all parameters of the model need to be updated during the training process. Inspired by recent studies on parameter-efficient learning, we propose Parse-Instructed Prefix (PIP), a novel adaptation of prefix-tuning to tune large pre-trained language models on syntactically controlled paraphrase generation task in a low-data setting with significantly less training cost. We introduce two methods to instruct a model's encoder prefix to capture syntax-related knowledge: direct initiation (PIP-Direct) and indirect optimization (PIP-Indirect). Comparing to traditional fine-tuning methods for this task, PIP is a compute-efficient alternative with 10 times less learnable parameters. Comparing to existing prefix-tuning methods, PIP excels at capturing syntax control information, achieving significantly higher performance at the same level of learnable parameter count.","anthology_url":"https://aclanthology.org/2023.findings-acl.659","authors":["Yixin Wan","Kuan-Hao Huang","Kai-Wei Chang"],"category":"Findings","demo_url":null,"display_track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","event_ids":["session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)"],"id":"P4387","is_paper":true,"keywords":["paraphrase generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.659.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77944/poster_document/dbd48c6189ad33d3bb7d5711c5bae9aa.pdf","preview_image":"https://assets.underline.io/lecture/77944/poster/1c616c7a1fe8f5a8f4a97929ca9613e7.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77944/slideshow/dc548467e317b4131f5dc58964f4afcc.pdf","title":"PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation","tldr":"Syntactically controlled paraphrase generation requires language models to generate paraphrases for sentences according to specific syntactic structures. Existing fine-tuning methods on this task is costly, as all parameters of the model need to be updated during the training process. Inspired by re...","track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","underline_id":77944,"underline_url":"https://underline.io/events/395/posters/15279/poster/77944-teast-temporal-knowledge-graph-embedding-via-archimedean-spiral-timeline","video_url":null},{"abstract":"Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a set of massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F\\_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research.{Anonymous link}","anthology_url":"https://aclanthology.org/2023.findings-acl.97","authors":["Ife Adebara","AbdelRahim Elmadany","Muhammad Abdul-Mageed","Alcides Alcoba Inciarte"],"category":"Findings","demo_url":null,"display_track":"Linguistic Diversity","event_ids":["session-7_-linguistic-diversity-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4395","is_paper":true,"keywords":["less-resourced languages","endangered languages","indigenous languages","minoritized languages","language documentation","resources for less-resourced languages","software and tools"],"languages":["517 african languages","language varieties"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.97.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77945/poster_document/7775f524d3cc25ea6ac1cdb984bca064.pdf","preview_image":"https://assets.underline.io/lecture/77945/poster/bd29b4e37bec79237cdec48378b854d9.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77945/slideshow/7db1ff19aedd5a65b7b612d70808f952.pptx","title":"SERENGETI: Massively Multilingual Language Models for Africa","tldr":"Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this ...","track":"Linguistic Diversity","underline_id":77945,"underline_url":"https://underline.io/events/395/posters/15279/poster/77945-serengeti-massively-multilingual-language-models-for-africa","video_url":null},{"abstract":"A promising approach to estimate the causal effects of peer review policies is to analyze data from publication venues that shift policies from single-blind to double-blind from one year to the next. However, in these settings the content of the manuscript is a confounding variable---each year has a different distribution of scientific content which may naturally affect the distribution of reviewer scores. To address this textual confounding, we extend variable ratio nearest neighbor matching to incorporate text embeddings. We compare this matching method to a widely-used causal method of stratified propensity score matching and a baseline of randomly selected matches. For our case study of the ICLR conference shifting from single- to double-blind review from 2017 to 2018, we find human judges prefer manuscript matches from our method in 70\\% of cases. While the unadjusted estimate of the average causal effect of reviewers' scores is -0.25, our method shifts the estimate to -0.17, a slightly smaller difference between the outcomes of single- and double-blind policies. We hope this case study enables exploration of additional text-based causal estimation methods and domains in the future.","anthology_url":"https://aclanthology.org/2023.findings-acl.83","authors":["Raymond Zhang","Neha Nayak Kennard","Daniel S Smith","Daniel A McFarland","Andrew McCallum","Katherine A Keith"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4422","is_paper":true,"keywords":["nlp tools for social analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.83.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77948/poster_document/b8f164cc2b9b999d7e2fed22a3f4dd70.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77948/slideshow/caa6341198ae0ca924d1031f5157823c.pdf","title":"Causal Matching with Text Embeddings: A Case Study in Estimating the Causal Effects of Peer Review Policies","tldr":"A promising approach to estimate the causal effects of peer review policies is to analyze data from publication venues that shift policies from single-blind to double-blind from one year to the next. However, in these settings the content of the manuscript is a confounding variable---each year has a...","track":"Computational Social Science and Cultural Analytics","underline_id":77948,"underline_url":"https://underline.io/events/395/posters/15279/poster/77948-causal-matching-with-text-embeddings-a-case-study-in-estimating-the-causal-effects-of-peer-review-policies","video_url":null},{"abstract":"The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs  produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks \nand analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPT's performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications.","anthology_url":"https://aclanthology.org/2023.findings-acl.29","authors":["Md Tahmid Rahman Laskar","M Saiful Bari","Mizanur Rahman","Md Amran Hossen Bhuiyan","Shafiq Joty","Jimmy Xiangji Huang"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-1_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4423","is_paper":true,"keywords":["evaluation","ai hype & expectations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.29.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77949/poster_document/dd72ca8d69cd84964033329ed9855e41.pdf","preview_image":"https://assets.underline.io/lecture/77949/poster/f16a490449d992c950fd914aa28f68df.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77949/slideshow/7a096aa834a01ec36ca7caff3eb79e7b.pdf","title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets","tldr":"The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs  produced by this model against the ground truth. In...","track":"Theme: Reality Check","underline_id":77949,"underline_url":"https://underline.io/events/395/posters/15200/poster/77949-fedlegal-the-first-real-world-federated-learning-benchmark-for-legal-nlp","video_url":null},{"abstract":"Large language models are known to produce output which  sounds fluent and convincing, but is also often wrong, e.g. ``unfaithful\" with respect to a rationale as retrieved from a knowledge base. In this paper, we \nshow that task-based systems which exhibit certain advanced linguistic dialog behaviors, such as lexical  alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred. \nWe use open-domain question answering systems as our test-bed for task based dialog generation and compare several open- and closed-book models. Our results highlight the danger of systems that appear to be trustworthy by parroting user input while providing an unfaithful response.","anthology_url":"https://aclanthology.org/2023.findings-acl.60","authors":["Sabrina Chiesurin","Dimitris Dimakopoulos","Marco Antonio Sobrevilla Cabezudo","Arash Eshghi","Ioannis Papaioannou","Verena Rieser","Ioannis Konstas"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-1_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4435","is_paper":true,"keywords":["conversational qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.60.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77950/poster_document/a7182e0749394e2a1e43be339699b0ec.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77950/slideshow/c56e77b67517b2dac8d89bfe8f0a335a.pdf","title":"The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering","tldr":"Large language models are known to produce output which  sounds fluent and convincing, but is also often wrong, e.g. ``unfaithful\" with respect to a rationale as retrieved from a knowledge base. In this paper, we \nshow that task-based systems which exhibit certain advanced linguistic dialog behavior...","track":"Question Answering","underline_id":77950,"underline_url":"https://underline.io/events/395/posters/15200/poster/77950-the-dangers-of-trusting-stochastic-parrots-faithfulness-and-trust-in-open-domain-conversational-question-answering","video_url":null},{"abstract":"Although Large Language Models (LLMs) are successful in abstractive summarization of short dialogues, summarization of long dialogues remains challenging. To address this challenge, \n we propose a novel algorithm that processes complete dialogues comprising thousands of tokens into topic-segment-level Abstract Meaning Representation (AMR) graphs, which explicitly capture the dialogue structure, highlight salient semantics, and preserve high-level information. We also develop a new text-graph attention to leverage both graph semantics and a pretrained LLM that exploits the text. Finally, we propose an AMR node selection loss used jointly with conventional cross-entropy loss, to create additional training signals that facilitate graph feature encoding and content selection. Experiments show that our system outperforms the state-of-the-art models on multiple long dialogue summarization datasets, especially in low-resource settings, and generalizes well to out-of-domain data.","anthology_url":"https://aclanthology.org/2023.findings-acl.871","authors":["Yilun Hua","Zhaoyuan Deng","Kathleen McKeown"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-4_-summarization-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4448","is_paper":true,"keywords":["abstractive summarisation","conversational summarization","long-form summarization","architectures"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.871.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77952/poster_document/cc89855d1b9af759fbf0a8dd5cdedbee.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77952/slideshow/0644e90a12579a0ca9863d5ac59b25ce.pdf","title":"Improving Long Dialogue Summarization with Semantic Graph Representation","tldr":"Although Large Language Models (LLMs) are successful in abstractive summarization of short dialogues, summarization of long dialogues remains challenging. To address this challenge, \n we propose a novel algorithm that processes complete dialogues comprising thousands of tokens into topic-segment-lev...","track":"Summarization","underline_id":77952,"underline_url":"https://underline.io/events/395/posters/15240/poster/77952-improving-long-dialogue-summarization-with-semantic-graph-representation","video_url":null},{"abstract":"Relation extraction (RE) aims to extract relations from sentences and documents. Existing relation extraction models typically rely on supervised machine learning. However, recent studies showed that many RE datasets are incompletely annotated. This is known as the false negative problem in which valid relations are falsely annotated as 'no\\_relation'. Models trained with such data inevitably make similar mistakes during the inference stage.  Self-training has been proven effective in alleviating the false negative problem. However, traditional self-training is vulnerable to confirmation bias and exhibits poor performance in minority classes. To overcome this limitation, we proposed a novel class-adaptive re-sampling self-training framework. Specifically, we re-sampled the pseudo-labels for each class by precision and recall scores. Our re-sampling strategy favored the pseudo-labels of classes with high precision and low recall, which improved the overall recall without significantly compromising precision. We conducted experiments on document-level and biomedical relation extraction datasets, and the results showed that our proposed self-training framework consistently outperforms existing competitive methods on the Re-DocRED and ChemDisgene datasets when the training data are incompletely annotated.","anthology_url":"https://aclanthology.org/2023.findings-acl.549","authors":["Qingyu Tan","Lu Xu","Lidong Bing","Hwee Tou Ng"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-1_-information-extraction-(virtual-poster)"],"id":"P4451","is_paper":true,"keywords":["named entity recognition and relation extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.549.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77954/poster_document/c11721a8a65ff4c4d78e75ae3ab25c64.pdf","preview_image":"https://assets.underline.io/lecture/77954/poster/cd4759e2372fd10d75356788e6d4bb9b.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77954/slideshow/29b00dde0f26a4cbbb7ddb569bc77cd8.pdf","title":"Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data","tldr":"Relation extraction (RE) aims to extract relations from sentences and documents. Existing relation extraction models typically rely on supervised machine learning. However, recent studies showed that many RE datasets are incompletely annotated. This is known as the false negative problem in which va...","track":"Information Extraction","underline_id":77954,"underline_url":"https://underline.io/events/395/posters/15200/poster/77954-class-adaptive-self-training-for-relation-extraction-with-incompletely-annotated-training-data","video_url":null},{"abstract":"Automatic extraction of party (dis)similarities from texts such as party election manifestos or parliamentary speeches plays an increasing role in computational political science. However, existing approaches are fundamentally limited to targeting only global party (dis)-similarity: they condense the relationship between a pair of parties into a single figure, their similarity. In aggregating over all policy domains (e.g., health or foreign policy), they do not provide any qualitative insights into which domains parties agree or disagree on.\nThis paper proposes a workflow for estimating policy domain aware party similarity that overcomes this limitation. The workflow covers (a) definition of suitable policy domains; (b) automatic labeling of domains, if no manual labels are available; (c) computation of domain-level similarities and aggregation at a global level; (d) extraction of interpretable party positions on major policy axes via multidimensional scaling. We evaluate our workflow on manifestos from the German federal elections. We find that our method (a) yields high correlation when predicting party similarity at a global level and (b) provides accurate party-specific positions, even with automatically labelled policy domains.","anthology_url":"https://aclanthology.org/2023.findings-acl.499","authors":["Tanise Ceron","Dmitry Nikolaev","Sebastian Pad\u00f3"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)"],"id":"P4456","is_paper":true,"keywords":["nlp tools for social analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.499.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77956/poster_document/3156eb85abd98062f86b0a77b2836dfb.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Additive manifesto decomposition: A policy domain aware method for understanding party positioning","tldr":"Automatic extraction of party (dis)similarities from texts such as party election manifestos or parliamentary speeches plays an increasing role in computational political science. However, existing approaches are fundamentally limited to targeting only global party (dis)-similarity: they condense th...","track":"Computational Social Science and Cultural Analytics","underline_id":77956,"underline_url":"https://underline.io/events/395/posters/15200/poster/77956-additive-manifesto-decomposition-a-policy-domain-aware-method-for-understanding-party-positioning","video_url":null},{"abstract":"Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models. Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic metaphors and their associated visual elaborations. Evaluation by professional illustrators shows the promise of LLM-Diffusion Model collaboration for this task.To evaluate the utility of our Human-AI collaboration framework and the quality of our dataset, we perform both an intrinsic human-based evaluation and an extrinsic evaluation using visual entailment as a downstream task.","anthology_url":"https://aclanthology.org/2023.findings-acl.465","authors":["Tuhin Chakrabarty","Arkadiy Saakyan","Olivia Winn","Artemis Panagopoulou","Yue Yang","Marianna Apidianaki","Smaranda Muresan"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4466","is_paper":true,"keywords":["cross-modal content generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.465.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77957/slideshow/3e6bca93981161171f0a84886b887497.pdf","title":"I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors","tldr":"Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic met...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77957,"underline_url":"https://underline.io/events/395/posters/15240/poster/77957-i-spy-a-metaphor-large-language-models-and-diffusion-models-co-create-visual-metaphors","video_url":null},{"abstract":"We present a new dataset for studying conversation disentanglement in movies and TV series. While previous work has focused on conversation disentanglement in IRC chatroom dialogues, movies and TV shows provide a space for studying complex pragmatic patterns of floor and topic change in face-to-face multi-party interactions. In this work, we draw on theoretical research in sociolinguistics, sociology, and film studies to operationalize a conversational thread (including the notion of a floor change) in dramatic texts, and use that definition to annotate a dataset of 10,033 dialogue turns (comprising 2,209 threads) from 831 movies. We compare the performance of several disentanglement models on this dramatic dataset, and apply the best-performing model to disentangle 808 movies. We see that, contrary to expectation, average thread lengths do not decrease significantly over the past 40 years, and characters portrayed by actors who are women, while underrepresented, initiate more new conversational threads relative to their speaking time.","anthology_url":"https://aclanthology.org/2023.findings-acl.248","authors":["Kent K. Chang","Danica Chen","David Bamman"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-4_-computational-social-science-and-cultural-analytics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4500","is_paper":true,"keywords":["nlp tools for social analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.248.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77960/poster_document/e55e51c2306c6fbb281851844860b4c0.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77960/slideshow/c664e37e6ae56036a31221dc2d4a90b9.pdf","title":"Dramatic Conversation Disentanglement","tldr":"We present a new dataset for studying conversation disentanglement in movies and TV series. While previous work has focused on conversation disentanglement in IRC chatroom dialogues, movies and TV shows provide a space for studying complex pragmatic patterns of floor and topic change in face-to-face...","track":"Computational Social Science and Cultural Analytics","underline_id":77960,"underline_url":"https://underline.io/events/395/posters/15240/poster/77960-dramatic-conversation-disentanglement","video_url":null},{"abstract":"Dual encoders have been used for retrieval tasks and representation learning with good results. A standard way to train dual encoders is using a contrastive loss with in-batch negatives. In this work, we propose an improved contrastive learning objective by adding queries or documents from the same encoder towers to the negatives, for which we name it as \"contrastive loss with SAMe TOwer NEgatives\" (SamToNe). By evaluating on question answering retrieval benchmarks from MS MARCO and MultiReQA, and heterogenous zero-shot information retrieval benchmarks (BEIR), we demonstrate that SamToNe can effectively improve the retrieval quality for both symmetric and asymmetric dual encoders. By directly probing the embedding spaces of the two encoding towers via the t-SNE algorithm (van der Maaten and Hinton, 2008), we observe that SamToNe ensures the alignment between the embedding spaces from the two encoder towers. Based on the analysis of the embedding distance distributions of the top-1 retrieved results, we further explain the efficacy of the method from the perspective of regularisation.","anthology_url":"https://aclanthology.org/2023.findings-acl.761","authors":["Fedor Moiseev","Gustavo Hernandez Abrego","Peter Dornbach","Imed Zitouni","Enrique Alfonseca","Zhe Dong"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-7_-information-retrieval-and-text-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4509","is_paper":true,"keywords":["contrastive learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.761.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77961/poster_document/2f47e92452b4b0cd4090963348d3c00b.pdf","preview_image":"https://assets.underline.io/lecture/77961/poster/8661284aa2ea07f9af23dc97079bc96b.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77961/slideshow/af11dbba1888f3227777d3ed99ee30b1.pdf","title":"SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives","tldr":"Dual encoders have been used for retrieval tasks and representation learning with good results. A standard way to train dual encoders is using a contrastive loss with in-batch negatives. In this work, we propose an improved contrastive learning objective by adding queries or documents from the same ...","track":"Information Retrieval and Text Mining","underline_id":77961,"underline_url":"https://underline.io/events/395/posters/15279/poster/77961-samtone-improving-contrastive-loss-for-dual-encoder-retrieval-models-with-same-tower-negatives","video_url":null},{"abstract":"Class-incremental learning (CIL) aims to develop a learning system that can continually learn new classes from a data stream without forgetting previously learned classes. When learning classes incrementally, the classifier must be constantly updated to incorporate new classes, and the drift in decision boundary may lead to severe forgetting. This fundamental challenge, however, has not yet been studied extensively, especially in the setting where no samples from old classes are stored for rehearsal. In this paper, we take a closer look at how the drift in the classifier leads to forgetting, and accordingly, design four simple yet (super-) effective solutions to alleviate the classifier drift: an Individual Classifiers with Frozen Feature Extractor (ICE) framework where we individually train a classifier for each learning session, and its three variants ICE-PL, ICE-O, and ICE-PL\\&O which further take the logits of previously learned classes from old sessions or a constant logit of an Other class as constraint to the learning of new classifiers. Extensive experiments and analysis on 6 class-incremental information extraction tasks demonstrate that our solutions, especially ICE-O, consistently show significant improvement over the previous state-of-the-art approaches with up to 44.7\\% absolute F-score gain, providing a strong baseline and insights for future research on class-incremental learning.","anthology_url":"https://aclanthology.org/2023.findings-acl.141","authors":["Minqian Liu","Lifu Huang"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-4_-information-extraction-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4519","is_paper":true,"keywords":["named entity recognition and relation extraction","event extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.141.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77962/poster_document/37138756e5bec553e7b150150e393eaf.pdf","preview_image":"https://assets.underline.io/lecture/77962/poster/6ad213bdc612ab97cb3c12607690d154.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77962/slideshow/a69456132b4616b826544f27e04ee60e.pptx","title":"Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction","tldr":"Class-incremental learning (CIL) aims to develop a learning system that can continually learn new classes from a data stream without forgetting previously learned classes. When learning classes incrementally, the classifier must be constantly updated to incorporate new classes, and the drift in deci...","track":"Information Extraction","underline_id":77962,"underline_url":"https://underline.io/events/395/posters/15240/poster/77962-teamwork-is-not-always-good-an-empirical-study-of-classifier-drift-in-class-incremental-information-extraction","video_url":null},{"abstract":"We propose EAR, a query Expansion And Reranking approach for improving passage retrieval, with the application to open-domain question answering. EAR first applies a query expansion model to generate a diverse set of queries, and then uses a query reranker to select the ones that could lead to better retrieval results. Motivated by the observation that the best query expansion often is not picked by greedy decoding, EAR trains its reranker to predict the rank orders of the gold passages when issuing the expanded queries to a given retriever. By connecting better the query expansion model and retriever, EAR significantly enhances a traditional sparse retrieval method, BM25. Empirically, EAR improves top-5/20 accuracy by 3-8 and 5-10 points in in-domain and out-of-domain settings, respectively, when compared to a vanilla query expansion model, GAR, and a dense retrieval model, DPR.","anthology_url":"https://aclanthology.org/2023.findings-acl.768","authors":["Yung-Sung Chuang","Wei Fang","Shang-Wen Li","Wen-tau Yih","James Glass"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-1_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P452","is_paper":true,"keywords":["open-domain qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.768.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77318/poster_document/323b8eaba43b32ea04f7c1e30f29bdf9.pdf","preview_image":"https://assets.underline.io/lecture/77318/poster/c7e386aec4ce049001efd56c1cec083d.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77318/slideshow/faea0ac88b44c3d1ef76f5f3b2d6633a.pdf","title":"Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering","tldr":"We propose EAR, a query Expansion And Reranking approach for improving passage retrieval, with the application to open-domain question answering. EAR first applies a query expansion model to generate a diverse set of queries, and then uses a query reranker to select the ones that could lead to bette...","track":"Question Answering","underline_id":77318,"underline_url":"https://underline.io/events/395/posters/15200/poster/77318-counterfactual-active-learning-for-out-of-distribution-generalization","video_url":null},{"abstract":"Obtaining labeled data to train a model for a task of interest is often expensive. Prior work shows training models on multitask data augmented with task descriptions (prompts) effectively transfers knowledge to new tasks. Towards efficiently building task-specific models, we assume access to a small number (32-1000) of unlabeled target-task examples and use those to retrieve the most similar labeled examples from a large pool of multitask data augmented with prompts. Compared to the current practice of finetuning models on uniformly sampled prompted multitask data (e.g.: FLAN, T0), our approach of finetuning on cross-task nearest neighbors is significantly more data-efficient. Using only 2\\% of the data from the P3 pool without any labeled target-task data, our models outperform strong baselines trained on all available data by 3-30\\% on 12 out of 14 datasets representing held-out tasks including legal and scientific document QA. Similarly, models trained on cross-task nearest neighbors from SuperNaturalInstructions, representing about 5\\% of the pool, obtain comparable performance to state-of-the-art models on 12 held-out tasks from that pool. Moreover, the models produced by our approach also provide a better initialization than single multitask finetuned models for few-shot finetuning on target-task data, as shown by a 2-23% relative improvement over few-shot finetuned T0-3B models on 8 datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.576","authors":["Hamish Ivison","Noah A. Smith","Hannaneh Hajishirzi","Pradeep Dasigi"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-4_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4524","is_paper":true,"keywords":["prompting","retrieval-augmented models","fine-tuning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.576.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77963/poster_document/24b53d4b3b3b2537fb7032fa8fe74557.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77963/slideshow/95bb42a2d503649cf946790c43fe46b3.pdf","title":"Data-Efficient Finetuning Using Cross-Task Nearest Neighbors","tldr":"Obtaining labeled data to train a model for a task of interest is often expensive. Prior work shows training models on multitask data augmented with task descriptions (prompts) effectively transfers knowledge to new tasks. Towards efficiently building task-specific models, we assume access to a smal...","track":"Large Language Models","underline_id":77963,"underline_url":"https://underline.io/events/395/posters/15240/poster/77963-data-efficient-finetuning-using-cross-task-nearest-neighbors","video_url":null},{"abstract":"Semi-supervised domain adaptation (SSDA) adopts a model trained from a label-rich source domain to a new but related domain with a few labels of target data. It is shown that, in an SSDA setting, a simple combination of domain adaptation (DA) with semi-supervised learning (SSL) techniques often fails to effectively utilize the target supervision and cannot address distribution shifts across different domains due to the training data bias toward the source-labeled samples. In this paper, inspired by the co-learning of multiple classifiers for the computer vision tasks, we propose to decompose the SSDA framework for emotion-related tasks into two subcomponents of unsupervised domain adaptation (UDA) from the source to the target domain and semi-supervised learning (SSL) in the target domain where the two models iteratively teach each other by interchanging their high confident predictions. We further propose a novel data cartography-based regularization technique for pseudo-label denoising that employs training dynamics to further hone our models' performance. We publicly release our code.","anthology_url":"https://aclanthology.org/2023.findings-acl.333","authors":["Mahshid Hosseini","Cornelia Caragea"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)"],"id":"P4536","is_paper":true,"keywords":["transfer learning / domain adaptation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.333.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77965/poster_document/80f4b61ba5ef80cad30d12136b4d5d49.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77965/slideshow/a549ef79a6a235ab63affcd7b72f726c.pdf","title":"Semi-Supervised Domain Adaptation for Emotion-Related Tasks","tldr":"Semi-supervised domain adaptation (SSDA) adopts a model trained from a label-rich source domain to a new but related domain with a few labels of target data. It is shown that, in an SSDA setting, a simple combination of domain adaptation (DA) with semi-supervised learning (SSL) techniques often fail...","track":"Machine Learning for NLP","underline_id":77965,"underline_url":"https://underline.io/events/395/posters/15279/poster/77965-semi-supervised-domain-adaptation-for-emotion-related-tasks","video_url":null},{"abstract":"In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the benefits of pre-training with masked language modeling (MLM) objective in models as small as 1.25M parameters, and establish a strong correlation between pre-training perplexity and downstream performance (GLUE benchmark). We examine downscaling effects, extending scaling laws to models as small as ~1M parameters. At this scale,  we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below $2.2 \\times 10^{15}$ FLOPs. We also find that adding layers does not always benefit downstream performance.{Our filtered pre-training data, reduced English vocabulary, and code are available at {https://github.com/text-machine-lab/mini\\_bert}{$github.com/text-machine-lab/mini\\_bert$}}","anthology_url":"https://aclanthology.org/2023.findings-acl.326","authors":["Vijeta Deshpande","Dan Pechi","Shree Thatte","Vladislav Lialin","Anna Rumshisky"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-7_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4543","is_paper":true,"keywords":["pre-training","scaling","fine-tuning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.326.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77966/poster/5be742352705e1184ac773a17989922c.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77966/slideshow/622a8c69cee88ca2d7384f34dd8dec67.pdf","title":"Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale.","tldr":"In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge la...","track":"Large Language Models","underline_id":77966,"underline_url":"https://underline.io/events/395/posters/15279/poster/77966-honey-i-shrunk-the-language-language-model-behavior-at-reduced-scale","video_url":null},{"abstract":"Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations.\nUnfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.","anthology_url":"https://aclanthology.org/2023.findings-acl.779","authors":["Marius Mosbach","Tiago Pimentel","Shauli Ravfogel","Dietrich Klakow","Yanai Elazar"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-7_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4544","is_paper":true,"keywords":["evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.779.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77967/slideshow/eae147db32eefd30997d0c742a772842.pdf","title":"Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation","tldr":"Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows t...","track":"Theme: Reality Check","underline_id":77967,"underline_url":"https://underline.io/events/395/posters/15279/poster/77967-few-shot-fine-tuning-vs-in-context-learning-a-fair-comparison-and-evaluation","video_url":null},{"abstract":"We study the problem of retrieval with instructions, where users provide explicit descriptions of their intent along with their queries to guide a retrieval system. \nOur solution is a general-purpose task-aware retrieval system, trained using multi-task instruction tuning and can follow human-written instructions to find relevant documents to a given query.\nWe introduce the first large-scale collection of 37 retrieval datasets with instructions, BERRI, and present TART, a single multi-task retrieval system trained on BERRI with instructions that can adapt to a new task without any parameter updates. \nTART advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. \nWe further introduce a new evaluation setup, X\\^2-Retrieval, to better reflect real-world scenarios in which diverse domains and tasks are pooled. \nTART significantly outperforms competitive baselines in this setup, further highlighting the effectiveness of guiding retrieval with instructions.","anthology_url":"https://aclanthology.org/2023.findings-acl.225","authors":["Akari Asai","Timo Schick","Patrick Lewis","Xilun Chen","Gautier Izacard","Sebastian Riedel","Hannaneh Hajishirzi","Wen-tau Yih"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-4_-information-retrieval-and-text-mining-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4552","is_paper":true,"keywords":["passage retrieval","dense retrieval","re-ranking"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.225.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77968/poster/0a8f56867e5184bd9cc2ec4b24882ec5.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77968/slideshow/07f572b4eb00ae2fb1da8b0ee3eb39b7.pdf","title":"Task-aware Retrieval with Instructions","tldr":"We study the problem of retrieval with instructions, where users provide explicit descriptions of their intent along with their queries to guide a retrieval system. \nOur solution is a general-purpose task-aware retrieval system, trained using multi-task instruction tuning and can follow human-writte...","track":"Information Retrieval and Text Mining","underline_id":77968,"underline_url":"https://underline.io/events/395/posters/15240/poster/77968-precise-zero-shot-dense-retrieval-without-relevance-labels","video_url":null},{"abstract":"With the evolution of Knowledge Graphs (KGs), new entities emerge which are not seen before. Representation learning of KGs in such an inductive setting aims to capture and transfer the structural patterns from existing entities to new entities. However, the performance of existing methods in inductive KGs are limited by sparsity and implicit transfer. In this paper, we propose VMCL, a Contrastive Learning (CL) framework with graph guided Variational autoencoder on Meta-KGs in the inductive setting. We first propose representation generation to capture the encoded and generated representations of entities, where the generated variations can densify representations with complementary features. Then, we design two CL objectives that work across entities and meta-KGs to simulate the transfer mode. With extensive experiments we demonstrate that our proposed VMCL can significantly outperform previous state-of-the-art baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.900","authors":["Qian Li","Shafiq Joty","Daling Wang","Shi Feng","Yifei Zhang","Chengwei Qin"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-1_-nlp-applications-(virtual-poster)"],"id":"P4557","is_paper":true,"keywords":["knowledge graphs"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.900.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77969/poster_document/56249fcf4e03a024481d5ebed7b475f6.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Contrastive Learning with Generated Representations for Inductive Knowledge Graph Embedding","tldr":"With the evolution of Knowledge Graphs (KGs), new entities emerge which are not seen before. Representation learning of KGs in such an inductive setting aims to capture and transfer the structural patterns from existing entities to new entities. However, the performance of existing methods in induct...","track":"NLP Applications","underline_id":77969,"underline_url":"https://underline.io/events/395/posters/15200/poster/77969-contrastive-learning-with-generated-representations-for-inductive-knowledge-graph-embedding","video_url":null},{"abstract":"Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts. We continue pre-training BART, a representative model, to obtain SimpleBART. It consistently and significantly improves the results on lexical simplification, sentence simplification, and document-level simplification tasks over BART. At the end, we compare SimpleBART with several representative large language models (LLMs).","anthology_url":"https://aclanthology.org/2023.findings-acl.595","authors":["Renliang Sun","Wei Xu","Xiaojun Wan"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-7_-generation-(virtual-poster)"],"id":"P456","is_paper":true,"keywords":["domain adaptation","text-to-text generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.595.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification","tldr":"Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the p...","track":"Generation","underline_id":77320,"underline_url":"https://underline.io/events/395/posters/15279/poster/77320-teaching-the-pre-trained-model-to-generate-simple-texts-for-text-simplification","video_url":null},{"abstract":"Scientific progress in NLP rests on the reproducibility of researchers' claims.\nThe *CL conferences created the NLP Reproducibility Checklist in 2020 to be completed by authors at submission to remind them of key information to include. We provide the first analysis of the Checklist by examining 10,405 anonymous responses to it. First, we find evidence of an increase in reporting of information on efficiency, validation performance, summary statistics, and hyperparameters after the Checklist's introduction. Further, we show acceptance rate grows for submissions with more Yes responses. We find that the 44\\% of submissions that gather new data are 5\\% less likely to be accepted than those that did not; the average reviewer-rated reproducibility of these submissions is also 2\\% lower relative to the rest. We find that only 46\\% of submissions claim to open-source their code, though submissions that do have 8\\% higher reproducibility score relative to those that do not, the most for any item. We discuss what can be inferred about the state of reproducibility in NLP, and provide a set of recommendations for future conferences, including: a) allowing submitting code and appendices one week after the deadline, and b) measuring dataset reproducibility by a checklist of data collection practices.","anthology_url":"https://aclanthology.org/2023.findings-acl.809","authors":["Ian Magnusson","Noah A. Smith","Jesse Dodge"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-4_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4619","is_paper":true,"keywords":["(non-)reproducibility"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.809.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77971/poster_document/9b11411e9a91a0f8731752c92b8f6cfa.pdf","preview_image":"https://assets.underline.io/lecture/77971/poster/494292ebeea012390ddd28c3e19178dc.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77971/slideshow/ffaef612032ca9a3936677517dd30090.pdf","title":"Reproducibility in NLP: What Have We Learned from the Checklist?","tldr":"Scientific progress in NLP rests on the reproducibility of researchers' claims.\nThe *CL conferences created the NLP Reproducibility Checklist in 2020 to be completed by authors at submission to remind them of key information to include. We provide the first analysis of the Checklist by examining 10,...","track":"Theme: Reality Check","underline_id":77971,"underline_url":"https://underline.io/events/395/posters/15240/poster/77971-reproducibility-in-nlp-what-have-we-learned-from-the-checklistquestion","video_url":null},{"abstract":"Extractive opinion summarization extracts sentences from users' reviews to represent the prevalent opinions about a product or service. However, the extracted sentences can be redundant and may miss some important aspects, especially for centroid-based extractive summarization models (Radev et al., 2004). To alleviate these issues, we introduce TokenCluster\u2013 a method for unsupervised extractive opinion summarization that automatically identifies the aspects described in the review sentences and then extracts sentences based on their aspects. It identifies the underlying aspects of the review sentences using roots of noun phrases and adjectives appearing in them. Empirical evaluation shows that TokenCluster improves aspect coverage in summaries and achieves strong performance on multiple opinion summarization datasets, for both general and aspect-specific summarization. We also perform extensive ablation and human evaluation studies to validate the design choices of our method. The implementation of our work is available at https://github.com/leehaoyuan/TokenCluster","anthology_url":"https://aclanthology.org/2023.findings-acl.802","authors":["Haoyuan Li","Somnath Basu Roy Chowdhury","Snigdha Chaturvedi"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-4_-summarization-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4627","is_paper":true,"keywords":["extractive summarisation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.802.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77973/slideshow/472fdf61d75fc4e9564e17664af61650.pdf","title":"Aspect-aware Unsupervised Extractive Opinion Summarization","tldr":"Extractive opinion summarization extracts sentences from users' reviews to represent the prevalent opinions about a product or service. However, the extracted sentences can be redundant and may miss some important aspects, especially for centroid-based extractive summarization models (Radev et al., ...","track":"Summarization","underline_id":77973,"underline_url":"https://underline.io/events/395/posters/15240/poster/77973-aspect-aware-unsupervised-extractive-opinion-summarization","video_url":null},{"abstract":"Recent studies on automatic note generation have shown that doctors can save significant  amounts of time when using automatic clinical note generation (Knoll et al., 2022). Summarization models have been used for this task to generate clinical notes as summaries of doctor-patient conversations (Krishna et al., 2021; Cai et al., 2022). However, assessing which model would best serve clinicians in their daily practice is still a challenging task due to the large set of possible correct summaries, and the potential limitations of automatic evaluation metrics. In this paper we study evaluation methods and metrics for the automatic generation of clinical notes from medical conversation. In particular, we propose new task-specific metrics and we compare them to SOTA evaluation metrics in text summarization and generation, including: (i) knowledge-graph embedding-based metrics, (ii) customized model-based metrics with domain-specific weights, (iii) domain-adapted/fine-tuned metrics, and (iv) ensemble metrics. To study the correlation between the automatic metrics and manual judgments, we evaluate automatic notes/summaries by comparing the system and reference facts and computing the factual correctness, and the hallucination and omission rates for critical medical facts. This study relied on seven datasets manually annotated by domain experts. Our experiments show that automatic evaluation metrics can have substantially different behaviors on different types of clinical notes datasets. However, the results highlight one stable subset of metrics as the most correlated with human judgments with a relevant aggregation of different evaluation criteria.","anthology_url":"https://aclanthology.org/2023.findings-acl.161","authors":["Asma Ben Abacha","Wen-wai Yim","George Michalopoulos","Thomas Lin"],"category":"Findings","demo_url":null,"display_track":"Summarization","event_ids":["session-7_-summarization-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4634","is_paper":true,"keywords":["evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.161.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"An Investigation of Evaluation Methods in Automatic Medical Note Generation","tldr":"Recent studies on automatic note generation have shown that doctors can save significant  amounts of time when using automatic clinical note generation (Knoll et al., 2022). Summarization models have been used for this task to generate clinical notes as summaries of doctor-patient conversations (Kri...","track":"Summarization","underline_id":77974,"underline_url":"https://underline.io/events/395/posters/15279/poster/77974-an-investigation-of-evaluation-methods-in-automatic-medical-note-generation","video_url":null},{"abstract":"The robustness of a model for real-world deployment is decided by how well it performs on unseen data and distinguishes between in-domain and out-of-domain samples. Visual document classifiers have shown impressive performance on in-distribution test sets. However, they tend to have a hard time correctly classifying and differentiating out-of-distribution examples. Image-based classifiers lack the text component, whereas multi-modality transformer-based models face the token serialization problem in visual documents due to their diverse layouts. They also require a lot of computing power during inference, making them impractical for many real-world applications. We propose, GVdoc, a graph-based document classification model that addresses both of these challenges. Our approach generates a document graph based on its layout, and then trains a graph neural network to learn node and graph embeddings. Through experiments, we show that our model, even with fewer parameters, outperforms state-of-the-art models on out-of-distribution data while retaining comparable performance on the in-distribution test set.","anthology_url":"https://aclanthology.org/2023.findings-acl.329","authors":["Fnu Mohbat","Mohammed J Zaki","Catherine Finegan-Dollak","Ashish Verma"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4674","is_paper":true,"keywords":["financial/business nlp"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.329.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77978/poster_document/ec05182119e2b82ea92b1c2b9a2d16c8.pdf","preview_image":"https://assets.underline.io/lecture/77978/poster/f939f3d30766e48f8de53cea7c3be2e3.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77978/slideshow/e25aff9c4e7173f48eedf8e3f8c8c409.pdf","title":"GVdoc - Graph-based Visual DOcument Classification","tldr":"The robustness of a model for real-world deployment is decided by how well it performs on unseen data and distinguishes between in-domain and out-of-domain samples. Visual document classifiers have shown impressive performance on in-distribution test sets. However, they tend to have a hard time corr...","track":"NLP Applications","underline_id":77978,"underline_url":"https://underline.io/events/395/posters/15279/poster/77978-topic-guided-sampling-for-data-efficient-multi-domain-stance-detection","video_url":null},{"abstract":"Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to 'hallucinations', low performance on non-news genres, and outputs which are not exactly summaries. Targeting ACL 2023's `Reality Check' theme, we present GUMSum, a small but carefully crafted dataset of English summaries in 12 written and spoken genres for evaluation of abstractive summarization. Summaries are highly constrained, focusing on substitutive potential, factuality, and faithfulness. We present guidelines and evaluate human agreement as well as subjective judgments on recent system outputs, comparing general-domain untuned approaches, a fine-tuned one, and a prompt-based approach, to human performance. Results show that while GPT3 achieves impressive scores, it still underperforms humans, with varying quality across genres. Human judgments reveal different types of errors in supervised, prompted, and human-generated summaries, shedding light on the challenges of producing a good summary.","anthology_url":"https://aclanthology.org/2023.findings-acl.593","authors":["Yang Janet Liu","Amir Zeldes"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-7_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4675","is_paper":true,"keywords":["evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.593.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77979/poster_document/ea7160588c85d25ce221e5c26d138cbb.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77979/slideshow/126a15310d561732baf78912b7924c08.pdf","title":"GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization","tldr":"Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to 'hallucinations', low performance on non-news genres, and outputs which are not exactly summaries. Targeting ACL 2023's `Reality Check' theme, we present GUMSum, a small but carefully cra...","track":"Theme: Reality Check","underline_id":77979,"underline_url":"https://underline.io/events/395/posters/15279/poster/77979-gumsum-multi-genre-data-and-evaluation-for-english-abstractive-summarization","video_url":null},{"abstract":"Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4\\%-16\\% on in-domain, and 3\\%-7\\% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours) MATE achieves a WER reduction of 8\\%-23\\% over the first-pass baseline.","anthology_url":"https://aclanthology.org/2023.findings-acl.682","authors":["Jinglun Cai","Monica Sunkara","Xilai Li","Anshu Bhatia","Xiao Pan","Sravan Bodapati"],"category":"Findings","demo_url":null,"display_track":"Speech and Multimodality","event_ids":["session-7_-speech-and-multimodality-(virtual-poster)"],"id":"P4679","is_paper":true,"keywords":["automatic speech recognition","speech technologies","multimodality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.682.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77980/poster_document/3ed3afca540022c6662eab4528e7d7a6.pdf","preview_image":"https://assets.underline.io/lecture/77980/poster/b9e11f673d67ab678b7d0751d534fa08.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77980/slideshow/e2227556e1d7ba3eb0e315441457f8e6.pdf","title":"Masked Audio Text Encoders are Effective Multi-Modal Rescorers","tldr":"Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space ...","track":"Speech and Multimodality","underline_id":77980,"underline_url":"https://underline.io/events/395/posters/15279/poster/77980-masked-audio-text-encoders-are-effective-multi-modal-rescorers","video_url":null},{"abstract":"Retrieval accuracy is crucial to the performance of open-domain question answering (ODQA) systems. Recent work has demonstrated that dense hierarchical retrieval (DHR), which retrieves document candidates first and then relevant passages from the refined document set, can significantly outperform the single stage dense passage retriever (DPR). While effective, this approach requires document structure information to learn document representation and is hard to adopt to other domains without this information. Additionally, the dense retrievers tend to generalize poorly on out-of-domain data comparing with sparse retrievers such as BM25. In this paper, we propose Hybrid Hierarchical Retrieval (HHR) to address the existing limitations. Instead of relying solely on dense retrievers, we can apply sparse retriever, dense retriever, and a combination of them in both stages of document and passage retrieval. We perform extensive experiments on ODQA benchmarks and observe that our framework not only brings in-domain gains, but also generalizes better to zero-shot TriviaQA and Web Questions datasets with an average of 4.69\\% improvement on recall@100 over DHR. We also offer practical insights to trade off between retrieval accuracy, latency, and storage cost. The code is available on github.","anthology_url":"https://aclanthology.org/2023.findings-acl.679","authors":["Manoj Ghuhan Arivazhagan","Lan Liu","Peng Qi","Xinchi Chen","William Yang Wang","zhiheng huang"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-1_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4681","is_paper":true,"keywords":["open-domain qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.679.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77981/poster_document/9ceed4152dbef330abb8676b296bbeb5.pdf","preview_image":"https://assets.underline.io/lecture/77981/poster/91e6f7360e60eda1472758310ae14466.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77981/slideshow/ffed92fbb30e7980342913f9d42dc32c.pdf","title":"Hybrid Hierarchical Retrieval for Open-Domain Question Answering","tldr":"Retrieval accuracy is crucial to the performance of open-domain question answering (ODQA) systems. Recent work has demonstrated that dense hierarchical retrieval (DHR), which retrieves document candidates first and then relevant passages from the refined document set, can significantly outperform th...","track":"Question Answering","underline_id":77981,"underline_url":"https://underline.io/events/395/posters/15200/poster/77981-hybrid-hierarchical-retrieval-for-open-domain-question-answering","video_url":null},{"abstract":"Information Synchronization of semi-structured data across languages is challenging. For example, Wikipedia tables in one language need to be synchronized with others.  To address this problem, we introduce a new dataset InfoSync and a two-step method for tabular synchronization. InfoSync contains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages, of which a subset (~3.5K pairs) are manually annotated. The proposed method includes 1) Information Alignment to map rows and 2) Information Update for updating missing/outdated information for aligned tables across multilingual tables. When evaluated on InfoSync, information alignment achieves an F1 score of 87.91 (en <-> non-en). To evaluate information updation, we perform human-assisted Wikipedia edits on Infoboxes for 532 table pairs. Our approach obtains an acceptance rate of 77.28\\% on Wikipedia, showing the effectiveness of the proposed method.","anthology_url":"https://aclanthology.org/2023.findings-acl.159","authors":["Siddharth Hemant Khincha","Chelsi Jain","Vivek Gupta","Tushar Kataria","Shuo Zhang"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-1_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4682","is_paper":true,"keywords":["corpus creation","multilingual corpora","nlp datasets","datasets for low resource languages"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.159.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77982/poster_document/799392c41562f70f9ae4f986fd956dec.pdf","preview_image":"https://assets.underline.io/lecture/77982/poster/d35d53962c22eff02e91f65f20bcd2fe.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77982/slideshow/2ee78ddf1f6a19160367780cb2bfcd05.pdf","title":"InfoSync: Information Synchronization across Multilingual Semi-structured Tables","tldr":"Information Synchronization of semi-structured data across languages is challenging. For example, Wikipedia tables in one language need to be synchronized with others.  To address this problem, we introduce a new dataset InfoSync and a two-step method for tabular synchronization. InfoSync contains 1...","track":"Resources and Evaluation","underline_id":77982,"underline_url":"https://underline.io/events/395/posters/15200/poster/77982-infosync-information-synchronization-across-multilingual-semi-structured-tables","video_url":null},{"abstract":"Multimodal Review Helpfulness Prediction (MRHP) aims to rank product reviews based on predicted helpfulness scores and has been widely applied in e-commerce via presenting customers with useful reviews. Previous studies commonly employ fully-connected neural networks (FCNNs) as the final score predictor and pairwise loss as the training objective. However, FCNNs have been shown to perform inefficient splitting for review features, making the model difficult to clearly differentiate helpful from unhelpful reviews. Furthermore, pairwise objective, which works on review pairs, may not completely capture the MRHP goal to produce the ranking for the entire review list, and possibly induces low generalization during testing. To address these issues, we propose a listwise attention network that clearly captures the MRHP ranking context and a listwise optimization objective that enhances model generalization. We further propose gradient-boosted decision tree as the score predictor to efficaciously partition product reviews' representations. Extensive experiments demonstrate that our method achieves state-of-the-art results and polished generalization performance on two large-scale MRHP benchmark datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.106","authors":["Thong Thanh Nguyen","Xiaobao Wu","Xinshuai Dong","Cong-Duy T. Nguyen","Zhen Hai","Lidong Bing","Anh Tuan Luu"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)"],"id":"P4689","is_paper":true,"keywords":["cross-modal application","cross-modal information extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.106.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77984/poster_document/85996c70ef72604f4d46e75cfb0c905f.pdf","preview_image":"https://assets.underline.io/lecture/77984/poster/f6dfdbf2ab6ff716a7d02bd889e4d5f7.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Gradient-Boosted Decision Tree for Listwise Context Model in Multimodal Review Helpfulness Prediction","tldr":"Multimodal Review Helpfulness Prediction (MRHP) aims to rank product reviews based on predicted helpfulness scores and has been widely applied in e-commerce via presenting customers with useful reviews. Previous studies commonly employ fully-connected neural networks (FCNNs) as the final score predi...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77984,"underline_url":"https://underline.io/events/395/posters/15240/poster/77984-gradient-boosted-decision-tree-for-listwise-context-model-in-multimodal-review-helpfulness-prediction","video_url":null},{"abstract":"Geometric representation of query embeddings (using points, particles, rectangles and cones) can effectively achieve the task of answering complex logical queries expressed in first-order logic (FOL) form over knowledge graphs, allowing intuitive encodings. However, current geometric-based methods depend on the neural approach to model FOL operators (conjunction, disjunction and negation), which are not easily explainable with considerable computation cost. We overcome this challenge by introducing a symbolic modeling approach for the FOL operators, emphasizing the direct calculation of the intersection between geometric shapes, particularly sector-cones in the embedding space, to model the conjunction operator. This approach reduces the computation cost as a non-neural approach is involved in the core logic operators. Moreover, we propose to accelerate the learning in the relation projection operator using the neural approach to emphasize the essential role of this operator in all query structures. Although empirical evidence for explainability is challenging, our approach demonstrates a significant improvement in answering complex logical queries (both non-negative and negative FOL forms) over previous geometric-based models.","anthology_url":"https://aclanthology.org/2023.findings-acl.755","authors":["Chau Duc Minh Nguyen","Tim N French","Wei Liu","Michael Stewart"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-7_-question-answering-(virtual-poster)"],"id":"P4704","is_paper":true,"keywords":["multihop qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.755.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77985/poster_document/faa8852e0609ab9cb426914e72b6f1fe.pdf","preview_image":"https://assets.underline.io/lecture/77985/poster/3353d52322b88731839d21e0e1b85dee.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"SConE: Simplified Cone Embeddings with Symbolic Operators for Complex Logical Queries","tldr":"Geometric representation of query embeddings (using points, particles, rectangles and cones) can effectively achieve the task of answering complex logical queries expressed in first-order logic (FOL) form over knowledge graphs, allowing intuitive encodings. However, current geometric-based methods d...","track":"Question Answering","underline_id":77985,"underline_url":"https://underline.io/events/395/posters/15279/poster/77985-scone-simplified-cone-embeddings-with-symbolic-operators-for-complex-logical-queries","video_url":null},{"abstract":"There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the\u00a0high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting. We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST.\nFirst, we formulate DST as a Python programming task, explicitly modeling language coreference as variable reference in Python. Second, since in-context learning depends highly on the context examples, we propose a method to retrieve a diverse set of relevant examples to improve performance. Finally, we introduce a novel re-weighting method during decoding that takes into account probabilities of competing surface forms, and produces a more accurate dialogue state prediction.\nWe evaluate our approach using MultiWOZ and achieve state-of-the-art multi-domain joint-goal accuracy in zero and few-shot settings.","anthology_url":"https://aclanthology.org/2023.findings-acl.344","authors":["Brendan King","Jeffrey Flanigan"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4706","is_paper":true,"keywords":["task-oriented","dialogue state tracking"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.344.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77986/poster_document/b077b96ae894a5f88e490a6a4f02725e.pdf","preview_image":"https://assets.underline.io/lecture/77986/poster/e459d85d5becb50cff4237dd8c993a36.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77986/slideshow/5d8705dff7d6edda10320b7862ffe376.pdf","title":"Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking","tldr":"There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the\u00a0high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperfo...","track":"Dialogue and Interactive Systems","underline_id":77986,"underline_url":"https://underline.io/events/395/posters/15200/poster/77986-whitenedcse-whitening-based-contrastive-learning-of-sentence-embeddings","video_url":null},{"abstract":"The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TagReal that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TagReal achieves state-of-the-art performance on two benchmark datasets. We find that TagReal has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods.","anthology_url":"https://aclanthology.org/2023.findings-acl.709","authors":["Pengcheng Jiang","Shivam Agarwal","Bowen Jin","Xuan Wang","Jimeng Sun","Jiawei Han"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-4_-information-extraction-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P471","is_paper":true,"keywords":["knowledge base construction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.709.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77323/poster_document/7f94e32f62a632610617f3c8ffe35f6d.pdf","preview_image":"https://assets.underline.io/lecture/77323/poster/fe8a2a5ff6a3841c46f901fa846c9047.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77323/slideshow/b6b5704152ec17cfb5e02be9975ac4f1.pdf","title":"Text Augmented Open Knowledge Graph Completion via Pre-Trained Language Models","tldr":"The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM...","track":"Information Extraction","underline_id":77323,"underline_url":"https://underline.io/events/395/posters/15240/poster/77323-square-a-large-scale-dataset-of-sensitive-questions-and-acceptable-responses-created-through-human-machine-collaboration","video_url":null},{"abstract":"Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few- and zero-shot abilities -- they can effectively learn from a handful of handcrafted, completed responses (\"in-context examples\u201d), or are prompted to reason spontaneously through specially designed triggers. Nonetheless, some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of the examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the lack of guidance to the LLMs. To address these limitations, we propose Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs. Requiring neither handcrafted responses nor ground-truth labels, COSP selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria combining consistency, diversity and repetition. In the zero-shot setting for three different LLMs, we show that using only LLM predictions, COSP significantly improves performance up to 15\\% compared to zero-shot baselines and matches or exceeds few-shot baselines at a range of reasoning tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.216","authors":["Xingchen Wan","Ruoxi SUN","Hanjun Dai","Sercan Arik","Tomas Pfister"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-7_-large-language-models-(virtual-poster)"],"id":"P4715","is_paper":true,"keywords":["prompting"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.216.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77988/poster_document/84ea0460cb654b20585a03e6b4fa4691.pdf","preview_image":"https://assets.underline.io/lecture/77988/poster/2a05f0d7b4da1f6e8f6354c79999a17f.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77988/slideshow/8ff41595c6f4ff60e40213e85d4cb393.pdf","title":"Better Zero-Shot Reasoning with Self-Adaptive Prompting","tldr":"Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few- and zero-shot abilities -- they can effectively learn from a handful of handcrafted, completed resp...","track":"Large Language Models","underline_id":77988,"underline_url":"https://underline.io/events/395/posters/15279/poster/77988-better-zero-shot-reasoning-with-self-adaptive-prompting","video_url":null},{"abstract":"Open-domain question answering (ODQA) is a crucial task in natural language processing. A typical ODQA system relies on a retriever module to select relevant contexts from a large corpus for a downstream reading comprehension model. Existing ODQA datasets consist mainly of Wikipedia corpus, and are insufficient to study models' generalizability across diverse domains as models are trained and evaluated on the same genre of data. We propose **RobustQA**, a novel benchmark consisting of datasets from 8 different domains, which facilitates the evaluation of ODQA's domain robustness. To build **RobustQA**, we annotate QA pairs in retrieval datasets with rigorous quality control. We further examine improving QA performances by incorporating unsupervised learning methods with target-domain corpus and adopting large generative language models. These methods can effectively improve model performances on **RobustQA**. However, experimental results demonstrate a significant gap from in-domain training, suggesting that **RobustQA** is a challenging benchmark to evaluate ODQA domain robustness.","anthology_url":"https://aclanthology.org/2023.findings-acl.263","authors":["Rujun Han","Peng Qi","Yuhao Zhang","Lan Liu","Juliette Burger","William Yang Wang","zhiheng huang","Bing Xiang","Dan Roth"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-7_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4722","is_paper":true,"keywords":["open-domain qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.263.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77990/poster_document/3fecb5ee0d118d9bf357ed63d159b3f2.pdf","preview_image":"https://assets.underline.io/lecture/77990/poster/d864baa2be50fd51e8c8d42d7101a1ca.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77990/slideshow/2a4af8ee299d5738cf4e19ca3782f8b6.pdf","title":"RobustQA: Benchmarking the Robustness of Domain Adaptation for Open-Domain Question Answering","tldr":"Open-domain question answering (ODQA) is a crucial task in natural language processing. A typical ODQA system relies on a retriever module to select relevant contexts from a large corpus for a downstream reading comprehension model. Existing ODQA datasets consist mainly of Wikipedia corpus, and are ...","track":"Question Answering","underline_id":77990,"underline_url":"https://underline.io/events/395/posters/15279/poster/77990-robustqa-benchmarking-the-robustness-of-domain-adaptation-for-open-domain-question-answering","video_url":null},{"abstract":"With a growing focus on morphological inflection systems for languages where high-quality data is scarce, training data noise is a serious but so far largely ignored concern. We aim at closing this gap by investigating the types of noise encountered within a pipeline for truly unsupervised morphological paradigm completion and its impact on morphological inflection systems: First, we propose an error taxonomy and annotation pipeline for inflection training data. Then, we compare the effect of different types of noise on multiple state-of-the- art inflection models. Finally, we propose a novel character-level masked language modeling (CMLM) pretraining objective and explore its impact on the models' resistance to noise. Our experiments show that various architectures are impacted differently by separate types of noise, but encoder-decoders tend to be more robust to noise than models trained with a copy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs.","anthology_url":"https://aclanthology.org/2023.findings-acl.207","authors":["Adam Wiemerslage","Changbing Yang","Garrett Nicolai","Miikka Silfverberg","Katharina Kann"],"category":"Findings","demo_url":null,"display_track":"Phonology, Morphology, and Word Segmentation","event_ids":["session-7_-phonology,-morphology,-and-word-segmentation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4724","is_paper":true,"keywords":["morphological inflection"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.207.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77991/poster_document/24219124032e578479299fd0389ae095.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77991/slideshow/6d85f6d5d83943ea55a06fc2f061092c.pdf","title":"An Investigation of Noise in Morphological Inflection","tldr":"With a growing focus on morphological inflection systems for languages where high-quality data is scarce, training data noise is a serious but so far largely ignored concern. We aim at closing this gap by investigating the types of noise encountered within a pipeline for truly unsupervised morpholog...","track":"Phonology, Morphology, and Word Segmentation","underline_id":77991,"underline_url":"https://underline.io/events/395/posters/15279/poster/77991-an-investigation-of-noise-in-morphological-inflection","video_url":null},{"abstract":"Grammar induction, the task of learning a set of grammatical rules from raw or minimally labeled text data, can provide clues about what kinds of syntactic structures are learnable without prior knowledge. Recent work (e.g., Kim et al., 2019; Zhu et al., 2020; Jin et al., 2021a) has achieved advances in unsupervised induction of probabilistic context-free grammars (PCFGs). However, categorial grammar induction has received less recent attention, despite allowing inducers to support a larger set of syntactic categories\u2014due to restrictions on how categories can combine\u2014and providing a transparent interface with compositional semantics, opening up possibilities for models that jointly learn form and meaning. Motivated by this, we propose a new model for inducing a basic (Ajdukiewicz, 1935; Bar-Hillel, 1953) categorial grammar. In contrast to earlier categorial grammar induction systems (e.g., Bisk and Hockenmaier, 2012), our model learns from raw data without any part-of-speech information. Experiments on child-directed speech show that our model attains a recall-homogeneity of 0.33 on average, which dramatically increases to 0.59 when a bias toward forward function application is added to the model.","anthology_url":"https://aclanthology.org/2023.findings-acl.149","authors":["Christian Clark","William Schuler"],"category":"Findings","demo_url":null,"display_track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","event_ids":["session-1_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4728","is_paper":true,"keywords":["cognitive modeling","computational psycholinguistics"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.149.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77993/poster_document/668ff1bc6dc106d26528cb51bcba5295.pdf","preview_image":"https://assets.underline.io/lecture/77993/poster/5ebc38d3fd77db8b7d3cee123e9ecfae.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77993/slideshow/4e3a8b94a66eda61d19f95805d877cfa.pdf","title":"Categorial grammar induction from raw data","tldr":"Grammar induction, the task of learning a set of grammatical rules from raw or minimally labeled text data, can provide clues about what kinds of syntactic structures are learnable without prior knowledge. Recent work (e.g., Kim et al., 2019; Zhu et al., 2020; Jin et al., 2021a) has achieved advance...","track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","underline_id":77993,"underline_url":"https://underline.io/events/395/posters/15200/poster/77993-categorial-grammar-induction-from-raw-data","video_url":null},{"abstract":"We present a novel end-to-end generative task and system for predicting event factuality holders, targets, and their associated factuality values. We perform the first experiments using all sources and targets of factuality statements from the FactBank corpus. We perform multi-task learning with other tasks and event-factuality corpora to improve on the FactBank source and target task. We argue that careful domain specific target text output format in generative systems is important and verify this with multiple experiments on target text output structure. We redo previous state-of-the-art author-only event factuality experiments and also offer insights towards a generative paradigm for the author-only event factuality prediction task.","anthology_url":"https://aclanthology.org/2023.findings-acl.44","authors":["John Murzaku","Tyler G Osborne","Amittai F Aviram","Owen Rambow"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-4_-discourse-and-pragmatics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4744","is_paper":true,"keywords":["discourse relations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.44.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77996/poster_document/66ba2fada513b9428457c35ead074a65.pdf","preview_image":"https://assets.underline.io/lecture/77996/poster/86a65dbffb3cb0b02e13e5717959c979.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77996/slideshow/3c4cfdb59685916c946d92df56c3dbb9.pdf","title":"Towards Generative Event Factuality Prediction","tldr":"We present a novel end-to-end generative task and system for predicting event factuality holders, targets, and their associated factuality values. We perform the first experiments using all sources and targets of factuality statements from the FactBank corpus. We perform multi-task learning with oth...","track":"Discourse and Pragmatics","underline_id":77996,"underline_url":"https://underline.io/events/395/posters/15240/poster/77996-lilgym-natural-language-visual-reasoning-with-reinforcement-learning","video_url":null},{"abstract":"Text classification in education, usually called auto-tagging, is the automated process of assigning relevant tags to educational content, such as questions and textbooks. However, auto-tagging suffers from a data scarcity problem, which stems from two major challenges: 1) it possesses a large tag space and 2) it is multi-label. Though a retrieval approach is reportedly good at low-resource scenarios, there have been fewer efforts to directly address the data scarcity problem. To mitigate these issues, here we propose a novel retrieval approach CEAA that provides effective learning in educational text classification. Our main contributions are as follows: 1) we leverage transfer learning from question-answering datasets, and 2) we propose a simple but effective data augmentation method introducing cross-encoder style texts to a bi-encoder architecture for more efficient inference. An extensive set of experiments shows that our proposed method is effective in multi-label scenarios and low-resource tags compared to state-of-the-art models.","anthology_url":"https://aclanthology.org/2023.findings-acl.137","authors":["Hyun Seung Lee","Seungtaek Choi","Yunsung Lee","Hyeongdon Moon","Shinhyeok Oh","Myeongho Jeong","Hyojun Go","Christian Wallraven"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)"],"id":"P4746","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.137.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77997/poster_document/96dff71fe10a7b9f7f9a9920f583f641.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Cross Encoding as Augmentation: Towards Effective Educational Text Classification","tldr":"Text classification in education, usually called auto-tagging, is the automated process of assigning relevant tags to educational content, such as questions and textbooks. However, auto-tagging suffers from a data scarcity problem, which stems from two major challenges: 1) it possesses a large tag s...","track":"NLP Applications","underline_id":77997,"underline_url":"https://underline.io/events/395/posters/15240/poster/77997-cross-encoding-as-augmentation-towards-effective-educational-text-classification","video_url":null},{"abstract":"Researchers are witnessing knowledge-inspired natural language processing shifts the focus from entity-level to event-level, whereas event coreference resolution is one of the core challenges. This paper proposes a novel model for within-document event coreference resolution. On the basis of event but not entity as before, our model learns and integrates multiple representations from both event alone and event pair. For the former, we introduce multiple linguistics-motivated event alone features for more discriminative event representations. For the latter, we consider multiple similarity measures to capture the distinction of event pair. Our proposed model achieves new state-of-the-art on the ACE 2005 benchmark, demonstrating the effectiveness of our proposed framework.","anthology_url":"https://aclanthology.org/2023.findings-acl.855","authors":["Yao Yao","Zuchao Li","Hai Zhao"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-1_-discourse-and-pragmatics-(virtual-poster)"],"id":"P4772","is_paper":true,"keywords":["coreference resolution"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.855.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77999/poster_document/2b0130b1b2b7c24daf2bba1eb8221564.pdf","preview_image":"https://assets.underline.io/lecture/77999/poster/aaa62c8919880e1608ed42e93ceb6f08.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Learning Event-aware Measures for Event Coreference Resolution","tldr":"Researchers are witnessing knowledge-inspired natural language processing shifts the focus from entity-level to event-level, whereas event coreference resolution is one of the core challenges. This paper proposes a novel model for within-document event coreference resolution. On the basis of event b...","track":"Discourse and Pragmatics","underline_id":77999,"underline_url":"https://underline.io/events/395/posters/15200/poster/77999-learning-event-aware-measures-for-event-coreference-resolution","video_url":null},{"abstract":"Goal-oriented Script Generation is a new task of generating a list of steps that can fulfill the given goal. In this paper, we propose to extend the task from the perspective of cognitive theory. Instead of a simple flat structure, the steps are typically organized hierarchically --- Human often decompose a complex task into subgoals, where each subgoal can be further decomposed into steps. To establish the benchmark, we contribute a new dataset, propose several baseline methods, and set up evaluation metrics. Both automatic and human evaluation verify the high-quality of dataset, as well as the effectiveness of incorporating subgoals into hierarchical script generation. Furthermore, We also design and evaluate the model to discover subgoal, and find that it is a bit more difficult to decompose the goals than summarizing from segmented steps.","anthology_url":"https://aclanthology.org/2023.findings-acl.644","authors":["XINZE LI","Yixin Cao","Muhao Chen","Aixin Sun"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P4810","is_paper":true,"keywords":["benchmarking"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.644.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78001/poster_document/e41f15caaf58598fb4422c8bbc54499b.pdf","preview_image":"https://assets.underline.io/lecture/78001/poster/93c936976b937919e99d63d513f69069.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78001/slideshow/082aaf45bf920e9dcdbe4dd1843e8f4d.pdf","title":"Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation","tldr":"Goal-oriented Script Generation is a new task of generating a list of steps that can fulfill the given goal. In this paper, we propose to extend the task from the perspective of cognitive theory. Instead of a simple flat structure, the steps are typically organized hierarchically --- Human often dec...","track":"Resources and Evaluation","underline_id":78001,"underline_url":"https://underline.io/events/395/posters/15279/poster/78001-take-a-break-in-the-middle-investigating-subgoals-towards-hierarchical-script-generation","video_url":null},{"abstract":"Natural language understanding (NLU) models often suffer from unintended dataset biases. Among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (PoE), have stood out for their impressive empirical success. However, previous ensemble-based debiasing methods typically apply debiasing on top-level logits without directly addressing biased attention patterns. Attention serves as the main media of feature interaction and aggregation in PLMs and plays a crucial role in providing robust prediction. In this paper, we propose REsidual Attention Debiasing (READ), an end-to-end debiasing method that mitigates unintended biases from attention. Experiments on three NLU benchmarks show that READ significantly improves the OOD performance of BERT-based models, including +12.9\\% accuracy on HANS, +11.0\\% accuracy on FEVER-Symmetric, and +2.7\\% F1 on PAWS. Detailed analyses demonstrate the crucial role of unbiased attention in robust NLU models and that READ effectively mitigates biases in attention.","anthology_url":"https://aclanthology.org/2023.findings-acl.32","authors":["Fei Wang","James Y. Huang","Tianyi Yan","Wenxuan Zhou","Muhao Chen"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4811","is_paper":true,"keywords":["data shortcuts/artifacts","robustness"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.32.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78002/poster_document/698bcb8c9caab2052219004de11da4ab.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Robust Natural Language Understanding with Residual Attention Debiasing","tldr":"Natural language understanding (NLU) models often suffer from unintended dataset biases. Among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (PoE), have stood out for their impressive empirical success. However, previous ensemble-based debiasing methods typ...","track":"Interpretability and Analysis of Models for NLP","underline_id":78002,"underline_url":"https://underline.io/events/395/posters/15200/poster/78002-robust-natural-language-understanding-with-residual-attention-debiasing","video_url":null},{"abstract":"Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (QUD) for discourse analysis and seeks to derive QUD structures automatically. QUD views each sentence as an answer to a question triggered in prior context; thus, we characterize relationships between sentences as free-form questions, in contrast to exhaustive fine-grained taxonomies. We develop the first-of-its-kind QUD parser that derives a dependency structure of questions over full documents, trained using a large, crowdsourced question-answering dataset DCQA (Ko et al., 2022). Human evaluation results show that QUD dependency parsing is possible for language models trained with this crowdsourced, generalizable annotation scheme. We illustrate how our QUD structure is distinct from RST trees, and demonstrate the utility of QUD analysis in the context of document simplification. Our findings show that QUD parsing is an appealing alternative for automatic discourse processing.","anthology_url":"https://aclanthology.org/2023.findings-acl.710","authors":["Wei-Jen Ko","Yating Wu","Cutter J Dalton","Dananjay T Srinivas","Greg Durrett","Junyi Jessy Li"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-4_-discourse-and-pragmatics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4813","is_paper":true,"keywords":["discourse parsing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.710.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78003/poster_document/f74f672eb92038309bf78758841f1d80.pdf","preview_image":"https://assets.underline.io/lecture/78003/poster/c240f43c089ff4fc8fd1cf7d90ad68d7.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78003/slideshow/2ba8f805a04774b784febedfbd43b113.pdf","title":"Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion","tldr":"Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (...","track":"Discourse and Pragmatics","underline_id":78003,"underline_url":"https://underline.io/events/395/posters/15240/poster/78003-discourse-analysis-via-questions-and-answers-parsing-dependency-structures-of-questions-under-discussion","video_url":null},{"abstract":"Vision-language tasks, such as VQA, SNLI-VE, and VCR are challenging because they require the model's reasoning ability to understand the semantics of the visual world and natural language. Supervised methods working for vision-language tasks have been well-studied. However, solving these tasks in a zero-shot setting is less explored. Since Contrastive Language-Image Pre-training (CLIP) has shown remarkable zero-shot performance on image-text matching, previous works utilized its strong zero-shot ability by converting vision-language tasks into an image-text matching problem, and they mainly consider global-level matching (e.g., the whole image or sentence). However, we find visual and textual fine-grained information, e.g., keywords in the sentence and objects in the image, can be fairly informative for semantics understanding. Inspired by this, we propose a unified framework to take advantage of the fine-grained information for zero-shot vision-language learning, covering multiple tasks such as VQA, SNLI-VE, and VCR. Our experiments show that our framework outperforms former zero-shot methods on VQA and achieves substantial improvement on SNLI-VE and VCR. Furthermore, our ablation studies confirm the effectiveness and generalizability of our proposed method.","anthology_url":"https://aclanthology.org/2023.findings-acl.49","authors":["Rui Sun","Zhecan Wang","Haoxuan You","Noel Codella","Kai-Wei Chang","Shih-Fu Chang"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-7_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)"],"id":"P4820","is_paper":true,"keywords":["vision question answering"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.49.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78004/poster_document/890979c68393eacc11922112780b3a93.pdf","preview_image":"https://assets.underline.io/lecture/78004/poster/d38468847d0c982d6e031a10beb8c9bd.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78004/slideshow/7cc066e7ef2fa62ba64c36a02f347d0d.pdf","title":"UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding","tldr":"Vision-language tasks, such as VQA, SNLI-VE, and VCR are challenging because they require the model's reasoning ability to understand the semantics of the visual world and natural language. Supervised methods working for vision-language tasks have been well-studied. However, solving these tasks in a...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":78004,"underline_url":"https://underline.io/events/395/posters/15279/poster/78004-unsupervised-subtitle-segmentation-with-masked-language-models","video_url":null},{"abstract":"The Open-Domain Question Answering (ODQA) task involves retrieving and subsequently generating answers from fine-grained relevant passages within a database. Current systems leverage Pretrained Language Models (PLMs) to model the relationship between questions and passages. However, the diversity in surface form expressions can hinder the model's ability to capture accurate correlations, especially within complex contexts. Therefore, we utilize Abstract Meaning Representation (AMR) graphs to assist the model in understanding complex semantic information. We introduce a method known as Graph-as-Token (GST) to incorporate AMRs into PLMs. Results from Natural Questions (NQ) and TriviaQA (TQ) demonstrate that our GST method can significantly improve performance, resulting in up to 2.44/3.17 Exact Match score improvements on NQ/TQ respectively. Furthermore, our method enhances robustness and outperforms alternative Graph Neural Network (GNN) methods for integrating AMRs. To the best of our knowledge, we are the first to employ semantic graphs in ODQA.","anthology_url":"https://aclanthology.org/2023.findings-acl.131","authors":["Cunxiang Wang","Zhikun Xu","Qipeng Guo","Xiangkun Hu","Xuefeng Bai","Zheng Zhang","Yue Zhang"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-1_-question-answering-(virtual-poster)"],"id":"P4827","is_paper":true,"keywords":["open-domain qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.131.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Exploiting Abstract Meaning Representation for Open-Domain Question Answering","tldr":"The Open-Domain Question Answering (ODQA) task involves retrieving and subsequently generating answers from fine-grained relevant passages within a database. Current systems leverage Pretrained Language Models (PLMs) to model the relationship between questions and passages. However, the diversity in...","track":"Question Answering","underline_id":78007,"underline_url":"https://underline.io/events/395/posters/15200/poster/78007-exploiting-abstract-meaning-representation-for-open-domain-question-answering","video_url":null},{"abstract":"Machine Reading Comprehension (MRC) is to answer questions based on a given passage, which has made great achievements using pre-trained Language Models (LMs). We study the robustness of MRC models to names which is flexible and repeatability.  MRC models based on LMs may overuse the name information to make predictions, which causes the representation of names to be non-interchangeable, called name bias. In this paper, we propose a novel Causal Interventional paradigm for MRC (CI4MRC) to mitigate name bias. Specifically, we uncover that the pre-trained knowledge concerning names is indeed a confounder by analyzing the causalities among the pre-trained knowledge, context representation and answers based on a Structural Causal Model (SCM). We develop effective CI4MRC algorithmic implementations to constrain the confounder based on the neuron-wise and token-wise adjustments. Experiments demonstrate that our proposed CI4MRC effectively mitigates the name bias and achieves competitive performance on the original SQuAD. Moreover, our method is general to various pre-trained LMs and performs robustly on the adversarial datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.812","authors":["Jiazheng Zhu","shaojuan wu","Xiaowang Zhang","Yuexian Hou","Zhiyong Feng"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)"],"id":"P4844","is_paper":true,"keywords":["model bias/unfairness mitigation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.812.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Causal Intervention for Mitigating Name Bias in Machine Reading Comprehension","tldr":"Machine Reading Comprehension (MRC) is to answer questions based on a given passage, which has made great achievements using pre-trained Language Models (LMs). We study the robustness of MRC models to names which is flexible and repeatability.  MRC models based on LMs may overuse the name informatio...","track":"Ethics and NLP","underline_id":78010,"underline_url":"https://underline.io/events/395/posters/15279/poster/78010-causal-intervention-for-mitigating-name-bias-in-machine-reading-comprehension","video_url":null},{"abstract":"Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a \"concise explanation\u201d of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP-Sym dataset will be released for future research on symbolic math problems.","anthology_url":"https://aclanthology.org/2023.findings-acl.364","authors":["Vedant Gaur","Nikunj Saunshi"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-7_-large-language-models-(virtual-poster)"],"id":"P4916","is_paper":true,"keywords":["prompting","interpretability/analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.364.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78019/poster_document/ae920db40635cf905d424f957d9b5564.pdf","preview_image":"https://assets.underline.io/lecture/78019/poster/53bec388fef34155befb053d614fa26d.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78019/slideshow/dc21934b98613bae94b12126038b6237.pptx","title":"Reasoning in Large Language Models Through Symbolic Math Word Problems","tldr":"Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic ...","track":"Large Language Models","underline_id":78019,"underline_url":"https://underline.io/events/395/posters/15279/poster/78019-reasoning-in-large-language-models-through-symbolic-math-word-problems","video_url":null},{"abstract":"*Warning: This paper contains several contents that may be toxic, harmful, or offensive.*\n\nIn the last few years, text-to-image generative models have gained remarkable success in generating images with unprecedented quality accompanied by a breakthrough of inference speed. Despite their rapid progress, human biases that manifest in the training examples, particularly with regard to common stereotypical biases, like gender and skin tone, still have been found in these generative models. In this work, we seek to measure more complex human biases exist in the task of text-to-image generations. Inspired by the well-known Implicit Association Test (IAT) from social psychology, we propose a novel Text-to-Image Association Test (T2IAT) framework that quantifies the implicit stereotypes between concepts and valence, and those in the images. We replicate the previously documented bias tests on generative models, including morally neutral tests on flowers and insects as well as demographic stereotypical tests on diverse social attributes. The results of these experiments demonstrate the presence of complex stereotypical behaviors in image generations.","anthology_url":"https://aclanthology.org/2023.findings-acl.160","authors":["Jialu Wang","Xinyue Gabby Liu","Zonglin Di","Yang Liu","Xin Eric Wang"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4937","is_paper":true,"keywords":["model bias/fairness evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.160.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78021/poster_document/45b41a3a4d228e73c73abc0c3370ffa5.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78021/slideshow/b5f306bb27a3c8d5764766814da165e8.pdf","title":"T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation","tldr":"*Warning: This paper contains several contents that may be toxic, harmful, or offensive.*\n\nIn the last few years, text-to-image generative models have gained remarkable success in generating images with unprecedented quality accompanied by a breakthrough of inference speed. Despite their rapid progr...","track":"Ethics and NLP","underline_id":78021,"underline_url":"https://underline.io/events/395/posters/15279/poster/78021-t2iat-measuring-valence-and-stereotypical-biases-in-text-to-image-generation","video_url":null},{"abstract":"Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate CodeExecutor on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization abilities of pre-trained models for code execution.","anthology_url":"https://aclanthology.org/2023.findings-acl.308","authors":["Chenxiao Liu","Shuai Lu","Weizhu Chen","Daxin Jiang","Alexey Svyatkovskiy","Shengyu Fu","Neel Sundaresan","Nan Duan"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-1_-large-language-models-(virtual-poster)"],"id":"P495","is_paper":true,"keywords":["pre-training"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.308.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77327/poster_document/814dc9c58b00af430c9fb15db7ec96ab.pdf","preview_image":"https://assets.underline.io/lecture/77327/poster/c3c6ee98f3e6c2c9bd08e1259121434e.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Code Execution with Pre-trained Language Models","tldr":"Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-t...","track":"Large Language Models","underline_id":77327,"underline_url":"https://underline.io/events/395/posters/15200/poster/77327-code-execution-with-pre-trained-language-models","video_url":null},{"abstract":"Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. This makes the task of text classification, particularly in the zero-shot scenario, extremely challenging. In this paper, we investigate the task of zero-shot text classification with the aim of improving the ability of pre-trained language models (PLMs) to generalize to both seen and unseen data across varying aspects and domains. To solve this we introduce two new simple yet effective pre-training strategies, Implicit and Explicit pre-training. These methods inject aspect-level understanding into the model at train time with the goal of conditioning the model to build task-level understanding. To evaluate this, we construct and release UTCD, a new benchmark dataset for evaluating text classification in zero-shot settings. Experimental results on UTCD show that our approach achieves improved zero-shot generalization on a suite of challenging datasets across an array of zero-shot formalizations.","anthology_url":"https://aclanthology.org/2023.findings-acl.64","authors":["Christopher Clarke","Yuzhao Heng","Yiping Kang","Krisztian Flautner","Lingjia Tang","Jason Mars"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P4956","is_paper":true,"keywords":["transfer learning / domain adaptation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.64.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78024/poster_document/974766a7a8e856c77521359dbc7bdabd.pdf","preview_image":"https://assets.underline.io/lecture/78024/poster/2efaf374ba1c61161a63899964dc10ea.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78024/slideshow/a154664794634a3f5a52e1cf5bbc779a.pdf","title":"Label Agnostic Pre-training for Zero-shot Text Classification","tldr":"Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sent...","track":"Machine Learning for NLP","underline_id":78024,"underline_url":"https://underline.io/events/395/posters/15200/poster/78024-label-agnostic-pre-training-for-zero-shot-text-classification","video_url":null},{"abstract":"In this paper, we address the task of cloze-style multiple choice question (MCQs) distractor generation. Our study is featured by the following designs. First, we propose to formulate the cloze distractor generation as a Text2Text task. Second, we propose pseudo Kullback-Leibler Divergence for regulating the generation to consider the item discrimination index in education evaluation. Third, we explore the candidate augmentation strategy and multi-tasking training with cloze-related tasks to further boost the generation performance. Through experiments with benchmarking datasets, our best perfomring model advances the state-of-the-art result from 10.81 to 22.00 (p@1 score).","anthology_url":"https://aclanthology.org/2023.findings-acl.790","authors":["Hui-Juan Wang","Kai-Yu Hsieh","Han-Cheng Yu","Jui-Ching Tsou","Yu An Shih","Chen-Hua Huang","Yao-Chung Fan"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P5015","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.790.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78032/poster_document/ce7629221ad8dd8003fcfd3cb4bf7c53.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78032/slideshow/064911798b8e9c6a702d8b9220a6f0cb.pptx","title":"Distractor Generation based on Text2Text Language Models with Pseudo Kullback-Leibler Divergence Regulation","tldr":"In this paper, we address the task of cloze-style multiple choice question (MCQs) distractor generation. Our study is featured by the following designs. First, we propose to formulate the cloze distractor generation as a Text2Text task. Second, we propose pseudo Kullback-Leibler Divergence for regul...","track":"NLP Applications","underline_id":78032,"underline_url":"https://underline.io/events/395/posters/15279/poster/78032-distractor-generation-based-on-text2text-language-models-with-pseudo-kullback-leibler-divergence-regulation","video_url":null},{"abstract":"Biases cause discrepancies in healthcare services.\nRace, gender, and age of a patient affect interactions with physicians and the medical treatments one receives.\nThese biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora.\nTo bring awareness to such repercussions, we examine social biases present in the biomedical masked language models.\nWe curate prompts based on evidence-based practice and compare generated diagnoses based on biases.\nFor a case study, we measure bias in diagnosing coronary artery disease and using cardiovascular procedures based on bias.\nOur study demonstrates that biomedical models are less biased than BERT in gender, while the opposite is true for race and age.","anthology_url":"https://aclanthology.org/2023.findings-acl.749","authors":["Michelle YoungJin Kim","Junghwan Kim","Kristen Johnson"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-4_-computational-social-science-and-cultural-analytics-(virtual-poster)"],"id":"P5024","is_paper":true,"keywords":["language/cultural bias analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.749.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78034/poster_document/b9b2898fb9d33385285e86215064c959.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Race, Gender, and Age Biases in Biomedical Masked Language Models","tldr":"Biases cause discrepancies in healthcare services.\nRace, gender, and age of a patient affect interactions with physicians and the medical treatments one receives.\nThese biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora.\nT...","track":"Computational Social Science and Cultural Analytics","underline_id":78034,"underline_url":"https://underline.io/events/395/posters/15240/poster/78034-race-gender-and-age-biases-in-biomedical-masked-language-models","video_url":null},{"abstract":"Text style transfer is an exciting task within the field of natural language generation that is often plagued by the need for high-quality paired datasets. Furthermore, training a model for multi-attribute text style transfer requires datasets with sufficient support across all combinations of the considered stylistic attributes, adding to the challenges of training a style transfer model. \nThis paper explores the impact of training data input diversity on the quality of the generated text from the multi-style transfer model. We construct a pseudo-parallel dataset by devising heuristics to adjust the style distribution in the training samples. We balance our training dataset using marginal and joint distributions to train our style transfer models. We observe that a balanced dataset produces more effective control effects over multiple styles than an imbalanced or skewed one. Through quantitative analysis, we explore the impact of multiple style distributions in training data on style-transferred output. These findings will better inform the design of style-transfer datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.243","authors":["Debarati Das","David Ma","Dongyeop Kang"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)"],"id":"P5031","is_paper":true,"keywords":["style analysis","style generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.243.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78036/poster_document/2feb943ad42fedb71b3da9410bd12fc6.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Balancing the Effect of Training Dataset Distribution of Multiple Styles for Multi-Style Text Transfer","tldr":"Text style transfer is an exciting task within the field of natural language generation that is often plagued by the need for high-quality paired datasets. Furthermore, training a model for multi-attribute text style transfer requires datasets with sufficient support across all combinations of the c...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":78036,"underline_url":"https://underline.io/events/395/posters/15240/poster/78036-balancing-the-effect-of-training-dataset-distribution-of-multiple-styles-for-multi-style-text-transfer","video_url":null},{"abstract":"Norms, which are culturally accepted guidelines for behaviours, can be integrated into conversational models to generate utterances that are appropriate for the socio-cultural context. Existing methods for norm recognition tend to focus only on surface-level features of dialogues and do not take into account the interactions within a conversation. \nTo address this issue, we propose NormMark, a probabilistic generative Markov model to carry the latent features throughout a dialogue. These features are captured by discrete and continuous latent variables   conditioned on the conversation history, and improve the model's ability in  norm recognition. The model is trainable on weakly annotated data using the variational technique. On a dataset with limited  norm annotations, we show that our approach achieves higher F1 score,  outperforming current state-of-the-art methods, including  GPT3.","anthology_url":"https://aclanthology.org/2023.findings-acl.314","authors":["Farhad Moghimifar","shilin qu","Tongtong Wu","Yuan-Fang Li","Gholamreza Haffari"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)"],"id":"P5059","is_paper":true,"keywords":["sociolinguistics"],"languages":["mandarin chinese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.314.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78038/poster_document/a7c67d05575103e81f1acf1ad99b7c55.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78038/slideshow/2e47ef4791bcb62736824e8ac531ef0b.pdf","title":"NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery","tldr":"Norms, which are culturally accepted guidelines for behaviours, can be integrated into conversational models to generate utterances that are appropriate for the socio-cultural context. Existing methods for norm recognition tend to focus only on surface-level features of dialogues and do not take int...","track":"Computational Social Science and Cultural Analytics","underline_id":78038,"underline_url":"https://underline.io/events/395/posters/15279/poster/78038-normmark-a-weakly-supervised-markov-model-for-socio-cultural-norm-discovery","video_url":null},{"abstract":"Textual scene graph parsing has become increasingly important in various vision-language applications, including image caption evaluation and image retrieval. However, existing scene graph parsers that convert image captions into scene graphs often suffer from two types of errors. First, the generated scene graphs fail to capture the true semantics of the captions or the corresponding images, resulting in a lack of faithfulness. Second, the generated scene graphs have high inconsistency, with the same semantics represented by different annotations.\n\nTo address these challenges, we propose a novel dataset, which involves re-annotating the captions in Visual Genome (VG) using a new intermediate representation called FACTUAL-MR. FACTUAL-MR can be directly converted into faithful and consistent scene graph annotations. Our experimental results clearly demonstrate that the parser trained on our dataset outperforms existing approaches in terms of faithfulness and consistency. This improvement leads to a significant performance boost in both image caption evaluation and zero-shot image retrieval tasks. Furthermore, we introduce a novel metric for measuring scene graph similarity, which, when combined with the improved scene graph parser, achieves state-of-the-art (SOTA) results on multiple benchmark datasets for the aforementioned tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.398","authors":["Zhuang Li","Yuyang Chai","Terry Yue Zhuo","Lizhen Qu","Gholamreza Haffari","Fei Li","Donghong Ji","Quan Hung Tran"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-4_-resources-and-evaluation-(virtual-poster)"],"id":"P5079","is_paper":true,"keywords":["nlp datasets"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.398.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph Parsing","tldr":"Textual scene graph parsing has become increasingly important in various vision-language applications, including image caption evaluation and image retrieval. However, existing scene graph parsers that convert image captions into scene graphs often suffer from two types of errors. First, the generat...","track":"Resources and Evaluation","underline_id":78039,"underline_url":"https://underline.io/events/395/posters/15240/poster/78039-factual-a-benchmark-for-faithful-and-consistent-textual-scene-graph-parsing","video_url":null},{"abstract":"Recent multilingual models benefit from strong unified semantic representation models. However, due to conflict linguistic regularities, ignoring language-specific features during multilingual learning may suffer from negative transfer. In this work, we analyze the relation\nbetween a language's position space and its typological characterization, and suggest deploying different position spaces for different languages. We develop a position generation network which combines prior knowledge from typology features and existing position vectors. Experiments on the multilingual dependency parsing task show that the learned position vectors exhibit meaningful hidden structures, and they can help achieving the best multilingual parsing results.","anthology_url":"https://aclanthology.org/2023.findings-acl.854","authors":["Tao Ji","Yuanbin Wu","xiaoling Wang"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-1_-multilingualism-and-cross-lingual-nlp-(virtual-poster)"],"id":"P5095","is_paper":true,"keywords":["mutlilingual representations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.854.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Typology Guided Multilingual Position Representations: Case on Dependency Parsing","tldr":"Recent multilingual models benefit from strong unified semantic representation models. However, due to conflict linguistic regularities, ignoring language-specific features during multilingual learning may suffer from negative transfer. In this work, we analyze the relation\nbetween a language's posi...","track":"Multilingualism and Cross-Lingual NLP","underline_id":78041,"underline_url":"https://underline.io/events/395/posters/15200/poster/78041-typology-guided-multilingual-position-representations-case-on-dependency-parsing","video_url":null},{"abstract":"In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are designed in a quite complicated way or rely too much on external structures. In this paper, we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand but highly effective further pre-training task which logically strengthens the pre-trained models with the help of 6 types of logical indicators and a logically rich dataset LoGic Pre-training (LGP). IDOL achieves state-of-the-art performance on ReClor and LogiQA, the two most representative benchmarks in logical reasoning MRC, and is proven to be capable of generalizing to different pre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0 while keeping competitive general language understanding ability through testing on tasks in GLUE. Besides, at the beginning of the era of large language models, we take several of them like ChatGPT into comparison and find that IDOL still shows its advantage.","anthology_url":"https://aclanthology.org/2023.findings-acl.513","authors":["Zihang Xu","Ziqing Yang","Yiming Cui","Shijin Wang"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-1_-question-answering-(virtual-poster)"],"id":"P5097","is_paper":true,"keywords":["reading comprehension","logical reasoning","generalization","reasoning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.513.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78042/poster_document/b4b97be40330f1f72c6dad7114a59005.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning","tldr":"In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are desi...","track":"Question Answering","underline_id":78042,"underline_url":"https://underline.io/events/395/posters/15200/poster/78042-idol-indicator-oriented-logic-pre-training-for-logical-reasoning","video_url":null},{"abstract":"Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems. However, there is a dearth of sign language resources for the Indian sign language. This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language. We provide a detailed analysis of the dataset. To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation.","anthology_url":"https://aclanthology.org/2023.findings-acl.665","authors":["Abhinav Joshi","Susmit Agrawal","Ashutosh Modi"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-1_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P5125","is_paper":true,"keywords":["corpus creation"],"languages":["sign language"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.665.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78044/poster_document/eb9867792cce2e8b26fbb0f7151e7d97.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78044/slideshow/4754e3dec904a4977848ce7c65695654.pdf","title":"ISLTranslate: Dataset for Translating Indian Sign Language","tldr":"Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of ...","track":"Resources and Evaluation","underline_id":78044,"underline_url":"https://underline.io/events/395/posters/15200/poster/78044-isltranslate-dataset-for-translating-indian-sign-language","video_url":null},{"abstract":"In this paper, we propose an alternative to the classic masked language modeling (MLM) pre-training paradigm, where the objective is altered from the reconstruction of the exact identity of randomly selected masked subwords to the prediction of their latent semantic properties. We coin the proposed pre-training technique masked latent semantic modeling (MLSM for short). In order to make the contextualized determination of the latent semantic properties of the masked subwords possible, we rely on an unsupervised technique which uses sparse coding. \nOur experimental results reveal that the fine-tuned performance of those models that we pre-trained via MLSM is consistently and significantly better compared to the use of vanilla MLM pretraining and other strong baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.876","authors":["G\u00e1bor Berend"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-7_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P5135","is_paper":true,"keywords":["pre-training"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.876.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78046/poster_document/371a0964a8e4d8b348d2c2fbf6782a7a.pdf","preview_image":"https://assets.underline.io/lecture/78046/poster/6a3ba4bf27d4b55ae19c72e86896480b.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78046/slideshow/16b844b19e446606febef9a5c77d5822.pdf","title":"Masked Latent Semantic Modeling: an Efficient Pre-training Alternative to Masked Language Modeling","tldr":"In this paper, we propose an alternative to the classic masked language modeling (MLM) pre-training paradigm, where the objective is altered from the reconstruction of the exact identity of randomly selected masked subwords to the prediction of their latent semantic properties. We coin the proposed ...","track":"Large Language Models","underline_id":78046,"underline_url":"https://underline.io/events/395/posters/15279/poster/78046-masked-latent-semantic-modeling-an-efficient-pre-training-alternative-to-masked-language-modeling","video_url":null},{"abstract":"Syntactic structures used to play a vital role in natural language processing (NLP), but since the deep learning revolution, NLP has been gradually dominated by neural models that do not consider syntactic structures in their design. One vastly successful class of neural models is transformers. When used as an encoder, a transformer produces contextual representation of words in the input sentence. In this work, we propose a new model of contextual word representation, not from a neural perspective, but from a purely syntactic and probabilistic perspective. Specifically, we design a conditional random field that models discrete latent representations of all words in a sentence as well as dependency arcs between them; and we use mean field variational inference for approximate inference. Strikingly, we find that the computation graph of our model resembles transformers, with correspondences between dependencies and self-attention and between distributions over latent representations and contextual embeddings of words. Experiments show that our model performs competitively to transformers on small to medium sized datasets. We hope that our work could help bridge the gap between traditional syntactic and probabilistic approaches and cutting-edge neural approaches to NLP, and inspire more linguistically-principled neural approaches in the future.","anthology_url":"https://aclanthology.org/2023.findings-acl.482","authors":["Haoyi Wu","Kewei Tu"],"category":"Findings","demo_url":null,"display_track":"Syntax: Tagging, Chunking, and Parsing","event_ids":["session-1_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)"],"id":"P5140","is_paper":true,"keywords":["dependency parsing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.482.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78047/poster_document/1decfe958d767f8bdc3085396916f553.pdf","preview_image":"https://assets.underline.io/lecture/78047/poster/ce45a89b2a2c123d4e7191baebbc94d0.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78047/slideshow/ffc65968e2576c49c6e1c12887df10c7.pdf","title":"Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation","tldr":"Syntactic structures used to play a vital role in natural language processing (NLP), but since the deep learning revolution, NLP has been gradually dominated by neural models that do not consider syntactic structures in their design. One vastly successful class of neural models is transformers. When...","track":"Syntax: Tagging, Chunking, and Parsing","underline_id":78047,"underline_url":"https://underline.io/events/395/posters/15200/poster/78047-learning-joint-structural-and-temporal-contextualized-knowledge-embeddings-for-temporal-knowledge-graph-completion","video_url":null},{"abstract":"Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage. On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. To overcome these limitations, we build a new challenging benchmark named KoRC in this paper. Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format. Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions. Moreover, we use labels in knowledge bases rather than spans or choices as the final answers. We test state-of-the-art models on KoRC and the experimental results show that the strongest baseline only achieves 68.3\\% and 30.0\\% F1 measure in the IID and OOD test set, respectively. These results indicate that deep text understanding is still an unsolved challenge. We will release our dataset and baseline methods upon acceptance.","anthology_url":"https://aclanthology.org/2023.findings-acl.743","authors":["Zijun Yao","Yantao Liu","Xin Lv","Shulin Cao","Jifan Yu","Juanzi Li","Lei Hou"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-7_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P5150","is_paper":true,"keywords":["reading comprehension"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.743.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/78048/poster/0fe997e7a5868d433289269502a25ac3.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78048/slideshow/655cff05b72911d7199908edc335a6a4.pptx","title":"KoRC: Knowledge Oriented Reading Comprehension Benchmark for Deep Text Understanding","tldr":"Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of k...","track":"Question Answering","underline_id":78048,"underline_url":"https://underline.io/events/395/posters/15279/poster/78048-korc-knowledge-oriented-reading-comprehension-benchmark-for-deep-text-understanding","video_url":null},{"abstract":"Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer. According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different ways. However, the evaluation for QG usually depends on single reference-based similarity metrics, such as n-gram-based metric or learned metric, which is not sufficient to fully evaluate the potential of QG methods. To this end, we propose to paraphrase the reference question for a more robust QG evaluation. Using large language models such as GPT-3, we created semantically and syntactically diverse questions, then adopt the simple aggregation of the popular evaluation metrics as the final scores. Through our experiments, we found that using multiple (pseudo) references is more effective for QG evaluation while showing a higher correlation with human evaluations than evaluation with a single reference.","anthology_url":"https://aclanthology.org/2023.findings-acl.396","authors":["Shinhyeok Oh","Hyojun Go","Hyeongdon Moon","Yunsung Lee","Myeongho Jeong","Hyun Seung Lee","Seungtaek Choi"],"category":"Findings","demo_url":null,"display_track":"Generation","event_ids":["session-1_-generation-(virtual-poster)"],"id":"P5166","is_paper":true,"keywords":["automatic evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.396.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78051/poster_document/b0b5cfba7ce319524ed783db1a18856f.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78051/slideshow/56b12ba1f261cc5615e27a6993dc3e3a.pdf","title":"Evaluation of Question Generation Needs More References","tldr":"Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer. According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different way...","track":"Generation","underline_id":78051,"underline_url":"https://underline.io/events/395/posters/15200/poster/78051-evaluation-of-question-generation-needs-more-references","video_url":null},{"abstract":"Named Entity Recognition (NER) state-of-the-art methods requires high-quality labeled datasets. Issues such as scarcity of labeled data, under-representation of entities, and privacy concerns with using sensitive data for training, can be significant barriers. Generating synthetic data to train models is a promising solution to mitigate these problems. We propose ECG-QALM, a contextual question and answering approach using pre-trained language models to synthetically generate entity-controlled text. Generated text is then used to augment small labeled datasets for downstream NER tasks. We evaluate our method on two publicly available datasets.  We find ECG-QALM is capable of producing full text samples with desired entities appearing in a controllable way, while retaining sentence coherence closest to the real world data. Evaluations on NER tasks show significant improvements (75\\% - 140\\%) in low-labeled data regimes.","anthology_url":"https://aclanthology.org/2023.findings-acl.349","authors":["Karan Aggarwal","Henry Jin","Aitzaz Ahmad"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-7_-information-extraction-(virtual-poster)"],"id":"P5168","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.349.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78052/poster_document/72adbf40c7ca7fefb46241ad2a4e6e50.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"ECG-QALM: Entity-Controlled Synthetic Text Generation using Contextual Q&A for NER","tldr":"Named Entity Recognition (NER) state-of-the-art methods requires high-quality labeled datasets. Issues such as scarcity of labeled data, under-representation of entities, and privacy concerns with using sensitive data for training, can be significant barriers. Generating synthetic data to train mode...","track":"Information Extraction","underline_id":78052,"underline_url":"https://underline.io/events/395/posters/15279/poster/78052-ecg-qalm-entity-controlled-synthetic-text-generation-using-contextual-qanda-for-ner","video_url":null},{"abstract":"The U.S. Securities and Exchange Commission (SEC) mandates all public companies to file periodic financial statements that should contain numerals annotated with a particular label from a taxonomy. In this paper, we formulate the task of automating the assignment of a label to a particular numeral span in a sentence from an extremely large label set. Towards this task, we release a dataset, Financial Numeric Extreme Labelling (FNXL), annotated with 2,794 labels. We benchmark the performance of the FNXL dataset by formulating the task as (a) a sequence labelling problem and (b) a pipeline with span extraction followed by Extreme Classification. Although the two approaches perform comparably, the pipeline solution provides a slight edge for the least frequent labels.","anthology_url":"https://aclanthology.org/2023.findings-acl.219","authors":["Soumya Sharma","Subhendu Khatuya","Manjunath Hegde","Afreen Shaikh","Koustuv Dasgupta","Pawan Goyal","Niloy Ganguly"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P5188","is_paper":true,"keywords":["financial/business nlp"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.219.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Financial Numeric Extreme Labelling: A dataset and benchmarking","tldr":"The U.S. Securities and Exchange Commission (SEC) mandates all public companies to file periodic financial statements that should contain numerals annotated with a particular label from a taxonomy. In this paper, we formulate the task of automating the assignment of a label to a particular numeral s...","track":"NLP Applications","underline_id":78058,"underline_url":"https://underline.io/events/395/posters/15240/poster/78058-financial-numeric-extreme-labelling-a-dataset-and-benchmarking","video_url":null},{"abstract":"Large-scale vision-language pre-training has exhibited strong performance in various visual and textual understanding tasks. Recently, the textual encoders of multi-modal pre-trained models have been shown to generate high-quality textual representations, which often outperform models that are purely text-based, such as BERT. In this study, our objective is to utilize both textual and visual encoders of multi-modal pre-trained models to enhance language understanding tasks. We achieve this by generating an image associated with a textual prompt, thus enriching the representation of a phrase for downstream tasks. Results from experiments conducted on four benchmark datasets demonstrate that our proposed method, which leverages visually-enhanced text representations, significantly improves performance in the entity clustering task.","anthology_url":"https://aclanthology.org/2023.findings-acl.363","authors":["Tsu-Yuan Hsu","Chen-An Li","Chao-Wei Huang","Yun-Nung Chen"],"category":"Findings","demo_url":null,"display_track":"Speech and Multimodality","event_ids":["session-1_-speech-and-multimodality-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P5193","is_paper":true,"keywords":["multimodality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.363.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78059/poster_document/7e338bd9e355f7ff345a9f50aefa15d6.pdf","preview_image":"https://assets.underline.io/lecture/78059/poster/399f6901fc02264edfa42911426dc36d.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78059/slideshow/49dc123acb64050b18b387dba7f879e2.pdf","title":"Visually-Enhanced Phrase Understanding","tldr":"Large-scale vision-language pre-training has exhibited strong performance in various visual and textual understanding tasks. Recently, the textual encoders of multi-modal pre-trained models have been shown to generate high-quality textual representations, which often outperform models that are purel...","track":"Speech and Multimodality","underline_id":78059,"underline_url":"https://underline.io/events/395/posters/15200/poster/78059-visually-enhanced-phrase-understanding","video_url":null},{"abstract":"Abstract Meaning Representation (AMR) is a semantic representation that can enhance natural language generation (NLG) by providing a logical semantic input. In this paper, we propose the AMR-TST, an AMR-based text style transfer (TST) technique. The AMR-TST converts the source text to an AMR graph and generates the transferred text based on the AMR graph modified by a TST policy named style rewriting. Our method combines both the explainability and diversity of explicit and implicit TST methods. The experiments show that the proposed method achieves state-of-the-art results compared with other baseline models in automatic and human evaluations. The generated transferred text in qualitative evaluation proves the AMR-TST have significant advantages in keeping semantic features and reducing hallucinations. To the best of our knowledge, this work is the first to apply the AMR method focusing on node-level features to the TST task.","anthology_url":"https://aclanthology.org/2023.findings-acl.260","authors":["Kaize Shi","Xueyao Sun","LI HE","Dingxian Wang","Qing Li","Guandong Xu"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)"],"id":"P52","is_paper":true,"keywords":["knowledge graphs"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.260.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77257/poster_document/62449990d91141120a76c1f2a91e8e3c.pdf","preview_image":"https://assets.underline.io/lecture/77257/poster/809d95c810dc385235613d5259c455c4.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"AMR-TST: Abstract Meaning Representation-based Text Style Transfer","tldr":"Abstract Meaning Representation (AMR) is a semantic representation that can enhance natural language generation (NLG) by providing a logical semantic input. In this paper, we propose the AMR-TST, an AMR-based text style transfer (TST) technique. The AMR-TST converts the source text to an AMR graph a...","track":"NLP Applications","underline_id":77257,"underline_url":"https://underline.io/events/395/posters/15279/poster/77257-amr-tst-abstract-meaning-representation-based-text-style-transfer","video_url":null},{"abstract":"Commonsense about quantitative properties is essential for a deep understanding of texts containing numerals.\nHowever, naive language models (LMs) treat numerals as string tokens; therefore, they lack an understanding of the magnitudes of numerals, resulting in a difficulty in acquiring the commonsense.\nIn this study, we apply the $k$-nearest neighbor LM ($k$NN-LM) to the masked numeral prediction (MNP) task, which measures the quantitative commonsense of LMs.\n$k$NN-LM extends pre-trained neural LMs with the $k$-nearest neighbor ($k$NN) search.\nSince it can utilize patterns that appear in the datastore for prediction, we expect an improvement in numeral prediction accuracy, which is associated with a high rate of occurrence of out-of-vocabulary (OOV) words.\nThrough experiments, we verified that the retrieval-based method is effective for fine-grained predictions of numerals from context, especially for the OOV numerals.\nWe also compared two different context spans for context representations to improve the accuracy of $k$NN search by using only the words that are closely related to the masked numeral: the mask and its surrounding words, and the mask and its subsequent words.\nOur results reveal that using only the embeddings of mask tokens for numerals in $k$NN search is the most effective approach for realizing MNP tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.295","authors":["Taku Sakamoto","Akiko Aizawa"],"category":"Findings","demo_url":null,"display_track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","event_ids":["session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)"],"id":"P5255","is_paper":true,"keywords":["reasoning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.295.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78067/poster_document/41fd5b675c90c3fe71b0a8e9d69a9310.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Predicting Numerals in Text Using Nearest Neighbor Language Models","tldr":"Commonsense about quantitative properties is essential for a deep understanding of texts containing numerals.\nHowever, naive language models (LMs) treat numerals as string tokens; therefore, they lack an understanding of the magnitudes of numerals, resulting in a difficulty in acquiring the commonse...","track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","underline_id":78067,"underline_url":"https://underline.io/events/395/posters/15200/poster/78067-predicting-numerals-in-text-using-nearest-neighbor-language-models","video_url":null},{"abstract":"Prediction head is a crucial component of Transformer language models.\nDespite its direct impact on prediction, this component has often been overlooked in analyzing Transformers.\nIn this study, we investigate the inner workings of the prediction head, specifically focusing on bias parameters.\nOur experiments with BERT and GPT-2 models reveal that the biases in their word prediction heads play a significant role in the models' ability to reflect word frequency in a corpus, aligning with the logit adjustment method commonly used in long-tailed learning. \nWe also quantify the effect of controlling the biases in practical auto-regressive text generation scenarios;\nunder a particular setting, more diverse text can be generated without compromising text quality.","anthology_url":"https://aclanthology.org/2023.findings-acl.276","authors":["Goro Kobayashi","Tatsuki Kuribayashi","Sho Yokoi","Kentaro Inui"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P5289","is_paper":true,"keywords":["probing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.276.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/78073/poster/58af72333fa8680ac070f2afe0b0f96f.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78073/slideshow/2cbb1ea719bf9e624f5bcc069c083710.pdf","title":"Transformer Language Models Handle Word Frequency in Prediction Head","tldr":"Prediction head is a crucial component of Transformer language models.\nDespite its direct impact on prediction, this component has often been overlooked in analyzing Transformers.\nIn this study, we investigate the inner workings of the prediction head, specifically focusing on bias parameters.\nOur e...","track":"Interpretability and Analysis of Models for NLP","underline_id":78073,"underline_url":"https://underline.io/events/395/posters/15200/poster/78073-transformer-language-models-handle-word-frequency-in-prediction-head","video_url":null},{"abstract":"Entailment graphs (EGs) with predicates as nodes and entailment relations as edges are typically incomplete, while EGs in different languages are often complementary to each other. In this paper, we propose a new task, multilingual entailment graph enhancement, which aims to utilize the entailment information from one EG to enhance another EG in a different language. The ultimate goal is to obtain an enhanced EG containing richer and more accurate entailment information. We present an align-then-enhance framework (ATE) to achieve accurate multilingual entailment graph enhancement, which first exploits a cross-graph guided interaction mechanism to automatically discover potential equivalent predicates between different EGs and then constructs more accurate enhanced entailment graphs based on soft predicate alignments. Extensive experiments show that ATE achieves better and more robust predicate alignment results between different EGs, and the enhanced entailment graphs generated by ATE outperform the original graphs for entailment detection.","anthology_url":"https://aclanthology.org/2023.findings-acl.56","authors":["Yuting Wu","Yutong Hu","Yansong Feng","Tianyi Li","Mark Steedman","Dongyan Zhao"],"category":"Findings","demo_url":null,"display_track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","event_ids":["session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P5325","is_paper":true,"keywords":["textual entailment"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.56.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78075/poster_document/37021d692347c5b51054baebe15598c2.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78075/slideshow/94ad49552d68b8b2547bf92b2c078895.pdf","title":"Align-then-Enhance: Multilingual Entailment Graph Enhancement with Soft Predicate Alignment","tldr":"Entailment graphs (EGs) with predicates as nodes and entailment relations as edges are typically incomplete, while EGs in different languages are often complementary to each other. In this paper, we propose a new task, multilingual entailment graph enhancement, which aims to utilize the entailment i...","track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","underline_id":78075,"underline_url":"https://underline.io/events/395/posters/15279/poster/78075-align-then-enhance-multilingual-entailment-graph-enhancement-with-soft-predicate-alignment","video_url":null},{"abstract":"This paper studies the problem of open-domain question answering, with the aim of answering a diverse range of questions leveraging knowledge resources. Two types of sources, QA-pair and document corpora, have been actively leveraged with the following complementary strength. The former is highly precise when the paraphrase of given question q was seen and answered during training, often posed as a retrieval problem, while the latter generalizes better for unseen questions. A natural follow-up is thus leveraging both models, while a naive pipelining or integration approaches have failed to bring additional gains over either model alone. Our distinction is interpreting the problem as calibration, which estimates the confidence of predicted answers as an indicator to decide when to use a document or QA-pair corpus. The effectiveness of our method was validated on widely adopted benchmarks such as Natural Questions and TriviaQA.","anthology_url":"https://aclanthology.org/2023.findings-acl.401","authors":["Kyungjae Lee","Sang-eun Han","Seung-won Hwang","Moontae Lee"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)"],"id":"P5345","is_paper":true,"keywords":["open-domain qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.401.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78077/poster_document/88d736268e2c120ce0ab650dfc1e3a4f.pdf","preview_image":"https://assets.underline.io/lecture/78077/poster/58a9016521169dfe53d2dfa26742e8f1.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78077/slideshow/11e21750569bd5698e86faca18747e18.pptx","title":"When to Read Documents or QA History: On Unified and Selective Open-domain QA","tldr":"This paper studies the problem of open-domain question answering, with the aim of answering a diverse range of questions leveraging knowledge resources. Two types of sources, QA-pair and document corpora, have been actively leveraged with the following complementary strength. The former is highly pr...","track":"Question Answering","underline_id":78077,"underline_url":"https://underline.io/events/395/posters/15240/poster/78077-movie101-a-new-movie-understanding-benchmark","video_url":null},{"abstract":"Disfluencies commonly occur in conversational speech. Speech with disfluencies can result in noisy Automatic Speech Recognition (ASR) transcripts, which affects downstream tasks like machine translation. In this paper, we propose an adversarially-trained sequence-tagging model for Disfluency Correction (DC) that utilizes a small amount of labeled real disfluent data in conjunction with a large amount of unlabeled data. We show the benefit of our proposed technique, which crucially depends on synthetically generated disfluent data, by evaluating it for DC in three Indian languages- Bengali, Hindi, and Marathi (all from the Indo-Aryan family). Our technique also performs well in removing stuttering disfluencies in ASR transcripts introduced by speech impairments. We achieve an average 6.15 points improvement in F1-score over competitive baselines across all three languages mentioned. To the best of our knowledge, we are the first to utilize adversarial training for DC and use it to correct stuttering disfluencies in English, establishing a new benchmark for this task.","anthology_url":"https://aclanthology.org/2023.findings-acl.514","authors":["Vineet Bhat","Preethi Jyothi","Pushpak Bhattacharyya"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)"],"id":"P5357","is_paper":true,"keywords":["multilingualism"],"languages":["hindi","marathi","bengali"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.514.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78079/poster_document/232755191f5d60627106ef1db06de96c.pdf","preview_image":"https://assets.underline.io/lecture/78079/poster/7caeead24b72e51d0de3a14964d94141.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78079/slideshow/1f0784da19632686b47765bee00fd831.pdf","title":"Adversarial Training for Low-Resource Disfluency Correction","tldr":"Disfluencies commonly occur in conversational speech. Speech with disfluencies can result in noisy Automatic Speech Recognition (ASR) transcripts, which affects downstream tasks like machine translation. In this paper, we propose an adversarially-trained sequence-tagging model for Disfluency Correct...","track":"Multilingualism and Cross-Lingual NLP","underline_id":78079,"underline_url":"https://underline.io/events/395/posters/15279/poster/78079-adversarial-training-for-low-resource-disfluency-correction","video_url":null},{"abstract":"Different ways of linguistically expressing the same real-world event can lead to different perceptions of what happened. Previous work has shown that different descriptions of gender-based violence (GBV) influence the reader's perception of who is to blame for the violence, possibly reinforcing stereotypes which see the victim as partly responsible, too. As a contribution to raise awareness on perspective-based writing, and to facilitate access to alternative perspectives, we introduce the novel task of automatically rewriting GBV descriptions as a means to alter the perceived level of blame on the perpetrator. We present a quasi-parallel dataset of sentences with low and high perceived responsibility levels for the perpetrator, and experiment with unsupervised (mBART-based), zero-shot and few-shot (GPT3-based) methods for rewriting sentences. We evaluate our models using a questionnaire study and a suite of automatic metrics.","anthology_url":"https://aclanthology.org/2023.findings-acl.501","authors":["Gosse Minnema","Huiyuan Lai","Benedetta Muscato","Malvina Nissim"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P5374","is_paper":true,"keywords":["frame detection and analysis","nlp tools for social analysis"],"languages":["italian"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.501.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78080/poster_document/ff45e66196a46d5003836fa993ad73cb.pdf","preview_image":"https://assets.underline.io/lecture/78080/poster/e11ac04c4932f738966f2f4e0da0fa63.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78080/slideshow/edad50be17734c01a7b455600f2ab9e2.pdf","title":"Responsibility Perspective Transfer for Italian Femicide News","tldr":"Different ways of linguistically expressing the same real-world event can lead to different perceptions of what happened. Previous work has shown that different descriptions of gender-based violence (GBV) influence the reader's perception of who is to blame for the violence, possibly reinforcing ste...","track":"Computational Social Science and Cultural Analytics","underline_id":78080,"underline_url":"https://underline.io/events/395/posters/15279/poster/78080-responsibility-perspective-transfer-for-italian-femicide-news","video_url":null},{"abstract":"We study feature interactions in the context of feature attribution methods for post-hoc interpretability. \nIn interpretability research, getting to grips with feature interactions is increasingly recognised as an important challenge, because interacting features are key to the success of neural networks. Feature interactions allow a model to build up hierarchical representations for its input, and might provide an ideal starting point for the investigation into linguistic structure in language models.\nHowever, uncovering the exact role that these interactions play is also difficult, and a diverse range of interaction attribution methods has been proposed.\nIn this paper, we focus on the question which of these methods most faithfully reflects the inner workings of the target models.\nWe work out a grey box methodology, in which we train models to perfection on a formal language classification task, using PCFGs. \nWe show that under specific configurations, some methods are indeed able to uncover the grammatical rules acquired by a model. \nBased on these findings we extend our evaluation to a case study on language models, providing novel insights into the linguistic structure that these models have acquired.","anthology_url":"https://aclanthology.org/2023.findings-acl.554","authors":["Jaap Jumelet","Willem Zuidema"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P5382","is_paper":true,"keywords":["explanation faithfulness"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.554.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78082/poster_document/d7b8e3c960d1081e30f37fab32e172d9.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Feature Interactions Reveal Linguistic Structure in Language Models","tldr":"We study feature interactions in the context of feature attribution methods for post-hoc interpretability. \nIn interpretability research, getting to grips with feature interactions is increasingly recognised as an important challenge, because interacting features are key to the success of neural net...","track":"Interpretability and Analysis of Models for NLP","underline_id":78082,"underline_url":"https://underline.io/events/395/posters/15240/poster/78082-feature-interactions-reveal-linguistic-structure-in-language-models","video_url":null},{"abstract":"The task of product attribute value extraction is to identify values of an attribute from product information. Product attributes are important features, which help improve online shopping experience of customers, such as product search, recommendation and comparison. Most existing works only focus on extracting values for a set of known attributes with sufficient training data. However, with the emerging nature of e-commerce, new products with their unique set of new attributes are constantly generated from different retailers and merchants. Collecting a large number of annotations for every new attribute is costly and time consuming. Therefore, it is an important research problem for product attribute value extraction with limited data. In this work, we propose a novel prompt tuning approach with \\textbf{Mix}ed \\textbf{P}rompts for few-shot \\textbf{A}ttribute \\textbf{V}alue \\textbf{E}xtraction, namely MixPAVE. Specifically, MixPAVE introduces only a small amount (< 1\\%) of trainable parameters, i.e., a mixture of two learnable prompts, while keeping the existing extraction model frozen. In this way, MixPAVE not only benefits from parameter-efficient training, but also avoids model overfitting on limited training examples. Experimental results on two product benchmarks demonstrate the superior performance of the proposed approach over several state-of-the-art baselines. A comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach.","anthology_url":"https://aclanthology.org/2023.findings-acl.633","authors":["Li Yang","Qifan Wang","Jingang Wang","Xiaojun Quan","Fuli Feng","Yu Chen","Madian Khabsa","Sinong Wang","Zenglin Xu","Dongfang Liu"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)"],"id":"P541","is_paper":true,"keywords":["financial/business nlp"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.633.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77334/poster/0db0c1fbd017d0cb1b61408784989482.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77334/slideshow/8d180f65edcc308443092010cf59b085.pdf","title":"MixPAVE: Mix-Prompt Tuning for Few-shot Product Attribute Value Extraction","tldr":"The task of product attribute value extraction is to identify values of an attribute from product information. Product attributes are important features, which help improve online shopping experience of customers, such as product search, recommendation and comparison. Most existing works only focus ...","track":"NLP Applications","underline_id":77334,"underline_url":"https://underline.io/events/395/posters/15279/poster/77334-importance-of-synthesizing-high-quality-data-for-text-to-sql-parsing","video_url":null},{"abstract":"Text augmentation is an effective technique for addressing the problem of insufficient data in natural language processing. However, existing text augmentation methods tend to focus on few-shot scenarios and usually perform poorly on large public datasets. Our research indicates that existing augmentation methods often generate instances with shifted feature spaces, which leads to a drop in performance on the augmented data (for example, EDA generally loses approximately 2\\% in aspect-based sentiment classification). To address this problem, we propose a hybrid instance-filtering framework (BoostAug) based on pre-trained language models that can maintain a similar feature space with natural datasets. BoostAug is transferable to existing text augmentation methods (such as synonym substitution and back translation) and significantly improves the augmentation performance by 2-3\\% in classification accuracy. Our experimental results on three classification tasks and nine public datasets show that BoostAug addresses the performance drop problem and outperforms state-of-the-art text augmentation methods. Additionally, we release the code to help improve existing augmentation methods on large datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.105","authors":["Heng Yang","Ke Li"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)"],"id":"P5464","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.105.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78086/poster_document/c6457b1482510577a7761f050c0fa608.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Boosting Text Augmentation via Hybrid Instance Filtering Framework","tldr":"Text augmentation is an effective technique for addressing the problem of insufficient data in natural language processing. However, existing text augmentation methods tend to focus on few-shot scenarios and usually perform poorly on large public datasets. Our research indicates that existing augmen...","track":"Machine Learning for NLP","underline_id":78086,"underline_url":"https://underline.io/events/395/posters/15200/poster/78086-boosting-text-augmentation-via-hybrid-instance-filtering-framework","video_url":null},{"abstract":"Generative adversarial networks (GANs) and denoising diffusion probabilistic models (DDPMs) have recently achieved impressive performances in image and audio synthesis. After revisiting their success in conditional speech synthesis, we find that 1) GANs sacrifice sample diversity for quality and speed, 2) diffusion models exhibit outperformed sample quality and diversity at a high computational cost, where achieving high-quality, fast, and diverse speech synthesis challenges all neural synthesizers. In this work, we propose to converge advantages from GANs and diffusion models by incorporating both classes, introducing dual-empowered modeling perspectives: 1) FastDiff 2 (DiffGAN), a diffusion model whose denoising process is parametrized by conditional GANs, and the non-Gaussian denoising distribution makes it much more stable to implement the reverse process with large steps sizes; and 2) FastDiff 2 (GANDiff), a generative adversarial network whose forward process is constructed by multiple denoising diffusion iterations, which exhibits better sample diversity than traditional GANs. Experimental results show that both variants enjoy an efficient 4-step sampling process and demonstrate superior sample quality and diversity. Audio samples are available at https://RevisitSpeech.github.io/","anthology_url":"https://aclanthology.org/2023.findings-acl.437","authors":["Rongjie Huang","Yi Ren","Ziyue Jiang","Chenye Cui","Jinglin Liu","Zhou Zhao"],"category":"Findings","demo_url":null,"display_track":"Speech and Multimodality","event_ids":["session-1_-speech-and-multimodality-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P548","is_paper":true,"keywords":["spoken language understanding","speech and vision","speech technologies"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.437.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"FastDiff 2: Revisiting and Incorporating GANs and Diffusion Models in High-Fidelity Speech Synthesis","tldr":"Generative adversarial networks (GANs) and denoising diffusion probabilistic models (DDPMs) have recently achieved impressive performances in image and audio synthesis. After revisiting their success in conditional speech synthesis, we find that 1) GANs sacrifice sample diversity for quality and spe...","track":"Speech and Multimodality","underline_id":77335,"underline_url":"https://underline.io/events/395/posters/15200/poster/77335-fastdiff-2-revisiting-and-incorporating-gans-and-diffusion-models-in-high-fidelity-speech-synthesis","video_url":null},{"abstract":"The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant attention due to the growing importance of scientific and health discussions on online platforms. Automated scientific fact-checking methods based on NLP can help combat the spread of misinformation, assist researchers in knowledge discovery, and help individuals understand new scientific breakthroughs. In this paper, we present a comprehensive survey of existing research in this emerging field and its related tasks. We provide a task description, discuss the construction process of existing datasets, and analyze proposed models and approaches. Based on our findings, we identify intriguing challenges and outline potential future directions to advance the field.","anthology_url":"https://aclanthology.org/2023.findings-acl.387","authors":["Juraj Vladika","Florian Matthes"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-7_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P5486","is_paper":true,"keywords":["fact checking, rumour/misinformation detection"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.387.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78087/poster_document/01a0e7b14150a1fd8a98016abad4b4a0.pdf","preview_image":"https://assets.underline.io/lecture/78087/poster/7592327a9677e71cee6f3206d934684b.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Scientific Fact-Checking: A Survey of Resources and Approaches","tldr":"The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant a...","track":"NLP Applications","underline_id":78087,"underline_url":"https://underline.io/events/395/posters/15279/poster/78087-openrt-an-open-source-framework-for-reasoning-over-tabular-data","video_url":null},{"abstract":"Natural logic reasoning has received increasing attention lately, with several datasets and neural models proposed, though with limited success. More recently, a new class of works have emerged adopting a Neuro-Symbolic approach, called transformer guided chaining, whereby the idea is to iteratively perform 1-step neural inferences and chain together the results to generate a multi-step reasoning trace.  Several works have adapted variants of this central idea and reported significantly high accuracies compared to vanilla LLM's.  In this paper, we perform a critical empirical investigation of the chaining approach on a multi-hop First-Order Logic (FOL) reasoning benchmark. In particular, we develop a reference implementation, called Chainformer, and conduct several experiments to analyze the accuracy, generalization, interpretability, and performance over FOLs. Our findings highlight key strengths and possible current limitations and suggest potential areas for future research in logic reasoning.","anthology_url":"https://aclanthology.org/2023.findings-acl.588","authors":["Kanagasabai Rajaraman","Saravanan Rajamanickam","Wei Shi"],"category":"Findings","demo_url":null,"display_track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","event_ids":["session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)"],"id":"P549","is_paper":true,"keywords":["reasoning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.588.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77336/poster_document/61d9b07c236a21e9e25e48cd38fbe6c6.pdf","preview_image":"https://assets.underline.io/lecture/77336/poster/2af610b3ac69add036b421622c970556.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77336/slideshow/a8a05a3f2863832006d718d1db202e99.pdf","title":"Investigating Transformer-Guided Chaining for Interpretable Natural Logic Reasoning","tldr":"Natural logic reasoning has received increasing attention lately, with several datasets and neural models proposed, though with limited success. More recently, a new class of works have emerged adopting a Neuro-Symbolic approach, called transformer guided chaining, whereby the idea is to iteratively...","track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","underline_id":77336,"underline_url":"https://underline.io/events/395/posters/15200/poster/77336-investigating-transformer-guided-chaining-for-interpretable-natural-logic-reasoning","video_url":null},{"abstract":"Expressive text-to-speech aims to generate high-quality samples with rich and diverse prosody, which is hampered by \\textbf{dual challenges}: 1) prosodic attributes in highly dynamic voices are difficult to capture and model without intonation; and 2) highly multimodal prosodic representations cannot be well learned by simple regression (e.g., MSE) objectives, which causes blurry and over-smoothing predictions. This paper proposes Prosody-TTS, a two-stage pipeline that enhances \\textbf{prosody modeling and sampling} by introducing several components: 1) a self-supervised masked autoencoder to model the prosodic representation without relying on text transcriptions or local prosody attributes, which ensures to cover diverse speaking voices with superior generalization; and 2) a diffusion model to sample diverse prosodic patterns within the latent space, which prevents TTS models from generating samples with dull prosodic performance. Experimental results show that Prosody-TTS achieves new state-of-the-art in text-to-speech with natural and expressive synthesis. Both subjective and objective evaluation demonstrate that it exhibits superior audio quality and prosody naturalness with rich and diverse prosodic attributes. Audio samples are available at https://improved\\_prosody.github.io","anthology_url":"https://aclanthology.org/2023.findings-acl.508","authors":["Rongjie Huang","Chunlei Zhang","Yi Ren","Zhou Zhao","Dong Yu"],"category":"Findings","demo_url":null,"display_track":"Speech and Multimodality","event_ids":["session-4_-speech-and-multimodality-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P553","is_paper":true,"keywords":["spoken language understanding","speech and vision","speech technologies","multimodality"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.508.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Prosody-TTS: Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech","tldr":"Expressive text-to-speech aims to generate high-quality samples with rich and diverse prosody, which is hampered by \\textbf{dual challenges}: 1) prosodic attributes in highly dynamic voices are difficult to capture and model without intonation; and 2) highly multimodal prosodic representations canno...","track":"Speech and Multimodality","underline_id":77337,"underline_url":"https://underline.io/events/395/posters/15240/poster/77337-prosody-tts-improving-prosody-with-masked-autoencoder-and-conditional-diffusion-model-for-expressive-text-to-speech","video_url":null},{"abstract":"Existing knowledge-enhanced methods have achieved remarkable results in certain Q\\&A tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishment simultaneously. To address the challenge, we propose \\textbf{CPACE}, a \\textbf{C}oncept-centric \\textbf{P}rompt-b\\textbf{A}sed \\textbf{C}ontrastive \\textbf{E}xplanation Generation model, which aims to convert obtained symbolic knowledge into the contrastive explanation for better distinguishing the differences among given candidates. Firstly, following previous works, we retrieve different types of symbolic knowledge with a concept-centric knowledge extraction module. After that, we generate corresponding contrastive explanation using acquired symbolic knowledge and prompt as guidance for better modeling the knowledge distinguishment and interpretability. Finally, we regard the generated contrastive explanation as external knowledge for downstream task enhancement. We conduct a series of experiments on three widely-used question-answering datasets: CSQA, QASC, and OBQA. Experimental results demonstrate that with the help of generated contrastive explanation, our CPACE model achieves new SOTA on CSQA (89.8\\% on the testing set, 0.9\\% higher than human performance), and gains impressive improvement on QASC and OBQA (4.2\\% and 3.5\\%, respectively).","anthology_url":"https://aclanthology.org/2023.findings-acl.835","authors":["Qianglong Chen","Guohai Xu","Ming Yan","Ji Zhang","Fei Huang","Luo Si","Yin Zhang"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-4_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P5567","is_paper":true,"keywords":["commonsense qa"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.835.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering","tldr":"Existing knowledge-enhanced methods have achieved remarkable results in certain Q\\&A tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishme...","track":"Question Answering","underline_id":78090,"underline_url":"https://underline.io/events/395/posters/15240/poster/78090-distinguish-before-answer-generating-contrastive-explanation-as-knowledge-for-commonsense-question-answering","video_url":null},{"abstract":"The broad adoption of continuous prompts has brought state-of-the-art results on a diverse array of downstream natural language processing (NLP) tasks. Nonetheless, little attention has been paid to the interpretability and transferability of continuous prompts. Faced with the challenges, we investigate the feasibility of interpreting continuous prompts as the weighting of discrete prompts by jointly optimizing prompt fidelity and downstream fidelity. Our experiments show that: (1) one can always find a combination of discrete prompts as the replacement of continuous prompts that performs well on downstream tasks; (2) our interpretable framework faithfully reflects the reasoning process of source prompts; (3) our interpretations provide effective readability and plausibility, which is helpful to understand the decision-making of continuous prompts and discover potential shortcuts. Moreover, through the bridge constructed between continuous prompts and discrete prompts using our interpretations, it is promising to implement the cross-model transfer of continuous prompts without extra training signals. We hope this work will lead to a novel perspective on the interpretations of continuous prompts.","anthology_url":"https://aclanthology.org/2023.findings-acl.494","authors":["Tianjie Ju","Yubin Zheng","Hanyi Wang","Haodong Zhao","Gongshen Liu"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P5577","is_paper":true,"keywords":["probing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.494.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/78094/poster/f69d2c8487a41b96784ffd5a7fdd8e26.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78094/slideshow/875c1b706241d3ab5f5b8bc6be12401e.pptx","title":"Is Continuous Prompt a Combination of Discrete Prompts? Towards a Novel View for Interpreting Continuous Prompts","tldr":"The broad adoption of continuous prompts has brought state-of-the-art results on a diverse array of downstream natural language processing (NLP) tasks. Nonetheless, little attention has been paid to the interpretability and transferability of continuous prompts. Faced with the challenges, we investi...","track":"Interpretability and Analysis of Models for NLP","underline_id":78094,"underline_url":"https://underline.io/events/395/posters/15200/poster/78094-summary-oriented-vision-modeling-for-multimodal-abstractive-summarization","video_url":null},{"abstract":"Word-level Quality Estimation (QE) of Machine Translation (MT) aims to detect potential translation errors in the translated sentence without reference. Typically, conventional works on word-level QE are usually designed to predict the quality of translated words in terms of the post-editing effort, where the word labels in the dataset, i.e., \\texttt{OK} or \\texttt{BAD}, are automatically generated by comparing words between MT sentences and the post-edited sentences through a Translation Error Rate (TER) toolkit. While the post-editing effort can be used to measure the translation quality to some extent, we find it usually conflicts with human judgment on whether the word is well or poorly translated.  To investigate this conflict, we first create a golden benchmark dataset, namely \\emph{HJQE} (Human Judgement on Quality Estimation), where the source and MT sentences are identical to the original TER-based dataset and the expert translators directly annotate the poorly translated words on their judgments. Based on our analysis, we further propose  two tag-correcting strategies which can make the TER-based artificial QE corpus closer to \\emph{HJQE}. We conduct substantial experiments based on the publicly available WMT En-De and En-Zh corpora. The results not only show our proposed dataset is more consistent with human judgment but also confirm the effectiveness of the proposed tag-correcting strategies.{For reviewers, the corpora and codes can be found in the attached files.}","anthology_url":"https://aclanthology.org/2023.findings-acl.126","authors":["Zhen Yang","Fandong Meng","Yuanmeng Yan","Jie Zhou"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-1_-machine-translation-(virtual-poster)"],"id":"P5625","is_paper":true,"keywords":["automatic evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.126.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Rethinking the Word-level Quality Estimation for Machine Translation from Human Judgement","tldr":"Word-level Quality Estimation (QE) of Machine Translation (MT) aims to detect potential translation errors in the translated sentence without reference. Typically, conventional works on word-level QE are usually designed to predict the quality of translated words in terms of the post-editing effort,...","track":"Machine Translation","underline_id":78102,"underline_url":"https://underline.io/events/395/posters/15200/poster/78102-rethinking-the-word-level-quality-estimation-for-machine-translation-from-human-judgement","video_url":null},{"abstract":"We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidence jointly learns to identify: (i) the relevant evidences to the given claim (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way --- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to final verdict or to detect disagreeing evidence. Despite its interpretable nature, our system achieves results competetive with state-of-the-art on the FEVER dataset, as compared to typical two-stage system pipelines, while using significantly fewer parameters. Furthermore, our analysis shows that our model can learn fine-grained relevance cues while using coarse-grained supervision and we demonstrate it in 2 ways. (i) We show that our model can achieve competitive sentence recall while using only paragraph-level relevance supervision. (ii) Traversing towards the finest granularity of relevance, we show that our model is capable of identifying relevance at the token level. To do this, we present a new benchmark TLR-FEVER focusing on token-level interpretability ---  humans annotate tokens in relevant evidences they considered essential when making their judgment. Then we measure how similar are these annotations to the tokens our model is focusing on.","anthology_url":"https://aclanthology.org/2023.findings-acl.647","authors":["Martin Fajcik","Petr Motlicek","Pavel Smrz"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P5627","is_paper":true,"keywords":["knowledge tracing/discovering/inducing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.647.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78103/poster_document/3462ce6cd9de8d3cc0d731e50eb69141.pdf","preview_image":"https://assets.underline.io/lecture/78103/poster/922642e0de4c8325a888b22946aac4de.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78103/slideshow/97b64b159c93fe56090ef4074a8fd932.pdf","title":"Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction","tldr":"We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidence jointly learns to identify: (i) the relevant evidences to the given claim (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance...","track":"Interpretability and Analysis of Models for NLP","underline_id":78103,"underline_url":"https://underline.io/events/395/posters/15279/poster/78103-cold-start-data-selection-for-better-few-shot-language-model-fine-tuning-a-prompt-based-uncertainty-propagation-approach","video_url":null},{"abstract":"We propose SETI (Systematicity Evaluation of Textual Inference), a novel and comprehensive benchmark designed for evaluating pre-trained language models (PLMs) for their systematicity capabilities in the domain of textual inference. Specifically, SETI offers three different NLI tasks and corresponding datasets to evaluate various types of systematicity in reasoning processes. In order to solve these tasks, models are required to perform compositional inference based on known primitive constituents. We conduct experiments of SETI on six widely used PLMs. Results show that various PLMs are able to solve unseen compositional inferences when having encountered the knowledge of how to combine primitives, with good performance. However, they are considerably limited when this knowledge is unknown to the model (40-100 \\% points decrease). Furthermore, we find that PLMs are able to improve dramatically once exposed to crucial compositional knowledge in minimalistic shots. These findings position SETI as the first benchmark for measuring the future progress of PLMs in achieving systematicity generalization in the textual inference.","anthology_url":"https://aclanthology.org/2023.findings-acl.252","authors":["Xiyan Fu","Anette Frank"],"category":"Findings","demo_url":null,"display_track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","event_ids":["session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)"],"id":"P5631","is_paper":true,"keywords":["natural language inference"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.252.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78105/poster_document/a01ede03bd28b7b372ac66a39c7ccf7e.pdf","preview_image":"https://assets.underline.io/lecture/78105/poster/55f409d0ec993f69cb2d231f0035f75f.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"SETI: Systematicity Evaluation of Textual Inference","tldr":"We propose SETI (Systematicity Evaluation of Textual Inference), a novel and comprehensive benchmark designed for evaluating pre-trained language models (PLMs) for their systematicity capabilities in the domain of textual inference. Specifically, SETI offers three different NLI tasks and correspondi...","track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","underline_id":78105,"underline_url":"https://underline.io/events/395/posters/15200/poster/78105-reanalyzing-l2-preposition-learning-with-bayesian-mixed-effects-and-a-pretrained-language-model","video_url":null},{"abstract":"While Neural Machine Translation (NMT) has achieved great progress in recent years, it still suffers from inaccurate translation of entities (e.g., person/organization name, location), due to the lack of entity training instances. When we humans encounter an unknown entity during translation, we usually first look up in a dictionary and then organize the entity translation together with the translations of other parts to form a smooth target sentence. Inspired by this translation process, we propose an Extract-and-Attend approach to enhance entity translation in NMT, where the translation candidates of source entities are first extracted from a dictionary and then attended to by the NMT model to generate the target sentence. Specifically, the translation candidates are extracted by first detecting the entities in a source sentence and then translating the entities through looking up in a dictionary. Then, the extracted candidates are added as a prefix of the decoder input to be attended to by the decoder when generating the target sentence through self-attention. Experiments conducted on En-Zh and En-Ru demonstrate that the proposed method is effective on improving both the translation accuracy of entities and the overall translation quality, with up to 35\\% reduction on entity error rate and 0.85 gain on BLEU and 13.8 gain on COMET.","anthology_url":"https://aclanthology.org/2023.findings-acl.107","authors":["Zixin Zeng","Rui Wang","Yichong Leng","Junliang Guo","Shufang Xie","Xu Tan","Tao Qin","Tie-Yan Liu"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-7_-machine-translation-(virtual-poster)"],"id":"P5655","is_paper":true,"keywords":["modelling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.107.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78109/poster_document/668db0d56c8e9869824d35cebdf7a2e7.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78109/slideshow/dc59d0a09dc374c444d4ca6562188528.pdf","title":"Extract and Attend: Improving Entity Translation in Neural Machine Translation","tldr":"While Neural Machine Translation (NMT) has achieved great progress in recent years, it still suffers from inaccurate translation of entities (e.g., person/organization name, location), due to the lack of entity training instances. When we humans encounter an unknown entity during translation, we usu...","track":"Machine Translation","underline_id":78109,"underline_url":"https://underline.io/events/395/posters/15279/poster/78109-extract-and-attend-improving-entity-translation-in-neural-machine-translation","video_url":null},{"abstract":"Syntactic parsing is the task of assigning a syntactic structure to a sentence. There are two popular syntactic parsing methods: constituency and dependency parsing. Recent works have used syntactic embeddings based on constituency trees, incremental top-down parsing, and other word syntactic features for brain activity prediction given the text stimuli to study how the syntax structure is represented in the brain's language network. However, the effectiveness of dependency parse trees or the relative predictive power of the various syntax parsers across brain areas, especially for the listening task, is yet unexplored. In this study, we investigate the predictive power of the brain encoding models in three settings: (i) individual performance of the constituency and dependency syntactic parsing based embedding methods, (ii) efficacy of these syntactic parsing based embedding methods when controlling for basic syntactic signals, (iii) relative effectiveness of each of the syntactic embedding methods when controlling for the other. Further, we explore the relative importance of syntactic information (from these syntactic embedding methods) versus semantic information using BERT embeddings. We find that constituency parsers help explain activations in the temporal lobe and middle-frontal gyrus, while dependency parsers better encode syntactic structure in the angular gyrus and posterior cingulate cortex. Although semantic signals from BERT are more effective compared to any of the syntactic features or embedding methods, syntactic embedding methods explain additional variance for a few brain regions.","anthology_url":"https://aclanthology.org/2023.findings-acl.415","authors":["SUBBA REDDY OOTA","Mounika Marreddy","Manish Gupta","Raju Surampudi Bapi"],"category":"Findings","demo_url":null,"display_track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","event_ids":["session-1_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)"],"id":"P5658","is_paper":true,"keywords":["cognitive modeling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.415.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78110/poster_document/9b32287f739138cee6652e903a2329fe.pdf","preview_image":"https://assets.underline.io/lecture/78110/poster/6173863c7a245f294d5348dcb70699cd.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78110/slideshow/cc3d13df8ff34c0cd5f1577c1bc212a0.pdf","title":"How does the brain process syntactic structure while listening?","tldr":"Syntactic parsing is the task of assigning a syntactic structure to a sentence. There are two popular syntactic parsing methods: constituency and dependency parsing. Recent works have used syntactic embeddings based on constituency trees, incremental top-down parsing, and other word syntactic featur...","track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","underline_id":78110,"underline_url":"https://underline.io/events/395/posters/15200/poster/78110-self-supervised-sentence-polishing-by-adding-engaging-modifiers","video_url":null},{"abstract":"A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful has been to use evaluation-by-agreement -- if multiple methods agree on an explanation, its credibility increases. However, recent work has found that saliency methods exhibit weak rank correlations even when applied to the same model instance and advocated for alternative diagnostic methods. In our work, we demonstrate that rank correlation is not a good fit for evaluating agreement and argue that Pearson-r is a better-suited alternative. We further show that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods. By connecting our findings to instance categories based on training dynamics, we show that the agreement of saliency method explanations is very low for easy-to-learn instances. Finally, we connect the improvement in agreement across instance categories to local representation space statistics of instances, paving the way for work on analyzing which intrinsic model properties improve their predisposition to interpretability methods.","anthology_url":"https://aclanthology.org/2023.findings-acl.582","authors":["Josip Juki\u0107","Martin Tutek","Jan Snajder"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P5660","is_paper":true,"keywords":["explanation faithfulness","hardness of samples"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.582.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78111/poster_document/bd8c61bbe66d72c6c00e2f8652373e7b.pdf","preview_image":"https://assets.underline.io/lecture/78111/poster/de5f67748f3fc6501f94d2f31de94db5.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78111/slideshow/3aebf6e43b632d3617c814d051183c85.pdf","title":"Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods","tldr":"A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful has been to use evaluation-by-agreement -- if multiple me...","track":"Interpretability and Analysis of Models for NLP","underline_id":78111,"underline_url":"https://underline.io/events/395/posters/15200/poster/78111-easy-to-decide-hard-to-agree-reducing-disagreements-between-saliency-methods","video_url":null},{"abstract":"Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.213","authors":["Yuan He","Jiaoyan Chen","Ernesto Jimenez-Ruiz","Hang Dong","Ian Horrocks"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P5666","is_paper":true,"keywords":["probing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.213.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78112/poster_document/cfa6537f1197970c8e112448b7d17410.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Language Model Analysis for Ontology Subsumption Inference","tldr":"Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To in...","track":"Interpretability and Analysis of Models for NLP","underline_id":78112,"underline_url":"https://underline.io/events/395/posters/15240/poster/78112-language-model-analysis-for-ontology-subsumption-inference","video_url":null},{"abstract":"Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97\\% (56.59\\% up) and 48.34\\% (3.99\\% up) under pre-training and post-training attack defense respectively, achieving the new state-of-the-art performance on prediction recovery over four benchmark datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.561","authors":["Jiazhao Li","Zhuofeng Wu","Wei Ping","Chaowei Xiao","V.G.Vinod Vydiswaran"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P5667","is_paper":true,"keywords":["adversarial attacks/examples/training"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.561.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78113/poster_document/783a8598165723f3fcc194fa8c7ba52f.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78113/slideshow/7a208811bd36479e8cc3bfa3cd85ac6d.pdf","title":"Defending against Insertion-based Textual Backdoor Attacks via Attribution","tldr":"Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two...","track":"Interpretability and Analysis of Models for NLP","underline_id":78113,"underline_url":"https://underline.io/events/395/posters/15240/poster/78113-defending-against-insertion-based-textual-backdoor-attacks-via-attribution","video_url":null},{"abstract":"Diachronic Word Sense Induction (DWSI) is the task of inducing the temporal representations of a word meaning  from the context, as a set of senses and their prevalence over time. We introduce two new models for DWSI, based on topic modelling techniques: one is based on Hierarchical Dirichlet Processes (HDP), a nonparametric model; the other is based on the Dynamic Embedded Topic Model (DETM), a recent dynamic neural model. We evaluate these models against two state of the art DWSI models, using a time-stamped labelled dataset from the biomedical domain. We demonstrate that the two proposed models perform better than the state of the art. In particular, the HDP-based model drastically outperforms all the other models, including the dynamic neural model.","anthology_url":"https://aclanthology.org/2023.findings-acl.567","authors":["Ashjan Alsulaimani","Erwan Moreau"],"category":"Findings","demo_url":null,"display_track":"Semantics: Lexical","event_ids":["session-7_-semantics_-lexical-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P5670","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.567.pdf","paper_type":"findings","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/78114/poster/2d48f06afc698f84c2699d58f7f9414a.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Improving Diachronic Word Sense Induction with a Nonparametric Bayesian method","tldr":"Diachronic Word Sense Induction (DWSI) is the task of inducing the temporal representations of a word meaning  from the context, as a set of senses and their prevalence over time. We introduce two new models for DWSI, based on topic modelling techniques: one is based on Hierarchical Dirichlet Proces...","track":"Semantics: Lexical","underline_id":78114,"underline_url":"https://underline.io/events/395/posters/15279/poster/78114-improving-diachronic-word-sense-induction-with-a-nonparametric-bayesian-method","video_url":null},{"abstract":"Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average F1 score on both the XFUND (+4.6\\%~10.1\\%) and the Payment (+3.2\\%~9.5\\%) zero-shot benchmark, with a smaller model size and no additional image input.","anthology_url":"https://aclanthology.org/2023.findings-acl.255","authors":["Zifeng Wang","Zizhao Zhang","Jacob Devlin","Chen-Yu Lee","Guolong Su","Hao Zhang","Jennifer Dy","Vincent Perot","Tomas Pfister"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-7_-information-extraction-(virtual-poster)"],"id":"P5694","is_paper":true,"keywords":["named entity recognition and relation extraction","document-level extraction","zero/few-shot extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.255.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78117/poster_document/06ffa55517ce0b7993292643f8423b5c.pdf","preview_image":"https://assets.underline.io/lecture/78117/poster/911ef7ed39a1b56582e0d5e34b184731.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"QueryForm: A Simple Zero-shot Form Entity Query Framework","tldr":"Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion....","track":"Information Extraction","underline_id":78117,"underline_url":"https://underline.io/events/395/posters/15279/poster/78117-help-me-think-a-simple-prompting-strategy-for-non-experts-to-create-customized-content-with-models","video_url":null},{"abstract":"Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts. Yet, the intricate relation between embodiment and features such as concreteness or age of acquisition has not been studied in the context of figurative language interpretation concerning language models. Hence, the presented study shows how larger language models perform better at interpreting metaphoric sentences when the action of the metaphorical sentence is more embodied. The analysis rules out multicollinearity with other features (e.g. word length or concreteness) and provides initial evidence that larger language models conceptualise embodied concepts to a degree that facilitates figurative language understanding.","anthology_url":"https://aclanthology.org/2023.findings-acl.302","authors":["Philipp Wicke"],"category":"Findings","demo_url":null,"display_track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","event_ids":["session-7_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)"],"id":"P5698","is_paper":true,"keywords":["linguistic theories"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.302.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78118/poster_document/411fff3d08edf2a337003e3740c7b9ee.pdf","preview_image":"https://assets.underline.io/lecture/78118/poster/b5234fded2990b1b1d67509ab16b891a.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78118/slideshow/f0fb7933424a08a3771ba43d19f9e391.pdf","title":"LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models","tldr":"Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. ...","track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","underline_id":78118,"underline_url":"https://underline.io/events/395/posters/15279/poster/78118-lms-stand-their-ground-investigating-the-effect-of-embodiment-in-figurative-language-interpretation-by-language-models","video_url":null},{"abstract":"Language models have been shown to exhibit positive scaling, where performance improves as models are scaled up in terms of size, compute, or data. In this work, we introduce NeQA, a dataset consisting of questions with negation in which language models do not exhibit straightforward positive scaling. We show that this task can exhibit inverse scaling, U-shaped scaling, or positive scaling, and the three scaling trends shift in this order as we use more powerful prompting methods or model families. We hypothesize that solving NeQA depends on two subtasks: question answering (task 1) and negation understanding (task 2). We find that task 1 has linear scaling, while task 2 has sigmoid-shaped scaling with an emergent transition point, and composing these two scaling trends yields the final scaling trend of NeQA. Our work reveals and provides a way to analyze the complex scaling trends of language models.","anthology_url":"https://aclanthology.org/2023.findings-acl.472","authors":["Yuhui Zhang","Michihiro Yasunaga","Zhengping Zhou","Jeff Z. HaoChen","James Zou","Percy Liang","Serena Yeung"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-7_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P571","is_paper":true,"keywords":["scaling","interpretability/analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.472.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77338/poster_document/4b6e36edc7d7dabb156fce2bf35bd52b.pdf","preview_image":"https://assets.underline.io/lecture/77338/poster/c62b0a6517208ce0002efbb2442dd66c.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77338/slideshow/69afb2a2a8b1d5757f8d80c1305a1b38.pdf","title":"Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models","tldr":"Language models have been shown to exhibit positive scaling, where performance improves as models are scaled up in terms of size, compute, or data. In this work, we introduce NeQA, a dataset consisting of questions with negation in which language models do not exhibit straightforward positive scalin...","track":"Large Language Models","underline_id":77338,"underline_url":"https://underline.io/events/395/posters/15279/poster/77338-beyond-positive-scaling-how-negation-impacts-scaling-trends-of-language-models","video_url":null},{"abstract":"During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of human children.  Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some -- but not all -- learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language.","anthology_url":"https://aclanthology.org/2023.findings-acl.773","authors":["Linnea Evanson","Yair Lakretz","Jean R\u00e9mi king"],"category":"Findings","demo_url":null,"display_track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","event_ids":["session-7_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P5739","is_paper":true,"keywords":["linguistic theories","cognitive modeling","computational psycholinguistics"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.773.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78126/poster_document/fe2c9cfb6c3623b900988fe633c3b3cd.pdf","preview_image":"https://assets.underline.io/lecture/78126/poster/2457c0a72d582a3cadd04e2de0ea18af.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78126/slideshow/dfbad323166f668bb5e4749eff90af5c.pdf","title":"Language acquisition: do children and language models follow similar learning stages?","tldr":"During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning traj...","track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","underline_id":78126,"underline_url":"https://underline.io/events/395/posters/15279/poster/78126-language-acquisition-do-children-and-language-models-follow-similar-learning-stagesquestion","video_url":null},{"abstract":"Privatized text rewriting with local differential privacy (LDP) is a recent approach that enables sharing of sensitive textual documents while formally guaranteeing privacy protection to individuals. However, existing systems face several issues, such as formal mathematical flaws, unrealistic privacy guarantees, privatization of only individual words, as well as a lack of transparency and reproducibility. In this paper, we propose a new system `DP-BART' that largely outperforms existing LDP systems. Our approach uses a novel clipping method, iterative pruning, and further training of internal representations which drastically reduces the amount of noise required for DP guarantees. We run experiments on five textual datasets of varying sizes, rewriting them at different privacy guarantees and evaluating the rewritten texts on downstream text classification tasks. Finally, we thoroughly discuss the privatized text rewriting approach and its limitations, including the problem of the strict text adjacency constraint in the LDP paradigm that leads to the high noise requirement.","anthology_url":"https://aclanthology.org/2023.findings-acl.874","authors":["Timour Igamberdiev","Ivan Habernal"],"category":"Findings","demo_url":null,"display_track":"Ethics and NLP","event_ids":["session-7_-ethics-and-nlp-(virtual-poster)"],"id":"P5741","is_paper":true,"keywords":["ethical considerations in nlp applications"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.874.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78127/poster_document/8bbb0ff71a432a455094c2f74f84f40c.pdf","preview_image":"https://assets.underline.io/lecture/78127/poster/3ec48c7c5eb1337060014e5556643e4e.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78127/slideshow/22d52612ecc1841754e7c056d6dbf071.pdf","title":"DP-BART for Privatized Text Rewriting under Local Differential Privacy","tldr":"Privatized text rewriting with local differential privacy (LDP) is a recent approach that enables sharing of sensitive textual documents while formally guaranteeing privacy protection to individuals. However, existing systems face several issues, such as formal mathematical flaws, unrealistic privac...","track":"Ethics and NLP","underline_id":78127,"underline_url":"https://underline.io/events/395/posters/15279/poster/78127-dp-bart-for-privatized-text-rewriting-under-local-differential-privacy","video_url":null},{"abstract":"In a real-world dialogue system, generated text must be truthful and informative while remaining fluent and adhering to a prescribed style. Satisfying these constraints simultaneously is\ndifficult for the two predominant paradigms in language generation: neural language modeling and rule-based generation. We describe a hybrid architecture for dialogue response generation that combines the strengths of both paradigms. The first component of this architecture is a rule-based content selection model defined using a new formal framework called dataflow transduction, which uses declarative rules to transduce a dialogue agent's actions and their results (represented as dataflow graphs) into context-free grammars representing the space of contextually acceptable responses. The second component is a constrained decoding procedure that uses these grammars to constrain the output of a neural language model, which selects fluent utterances. Our experiments show that this system outperforms both rule-based and learned approaches in human evaluations of fluency, relevance, and truthfulness.","anthology_url":"https://aclanthology.org/2023.findings-acl.351","authors":["Hao Fang","Anusha Balakrishnan","Harsh Jhamtani","John Bufe","Jean Crawford","Jayant Krishnamurthy","Adam Pauls","Jason Eisner","Jacob Andreas","Dan Klein"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-7_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P5756","is_paper":true,"keywords":["grounded dialog"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.351.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78131/poster_document/61366487b178f0d1f2ef4964fc693fa0.pdf","preview_image":"https://assets.underline.io/lecture/78131/poster/5be0bb25d8be9b9f3c0797424d13cc6c.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78131/slideshow/f116cd8e308b08362994e70d30c4d3ce.pdf","title":"The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding","tldr":"In a real-world dialogue system, generated text must be truthful and informative while remaining fluent and adhering to a prescribed style. Satisfying these constraints simultaneously is\ndifficult for the two predominant paradigms in language generation: neural language modeling and rule-based gener...","track":"Dialogue and Interactive Systems","underline_id":78131,"underline_url":"https://underline.io/events/395/posters/15279/poster/78131-the-whole-truth-and-nothing-but-the-truth-faithful-and-controllable-dialogue-response-generation-with-dataflow-transduction-and-constrained-decoding","video_url":null},{"abstract":"Procedure planning, or the ability to predict a series of steps that can achieve a given goal conditioned on the current observation, is critical for building intelligent embodied agents that can assist users in everyday tasks. \n    Encouraged by the recent success of language models (LMs) for zero-shot and few-shot planning, we hypothesize that LMs may be equipped with stronger priors for planning compared to their visual counterparts.  \n     To this end, we propose a language-first procedure planning framework with a modularized design: we first align the current and goal observations with corresponding steps and then use a pre-trained LM to predict the intermediate steps. \n     Under this framework, we find that using an image captioning model for alignment can already match state-of-the-art performance and by designing a double retrieval model conditioned over current and goal observations jointly, we can achieve large improvements (19.2\\%-98.9\\% relatively higher success rate than state-of-the-art) on both COIN and CrossTask benchmarks. Our work verifies the planning ability of LMs and demonstrates how LMs can serve as a powerful ``reasoning engine'' even when the input is provided in another modality.","anthology_url":"https://aclanthology.org/2023.findings-acl.122","authors":["Jiateng Liu","Sha Li","Zhenhailong Wang","Manling Li","Heng Ji"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)"],"id":"P5758","is_paper":true,"keywords":["cross-modal application"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.122.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78132/poster_document/fdeadc2e07e37db2fb5a399932d5b1be.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"A Language-First Approach for Procedure Planning","tldr":"Procedure planning, or the ability to predict a series of steps that can achieve a given goal conditioned on the current observation, is critical for building intelligent embodied agents that can assist users in everyday tasks. \n    Encouraged by the recent success of language models (LMs) for zero-...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":78132,"underline_url":"https://underline.io/events/395/posters/15240/poster/78132-a-language-first-approach-for-procedure-planning","video_url":null},{"abstract":"Collecting labeled data for Named Entity Recognition (NER) tasks is challenging due to the high cost of manual annotations. Instead, researchers have proposed few-shot self-training and rule-augmentation techniques to minimize the reliance on large datasets. However, inductive biases and restricted logical language lexicon, respectively, can limit the ability of these models to perform well. In this work, we propose CoAug, a co-augmentation framework that allows us to improve few-shot models and rule-augmentation models by bootstrapping predictions from each model. By leveraging rules and neural model predictions to train our models, we complement the benefits of each and achieve the best of both worlds. In our experiments, we show that our best CoAug model can outperform strong weak-supervision-based NER models at least by 6.5 F1 points.","anthology_url":"https://aclanthology.org/2023.findings-acl.577","authors":["Rakesh R. Menon","Bingqing Wang","Jun Araki","Zhengyu Zhou","Zhe Feng","Liu Ren"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-7_-information-extraction-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P5809","is_paper":true,"keywords":["named entity recognition and relation extraction","zero/few-shot extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.577.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78138/poster_document/9a99dca5c870ed89a9656b9d557e3fef.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78138/slideshow/6a5915ddbf0872aa21bd248f0cfe27fc.pdf","title":"CoAug: Combining Augmentation of Labels and Labelling Rules","tldr":"Collecting labeled data for Named Entity Recognition (NER) tasks is challenging due to the high cost of manual annotations. Instead, researchers have proposed few-shot self-training and rule-augmentation techniques to minimize the reliance on large datasets. However, inductive biases and restricted ...","track":"Information Extraction","underline_id":78138,"underline_url":"https://underline.io/events/395/posters/15279/poster/78138-coaug-combining-augmentation-of-labels-and-labelling-rules","video_url":null},{"abstract":"Lexically-constrained NMT (LNMT) aims to incorporate user-provided terminology into translations. Despite its practical advantages, existing work has not evaluated LNMT models under challenging real-world conditions. In this paper, we focus on two important but understudied issues that lie in the current evaluation process of LNMT studies. The model needs to cope with challenging lexical constraints that are \"homographs\" or \"unseen\" during training. To this end, we first design a homograph disambiguation module to differentiate the meanings of homographs. Moreover, we propose PLUMCOT which integrates contextually rich information about unseen lexical constraints from pre-trained language models and strengthens a copy mechanism of the pointer network via direct supervision of a copying score. We also release HOLLY, an evaluation benchmark for assessing the ability of model to cope with \"homographic\" and \"unseen\" lexical constraints. Experiments on HOLLY and the previous test setup show the effectiveness of our method. The effects of PLUMCOT are shown to be remarkable in \"unseen\" constraints.Our dataset is available at https://github.com/papago-lab/HOLLY-benchmark.","anthology_url":"https://aclanthology.org/2023.findings-acl.298","authors":["Yujin Baek","Koanho Lee","Dayeon Ki","Cheonbok Park","Hyoung-Gyu Lee","Jaegul Choo"],"category":"Findings","demo_url":null,"display_track":"Machine Translation","event_ids":["session-4_-machine-translation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P5811","is_paper":true,"keywords":["automatic evaluation","interactive mt","modelling"],"languages":["korean"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.298.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78139/poster_document/2f61b7c2211c238aedacd9865dfe8988.pdf","preview_image":"https://assets.underline.io/lecture/78139/poster/92d532c58111c9e3762816bd5103794b.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78139/slideshow/820fac872d913487c6cdd64a20d51849.pdf","title":"Towards Accurate Translation via Semantically Appropriate Application of Lexical Constraints","tldr":"Lexically-constrained NMT (LNMT) aims to incorporate user-provided terminology into translations. Despite its practical advantages, existing work has not evaluated LNMT models under challenging real-world conditions. In this paper, we focus on two important but understudied issues that lie in the cu...","track":"Machine Translation","underline_id":78139,"underline_url":"https://underline.io/events/395/posters/15240/poster/78139-towards-accurate-translation-via-semantically-appropriate-application-of-lexical-constraints","video_url":null},{"abstract":"Pre-trained seq2seq models have achieved state-of-the-art results in the grammatical error correction task. However, these models still suffer from a prediction bias due to their unidirectional decoding. Thus, we propose a bidirectional Transformer reranker (BTR), that re-estimates the probability of each candidate sentence generated by the pre-trained seq2seq model. The BTR preserves the seq2seq-style Transformer architecture but utilizes a BERT-style self-attention mechanism in the decoder to compute the probability of each target token by using masked language modeling to capture bidirectional representations from the target context. For guiding the reranking, the BTR adopts negative sampling in the objective function to minimize the unlikelihood. During inference, the BTR gives final results after comparing the reranked top-1 results with the original ones by an acceptance threshold. Experimental results show that, in reranking candidates from a pre-trained seq2seq model, T5-base, the BTR on top of T5-base could yield 65.47 and 71.27 F0.5 scores on the CoNLL-14 and BEA test sets, respectively, and yield 59.52 GLEU score on the JFLEG corpus, with improvements of 0.36, 0.76 and 0.48 points compared with the original T5-base. Furthermore, when reranking candidates from T5-large, the BTR on top of T5-base improved the original T5-large by 0.26 points on the BEA test set.","anthology_url":"https://aclanthology.org/2023.findings-acl.234","authors":["Ying Zhang","Hidetaka Kamigaito","Manabu Okumura"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)"],"id":"P5815","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.234.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78140/poster_document/8eee498c15b5bfc91cd17fce39148b25.pdf","preview_image":"https://assets.underline.io/lecture/78140/poster/3facb58b027d0348a3bd655e6b161b1a.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78140/slideshow/4738ccd06cfec1f63bf8fec9e094e0c5.pdf","title":"Bidirectional Transformer Reranker for Grammatical Error Correction","tldr":"Pre-trained seq2seq models have achieved state-of-the-art results in the grammatical error correction task. However, these models still suffer from a prediction bias due to their unidirectional decoding. Thus, we propose a bidirectional Transformer reranker (BTR), that re-estimates the probability o...","track":"NLP Applications","underline_id":78140,"underline_url":"https://underline.io/events/395/posters/15240/poster/78140-bidirectional-transformer-reranker-for-grammatical-error-correction","video_url":null},{"abstract":"Data augmentation is an effective way to improve model performance of grammatical error correction (GEC). This paper identifies a critical side-effect of GEC data augmentation, which is due to the style discrepancy between the data used in GEC tasks (i.e., texts produced by non-native speakers) and data augmentation (i.e., native texts). To alleviate this issue, we propose to use an alternative data source, translationese (i.e., human-translated texts), as input for GEC data augmentation, which 1) is easier to obtain and usually has better quality than non-native texts, and 2) has a more similar style to non-native texts. Experimental results on the CoNLL14 and BEA19 English, NLPCC18 Chinese, Falko-MERLIN German, and RULEC-GEC Russian GEC benchmarks show that our approach consistently improves correction accuracy over strong baselines. Further analyses reveal that our approach is helpful for overcoming mainstream correction difficulties such as the corrections of frequent words, missing words, and substitution errors. Data, code, models and scripts are freely available at https://github.com/NLP2CT/TransGEC.","anthology_url":"https://aclanthology.org/2023.findings-acl.223","authors":["Tao Fang","Xuebo Liu","Derek F. Wong","Runzhe Zhan","Liang Ding","Lidia S. Chao","Dacheng Tao","Min Zhang"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-4_-nlp-applications-(virtual-poster)"],"id":"P5818","is_paper":true,"keywords":["educational applications, gec, essay scoring"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.223.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78141/poster_document/22540d79a5be2439773b5eaa5ad9e911.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"TransGEC: Improving Grammatical Error Correction with Translationese","tldr":"Data augmentation is an effective way to improve model performance of grammatical error correction (GEC). This paper identifies a critical side-effect of GEC data augmentation, which is due to the style discrepancy between the data used in GEC tasks (i.e., texts produced by non-native speakers) and ...","track":"NLP Applications","underline_id":78141,"underline_url":"https://underline.io/events/395/posters/15240/poster/78141-transgec-improving-grammatical-error-correction-with-translationese","video_url":null},{"abstract":"Pre-trained multilingual language models (PMLMs) are commonly used when dealing with data from multiple languages and cross-lingual transfer. However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many other languages. We explore to what extent this also applies to moral norms. Do the models capture moral norms from English and impose them on other languages? Do the models exhibit random and thus potentially harmful beliefs in certain languages? Both these issues could negatively impact cross-lingual transfer and potentially lead to harmful outcomes. In this paper, we (1) apply the MORALDIRECTION framework to multilingual models, comparing results in German, Czech, Arabic, Chinese, and English, (2) analyse model behaviour on filtered parallel subtitles corpora, and (3) apply the models to a Moral Foundations Questionnaire, comparing with human responses from different countries. Our experiments demonstrate that, indeed, PMLMs encode differing moral biases, but these do not necessarily correspond to cultural differences or commonalities in human opinions. We release our code and models.","anthology_url":"https://aclanthology.org/2023.findings-acl.134","authors":["Katharina Haemmerl","Bjoern Deiseroth","Patrick Schramowski","Jind\u0159ich Libovick\u00fd","Constantin A Rothkopf","Alexander Fraser","Kristian Kersting"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-1_-multilingualism-and-cross-lingual-nlp-(virtual-poster)"],"id":"P5822","is_paper":true,"keywords":["mutlilingual representations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.134.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78142/poster_document/86a42996c4de26b37f54f290d95c884f.pdf","preview_image":"https://assets.underline.io/lecture/78142/poster/5f56c1e3285e2c2cf3e247b4b1a2e6a6.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78142/slideshow/3dd413345b4768537beacf85047d06b7.pdf","title":"Speaking Multiple Languages Affects the Moral Bias of Language Models","tldr":"Pre-trained multilingual language models (PMLMs) are commonly used when dealing with data from multiple languages and cross-lingual transfer. However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many ot...","track":"Multilingualism and Cross-Lingual NLP","underline_id":78142,"underline_url":"https://underline.io/events/395/posters/15200/poster/78142-speaking-multiple-languages-affects-the-moral-bias-of-language-models","video_url":null},{"abstract":"Good datasets are a foundation of NLP research, and form the basis for training and evaluating models of language use.  While creating datasets, the standard practice is to verify the annotation consistency using a committee of human annotators.  This norm assumes that multiple annotators are available, which is not the case for highly specialized tasks or low-resource languages. In this paper, we ask: Can we evaluate the quality of a dataset constructed by a single human annotator? To address this question, we propose four weak verifiers to help estimate dataset quality, and outline when each may be employed.\nWe instantiate these strategies for the task of semantic analysis of adpositions in Gujarati, a low-resource language, and show that our weak verifiers concur with a double-annotation study.  As an added contribution, we also release the first dataset with semantic annotations in Gujarati along with several model baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.696","authors":["Maitrey Mehta","Vivek Srikumar"],"category":"Findings","demo_url":null,"display_track":"Linguistic Diversity","event_ids":["session-4_-linguistic-diversity-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P5830","is_paper":true,"keywords":["less-resourced languages","resources for less-resourced languages"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.696.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78143/poster_document/6378a4690de64fdd777220259f06e7cc.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78143/slideshow/e590ffc4ded6240f0eb288b6b2befbeb.pdf","title":"Verifying Annotation Agreement without Multiple Experts: A Case Study with Gujarati SNACS","tldr":"Good datasets are a foundation of NLP research, and form the basis for training and evaluating models of language use.  While creating datasets, the standard practice is to verify the annotation consistency using a committee of human annotators.  This norm assumes that multiple annotators are availa...","track":"Linguistic Diversity","underline_id":78143,"underline_url":"https://underline.io/events/395/posters/15240/poster/78143-verifying-annotation-agreement-without-multiple-experts-a-case-study-with-gujarati-snacs","video_url":null},{"abstract":"Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking model towards a goal in a zero-shot fashion by projecting the goal utterance and the corresponding reply candidates into a latent space. Here the cosine similarity indicates the distance/reachability of a candidate utterance toward the corresponding goal. Furthermore, we explore how these forward-entailing language representations can be utilized for assessing the likelihood of sequences by the entailment strength i.e. through the cosine similarity of its individual members (encoded separately) as an emergent property in the curved space. These non-local properties allow us to imagine the likelihood of future patterns in dialogues, specifically by ordering/identifying future goal utterances that are multiple turns away, given a dialogue context. As part of our analysis, we investigate characteristics that make conversations (un)plannable and find strong evidence of planning capability over multiple turns (in 61.56\\% over 3 turns) in conversations from the DailyDialog dataset. Finally, we show how we achieve higher efficiency in sequence modeling tasks compared to previous work thanks to our relativistic approach, where only the last utterance needs to be encoded and computed during inference.","anthology_url":"https://aclanthology.org/2023.findings-acl.319","authors":["Justus-Jonas Erker","Stefan Schaffer","Gerasimos Spanakis"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-4_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P5831","is_paper":true,"keywords":["conversational modeling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.319.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78144/poster_document/c61dcad877ec31d48d32b0fdc7d728ad.pdf","preview_image":"https://assets.underline.io/lecture/78144/poster/bd7ae30e2cbd4976122556d3d08b6f91.png","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning","tldr":"Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking mod...","track":"Dialogue and Interactive Systems","underline_id":78144,"underline_url":"https://underline.io/events/395/posters/15240/poster/78144-imagination-is-all-you-need-curved-contrastive-learning-for-abstract-sequence-modeling-utilized-on-long-short-term-dialogue-planning","video_url":null},{"abstract":"Experts across diverse disciplines are often interested in making sense of large text collections. Traditionally, this challenge is approached either by noisy unsupervised techniques such as topic models, or by following a manual theme discovery process. In this paper, we expand the definition of a theme to account for more than just a word distribution, and include generalized concepts deemed relevant by domain experts. Then, we propose an interactive framework that receives and encodes expert feedback at different levels of abstraction. Our framework strikes a balance between automation and manual coding, allowing experts to maintain control of their study while reducing the manual effort required.","anthology_url":"https://aclanthology.org/2023.findings-acl.313","authors":["Maria Leonor Pacheco","Tunazzina Islam","Lyle Ungar","Ming Yin","Dan Goldwasser"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)"],"id":"P5837","is_paper":true,"keywords":["nlp tools for social analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.313.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78145/poster_document/7b0ecb526f0c9925fe4426fa3fab0c79.pdf","preview_image":"https://assets.underline.io/lecture/78145/poster/54261262158a865716029ee366261b8c.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78145/slideshow/e13e97b9347093e9a88743f73ce12dc6.pdf","title":"Interactive Concept Learning for Uncovering Latent Themes in Large Text Collections","tldr":"Experts across diverse disciplines are often interested in making sense of large text collections. Traditionally, this challenge is approached either by noisy unsupervised techniques such as topic models, or by following a manual theme discovery process. In this paper, we expand the definition of a ...","track":"Computational Social Science and Cultural Analytics","underline_id":78145,"underline_url":"https://underline.io/events/395/posters/15200/poster/78145-interactive-concept-learning-for-uncovering-latent-themes-in-large-text-collections","video_url":null},{"abstract":"A key component of modern conversational systems is the Dialogue State Tracker (or DST), which models a user's goals and needs. Toward building more robust and reliable DSTs, we introduce a prompt-based learning approach to automatically generate effective adversarial examples to probe DST models. Two key characteristics of this approach are: (i) it only needs the output of the DST with no need for model parameters, and (ii) it can learn to generate natural language utterances that can target any DST. Through experiments over state-of-the-art DSTs, the proposed framework leads to the greatest reduction in accuracy and the best attack success rate while maintaining good fluency and a low perturbation ratio. We also show how much the generated adversarial examples can bolster a DST through adversarial training. These results indicate the strength of prompt-based attacks on DSTs and leave open avenues for continued refinement.","anthology_url":"https://aclanthology.org/2023.findings-acl.677","authors":["Xiangjue Dong","Yun He","Ziwei Zhu","James Caverlee"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P5841","is_paper":true,"keywords":["adversarial attacks/examples/training"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.677.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78146/slideshow/2a12ea36c1ca074ae03d015cf6b080ea.pdf","title":"PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts","tldr":"A key component of modern conversational systems is the Dialogue State Tracker (or DST), which models a user's goals and needs. Toward building more robust and reliable DSTs, we introduce a prompt-based learning approach to automatically generate effective adversarial examples to probe DST models. T...","track":"Interpretability and Analysis of Models for NLP","underline_id":78146,"underline_url":"https://underline.io/events/395/posters/15279/poster/78146-promptattack-probing-dialogue-state-trackers-with-adversarial-prompts","video_url":null},{"abstract":"Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20\\% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.","anthology_url":"https://aclanthology.org/2023.findings-acl.191","authors":["Hanlin Zhang","Jiani Huang","Ziyang Li","MAYUR NAIK","Eric Xing"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-7_-large-language-models-(virtual-poster)"],"id":"P5847","is_paper":true,"keywords":["interpretability/analysis"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.191.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78148/slideshow/cc72dc9233ef06d5ebac359a3941fd25.pdf","title":"Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming","tldr":"Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained L...","track":"Large Language Models","underline_id":78148,"underline_url":"https://underline.io/events/395/posters/15279/poster/78148-improved-logical-reasoning-of-language-models-via-differentiable-symbolic-programming","video_url":null},{"abstract":"Task-oriented dialog (TOD) agents often ground their responses on external knowledge bases (KBs). These KBs can be dynamic and may be updated frequently. Existing approaches for learning TOD agents assume the KB snapshot contemporary to each individual dialog is available during training. However, in real-world scenarios, only the latest KB snapshot is available during training and as a result, the train dialogs may contain facts conflicting with the latest KB. These dialog-KB inconsistencies in the training data may potentially confuse the TOD agent learning algorithm.\n\nIn this work, we define the novel problem of learning a TOD agent with dialog-KB inconsistencies in the training data. We propose a Dialog-KB Arbitration Framework (DKAF) which reduces the dialog-KB inconsistencies by predicting the contemporary KB snapshot for each train dialog. These predicted KB snapshots are then used for training downstream TOD agents. As there are no existing datasets with dialog-KB inconsistencies, we systematically introduce inconsistencies in two publicly available dialog datasets. We show that TOD agents trained with DKAF perform better than existing baselines on both these datasets.","anthology_url":"https://aclanthology.org/2023.findings-acl.744","authors":["Vishal Vivek Saley","Rocktim Jyoti Das","Dinesh Raghu","Mausam -"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-4_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P5856","is_paper":true,"keywords":["task-oriented"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.744.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78149/poster_document/94af4e3a778be4e63cc9b199305f4bd6.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with Dialog-KB Inconsistencies","tldr":"Task-oriented dialog (TOD) agents often ground their responses on external knowledge bases (KBs). These KBs can be dynamic and may be updated frequently. Existing approaches for learning TOD agents assume the KB snapshot contemporary to each individual dialog is available during training. However, i...","track":"Dialogue and Interactive Systems","underline_id":78149,"underline_url":"https://underline.io/events/395/posters/15240/poster/78149-dkaf-kb-arbitration-for-learning-task-oriented-dialog-systems-with-dialog-kb-inconsistencies","video_url":null},{"abstract":"Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a masked token, have recently shown comparable results to MLM, no objectives with a masking scheme actually outperform it in downstream tasks. Motivated by the assumption that their lack of complexity plays a vital role in the degradation, we validate whether more complex masked objectives can achieve better results and investigate how much complexity they should have to perform comparably to MLM. Our results using GLUE, SQuAD, and Universal Dependencies benchmarks demonstrate that more complicated objectives tend to show better downstream results with at least half of the MLM complexity needed to perform comparably to MLM. Finally, we discuss how we should pretrain a model using a masked objective from the task complexity perspective.","anthology_url":"https://aclanthology.org/2023.findings-acl.669","authors":["Atsuki Yamaguchi","Hiroaki Ozaki","Terufumi Morishita","Gaku Morio","Yasuhiro Sogawa"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-1_-large-language-models-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P590","is_paper":true,"keywords":["pre-training"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.669.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77341/poster_document/5ef374d2d65e246b8ed032ad1fda8bb3.pdf","preview_image":"https://assets.underline.io/lecture/77341/poster/5b95e0b08999a55a1cb748a6ae15790c.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77341/slideshow/3d0d29b48ffd9b6684d3d19aa1e3753d.pdf","title":"How does the task complexity of masked pretraining objectives affect downstream performance?","tldr":"Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a masked...","track":"Large Language Models","underline_id":77341,"underline_url":"https://underline.io/events/395/posters/15200/poster/77341-how-does-the-task-complexity-of-masked-pretraining-objectives-affect-downstream-performancequestion","video_url":null},{"abstract":"Relation prediction on knowledge graphs (KGs) attempts to infer the missing links  between entities.  Most previous studies are limited to the transductive setting where all entities must be seen during the training, making them unable to perform reasoning on emerging entities. Recently, the inductive setting is proposed to handle the entities in the test phase to be unseen during training, However, it suffers from the inefficient reasoning under the enclosing subgraph extraction issue and the lack of effective entity-independent feature modeling.  To this end, we propose a novel Query Adaptive Anchor Representation (QAAR)  model for inductive relation prediction. First, we extract one opening subgraph and perform reasoning by one time for all candidate triples, which is more efficient when the number of candidate triples is large. Second, we define some query adaptive anchors which are independent on any specific entity. Based on these anchors, we take advantage of the transferable entity-independent features  (relation-aware, structure-aware and distance features) that can be used to produce entity embeddings for emerging unseen entities. Such entity-independent features is modeled by a query-aware graph attention network on the opening subgraph.  Experimental results demonstrate that our proposed QAAR outperforms state-of-the-art baselines in inductive relation prediction task.","anthology_url":"https://aclanthology.org/2023.findings-acl.882","authors":["Zhiwen Xie","Yi Zhang","Jin Liu","Guangyou Zhou","Jimmy Xiangji Huang"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-1_-nlp-applications-(virtual-poster)"],"id":"P604","is_paper":true,"keywords":["knowledge graphs"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.882.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77343/poster_document/cb2c2eaefd28368b47facc099fc64dd7.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Learning Query Adaptive Anchor Representation for Inductive Relation Prediction","tldr":"Relation prediction on knowledge graphs (KGs) attempts to infer the missing links  between entities.  Most previous studies are limited to the transductive setting where all entities must be seen during the training, making them unable to perform reasoning on emerging entities. Recently, the inducti...","track":"NLP Applications","underline_id":77343,"underline_url":"https://underline.io/events/395/posters/15200/poster/77343-learning-query-adaptive-anchor-representation-for-inductive-relation-prediction","video_url":null},{"abstract":"Text-based personality computing (TPC) has gained many research interests in NLP. In this paper, we describe 15 challenges that we consider deserving the attention of the NLP research community. These challenges are organized by the following topics: personality taxonomies, measurement quality, datasets, performance evaluation, modelling choices, as well as ethics and fairness. When addressing each challenge, not only do we combine perspectives from both NLP and social sciences, but also offer concrete suggestions. We hope to inspire more valid and reliable TPC research.","anthology_url":"https://aclanthology.org/2023.findings-acl.691","authors":["Qixiang Fang","Anastasia Giachanou","Ayoub Bagheri","Laura Boeschoten","Erik-Jan van Kesteren","Mahdi Shafiee Kamalabad","Daniel Oberski"],"category":"Findings","demo_url":null,"display_track":"Computational Social Science and Cultural Analytics","event_ids":["session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P606","is_paper":true,"keywords":["psycho-demographic trait prediction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.691.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77344/poster_document/095624e268aafe26c2fd49a537c57f00.pdf","preview_image":"https://assets.underline.io/lecture/77344/poster/fbcba5f4677dab7af3203d82b08e588c.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77344/slideshow/de5d19d7ea746d05678daae3e0dcb5a4.pptx","title":"On Text-based Personality Computing: Challenges and Future Directions","tldr":"Text-based personality computing (TPC) has gained many research interests in NLP. In this paper, we describe 15 challenges that we consider deserving the attention of the NLP research community. These challenges are organized by the following topics: personality taxonomies, measurement quality, data...","track":"Computational Social Science and Cultural Analytics","underline_id":77344,"underline_url":"https://underline.io/events/395/posters/15279/poster/77344-peer-pre-training-electra-extended-by-ranking","video_url":null},{"abstract":"Chain-of-thought (CoT) prompting with large language models has proven effective in numerous natural language process tasks, but designing prompts that generalize well to diverse problem types can be challenging~CITATION, especially in the context of math word problem solving.\nAdditionally, it is common to have a large amount of training data that have a better diversity coverage but CoT annotations are not available, which limits the use of supervised learning techniques. \nTo address these issues, we investigate two approaches to leverage the training data in few-shot prompting scenario: \\textit{dynamic program prompting} and \\textit{program distillation}.\nOur approach is largely inspired by CITATION where they proposed to replace the CoT with the programs as the intermediate reasoning step. \nSuch a prompting strategy allows us to accurately verify the answer correctness through program execution in MWP solving.\nOur dynamic program prompting involves annotating the training data by sampling correct programs from a large language model, while program distillation involves adapting a smaller model to the program-annotated training data.\nOur experiments on three standard MWP datasets demonstrate the effectiveness of these approaches, yielding significant improvements over previous baselines for prompting and fine-tuning.\nOur results suggest that leveraging a large amount of training data can improve the generalization ability of prompts and boost the performance of fine-tuned smaller models in MWP solving.","anthology_url":"https://aclanthology.org/2023.findings-acl.668","authors":["Zhanming Jie","Wei Lu"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-4_-large-language-models-(virtual-poster)"],"id":"P645","is_paper":true,"keywords":["prompting"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.668.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning","tldr":"Chain-of-thought (CoT) prompting with large language models has proven effective in numerous natural language process tasks, but designing prompts that generalize well to diverse problem types can be challenging~CITATION, especially in the context of math word problem solving.\nAdditionally, it is co...","track":"Large Language Models","underline_id":77350,"underline_url":"https://underline.io/events/395/posters/15240/poster/77350-leveraging-training-data-in-few-shot-prompting-for-numerical-reasoning","video_url":null},{"abstract":"In recent years, the main focus of research on automatic readability assessment (ARA) has shifted towards using expensive deep learning-based methods with the primary goal of increasing models' accuracy. This, however, is rarely applicable for low-resource languages where traditional handcrafted features are still widely used due to the lack of existing NLP tools to extract deeper linguistic representations. In this work, we take a step back from the technical component and focus on how linguistic aspects such as mutual intelligibility or degree of language relatedness can improve ARA in a low-resource setting. We collect short stories written in three languages in the Philippines\u2014Tagalog, Bikol, and Cebuano\u2014to train readability assessment models and explore the interaction of data and features in various cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel specialized feature exploiting n-gram overlap applied to languages with high mutual intelligibility, significantly improves the performance of ARA models compared to the use of off-the-shelf large multilingual language models alone. Consequently, when both linguistic representations are combined, we achieve state-of-the-art results for Tagalog and Cebuano, and baseline scores for ARA in Bikol.","anthology_url":"https://aclanthology.org/2023.findings-acl.331","authors":["Joseph Marvin Imperial","Ekaterina Kochmar"],"category":"Findings","demo_url":null,"display_track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","event_ids":["session-7_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P657","is_paper":true,"keywords":["linguistic theories"],"languages":["tagalog","bikol","cebuano"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.331.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77352/poster_document/f0c4003f233dc66af7eb50dfa68341ff.pdf","preview_image":"https://assets.underline.io/lecture/77352/poster/ba868c938c485c5a3e5cdd8f51b9921c.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77352/slideshow/1935422c7babba14ed9784c5d9765443.pdf","title":"Automatic Readability Assessment for Closely Related Languages","tldr":"In recent years, the main focus of research on automatic readability assessment (ARA) has shifted towards using expensive deep learning-based methods with the primary goal of increasing models' accuracy. This, however, is rarely applicable for low-resource languages where traditional handcrafted fea...","track":"Linguistic Theories, Cognitive Modeling, and Psycholinguistics","underline_id":77352,"underline_url":"https://underline.io/events/395/posters/15279/poster/77352-automatic-readability-assessment-for-closely-related-languages","video_url":null},{"abstract":"Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.","anthology_url":"https://aclanthology.org/2023.findings-acl.67","authors":["Jie Huang","Kevin Chen-Chuan Chang"],"category":"Findings","demo_url":null,"display_track":"Theme: Reality Check","event_ids":["session-4_-theme_-reality-check-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P661","is_paper":true,"keywords":["ai hype & expectations"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.67.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77353/poster_document/d13730d5a7049b23613e43f6ac0d3c60.pdf","preview_image":"https://assets.underline.io/lecture/77353/poster/39711f091f455c732f9a6be9f3e0a849.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77353/slideshow/72171b1f6f6b61d9ae36996d0a76919a.pdf","title":"Towards Reasoning in Large Language Models: A Survey","tldr":"Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that ...","track":"Theme: Reality Check","underline_id":77353,"underline_url":"https://underline.io/events/395/posters/15240/poster/77353-towards-reasoning-in-large-language-models-a-survey","video_url":null},{"abstract":"Accounts of human language processing have long appealed to implicit \"situation models\u201d that enrich comprehension with relevant but unstated world knowledge. Here, we apply causal intervention techniques to recent transformer models to analyze performance on the Winograd Schema Challenge (WSC), where a single context cue shifts interpretation of an ambiguous pronoun. We identify a relatively small circuit of attention heads that are responsible for propagating information from the context word that guides which of the candidate noun phrases the pronoun ultimately attends to. We then compare how this circuit behaves in a closely matched \"syntactic\u201d control where the situation model is not strictly necessary. These analyses suggest a distinct pathway through which implicit situation models may be constructed to guide pronoun resolution","anthology_url":"https://aclanthology.org/2023.findings-acl.839","authors":["Takateru Yamakoshi","James L McClelland","Adele Goldberg","Robert Hawkins"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P664","is_paper":true,"keywords":["probing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.839.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77354/poster_document/a6164432999d8ad81175dd0850cd93c0.pdf","preview_image":"https://assets.underline.io/lecture/77354/poster/0a7ee84d1a5bcd597dea788d4b2561fa.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77354/slideshow/a38d4574998d992f281e842340f9c5d6.pdf","title":"Causal interventions expose implicit situation models for commonsense language understanding","tldr":"Accounts of human language processing have long appealed to implicit \"situation models\u201d that enrich comprehension with relevant but unstated world knowledge. Here, we apply causal intervention techniques to recent transformer models to analyze performance on the Winograd Schema Challenge (WSC), wher...","track":"Interpretability and Analysis of Models for NLP","underline_id":77354,"underline_url":"https://underline.io/events/395/posters/15200/poster/77354-causal-interventions-expose-implicit-situation-models-for-commonsense-language-understanding","video_url":null},{"abstract":"Task-oriented dialogues often require agents to enact complex, multi-step procedures in order to meet user requests. While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of task-specific data required for training. The following paper presents a data-efficient solution to constructing dialogue systems, leveraging explicit instructions derived from agent guidelines, such as company policies or customer service manuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a large language model with a knowledge retrieval module that pulls documents outlining relevant procedures from a predefined set of policies, given a user-agent interaction. To train this system, we introduce a semi-supervised pre-training scheme that employs dialogue-document matching and action-oriented masked language modeling with partial parameter freezing. We evaluate the effectiveness of our approach on prominent task-oriented dialogue datasets, Action-Based Conversations Dataset and Schema-Guided Dialogue, for two dialogue tasks: action state tracking and workflow discovery. Our results demonstrate that procedural knowledge augmentation improves accuracy predicting in- and out-of-distribution actions while preserving high performance in settings with low or sparse data.","anthology_url":"https://aclanthology.org/2023.findings-acl.182","authors":["Julia Isabel White","Arushi Raghuvanshi","Yada Pruksachatkun"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P705","is_paper":true,"keywords":["knowledge augmented"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.182.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77356/poster_document/30a77266dbec3b03400e3a8ff5010401.pdf","preview_image":"https://assets.underline.io/lecture/77356/poster/b252ac6eb6a5d25c22e9242ce1cfeeae.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77356/slideshow/85df958e31f5d6d01bdc792a6be4a85d.pdf","title":"Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction","tldr":"Task-oriented dialogues often require agents to enact complex, multi-step procedures in order to meet user requests. While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of task-spe...","track":"Dialogue and Interactive Systems","underline_id":77356,"underline_url":"https://underline.io/events/395/posters/15200/poster/77356-exploring-better-text-image-translation-with-multimodal-codebook","video_url":null},{"abstract":"The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually.\nWe propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels.","anthology_url":"https://aclanthology.org/2023.findings-acl.565","authors":["Sihao Chen","Senaka Buthpitiya","Alex Fabrikant","Dan Roth","Tal Schuster"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-1_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P707","is_paper":true,"keywords":["corpus creation","language resources","nlp datasets"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.565.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77358/poster_document/ad4ef76ef2c530ccf8b763d7951e996b.pdf","preview_image":"https://assets.underline.io/lecture/77358/poster/55d71e69d9ac672c8b22d145efa210d1.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77358/slideshow/aba5e68089b78eb43e37b506517dfa84.pdf","title":"PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition","tldr":"The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typical...","track":"Resources and Evaluation","underline_id":77358,"underline_url":"https://underline.io/events/395/posters/15200/poster/77358-exploring-how-generative-adversarial-networks-learn-phonological-representations","video_url":null},{"abstract":"Goal-oriented generative script learning aims to generate subsequent steps to reach a particular goal, which is an essential task to assist robots or humans in performing stereotypical activities. An important aspect of this process is the ability to capture historical states visually, which provides detailed information that is not covered by text and will guide subsequent steps. Therefore, we propose a new task, Multimedia Generative Script Learning, to generate subsequent steps by tracking historical states in both text and vision modalities, as well as presenting the first benchmark containing 5,652 tasks and 79,089 multimedia steps. This task is challenging in three aspects: the multimedia challenge of capturing the visual states in images, the induction challenge of performing unseen tasks, and the diversity challenge of covering different information in individual steps. We propose to encode visual state changes through a selective multimedia encoder to address the multimedia challenge, transfer knowledge from previously observed tasks using a retrieval-augmented decoder to overcome the induction challenge, and further present distinct information at each step by optimizing a diversity-oriented contrastive learning objective.  We define metrics to evaluate both generation and inductive quality. Experiment results demonstrate that our approach significantly outperforms strong baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.63","authors":["Qingyun Wang","Manling Li","Hou Pong Chan","Lifu Huang","Julia Hockenmaier","Girish Chowdhary","Heng Ji"],"category":"Findings","demo_url":null,"display_track":"Language Grounding to Vision, Robotics, and Beyond","event_ids":["session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P726","is_paper":true,"keywords":["cross-modal application"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.63.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77361/poster_document/b4640133decc3bf5b4f43011ed21178c.pdf","preview_image":"https://assets.underline.io/lecture/77361/poster/7df7ab03611a3dd7fab9cd08f73c5bea.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77361/slideshow/2cfae4a7a33e8517b12ee8253cb7176b.pdf","title":"Multimedia Generative Script Learning for Task Planning","tldr":"Goal-oriented generative script learning aims to generate subsequent steps to reach a particular goal, which is an essential task to assist robots or humans in performing stereotypical activities. An important aspect of this process is the ability to capture historical states visually, which provide...","track":"Language Grounding to Vision, Robotics, and Beyond","underline_id":77361,"underline_url":"https://underline.io/events/395/posters/15200/poster/77361-multimedia-generative-script-learning-for-task-planning","video_url":null},{"abstract":"Knowing exactly how many data points need to be labeled to achieve a certain model performance is a hugely beneficial step towards reducing the overall budgets for annotation. It pertains to both active learning and traditional data annotation, and is particularly beneficial for low resource scenarios. Nevertheless, it remains a largely under-explored area of research in NLP. We therefore explored various techniques for estimating the training sample size necessary to achieve a targeted performance value. We derived a simple yet effective approach to predict the maximum achievable model performance based on small amount of training samples -- which serves as an early indicator during data annotation for data quality and sample size determination. We performed ablation studies on four language understanding tasks, and showed that the proposed approach allows us to forecast model performance within a small margin of mean absolute error (~0.9\\%) with only 10\\% data.","anthology_url":"https://aclanthology.org/2023.findings-acl.419","authors":["Ernie Chang","Muhammad Hassan Rashid","Pin-Jie Lin","Changsheng Zhao","Vera Demberg","Yangyang Shi","Vikas Chandra"],"category":"Findings","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-7_-resources-and-evaluation-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P736","is_paper":true,"keywords":["automatic creation and evaluation of language resources"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.419.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Revisiting Sample Size Determination in Natural Language Understanding","tldr":"Knowing exactly how many data points need to be labeled to achieve a certain model performance is a hugely beneficial step towards reducing the overall budgets for annotation. It pertains to both active learning and traditional data annotation, and is particularly beneficial for low resource scenari...","track":"Resources and Evaluation","underline_id":77364,"underline_url":"https://underline.io/events/395/posters/15279/poster/77364-revisiting-sample-size-determination-in-natural-language-understanding","video_url":null},{"abstract":"The rapid development of aspect-based sentiment analysis (ABSA) within recent decades shows great potential for real-world society. The current ABSA works, however, are mostly limited to the scenario of a single text piece, leaving the study in dialogue contexts unexplored. To bridge the gap between fine-grained sentiment analysis and conversational opinion mining, in this work, we introduce a novel task of conversational aspect-based sentiment quadruple analysis, namely DiaASQ, aiming to detect the quadruple of target-aspect-opinion-sentiment in a dialogue. We manually construct a large-scale high-quality DiaASQ dataset in both Chinese and English languages. We deliberately develop a neural model to benchmark the task, which advances in effectively performing end-to-end quadruple prediction, and manages to incorporate rich dialogue-specific and discourse feature representations for better cross-utterance quadruple extraction. We hope the new benchmark will spur more advancements in the sentiment analysis community.","anthology_url":"https://aclanthology.org/2023.findings-acl.849","authors":["Bobo Li","Hao Fei","Fei Li","Yuhan Wu","Jinsong Zhang","Shengqiong Wu","Jingye Li","Yijiang Liu","Lizi Liao","Tat-Seng Chua","Donghong Ji"],"category":"Findings","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)"],"id":"P739","is_paper":true,"keywords":["argument mining"],"languages":["chinese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.849.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77366/poster_document/7581fd014ae29ae38db37ee66a6cad1d.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"DiaASQ: A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis","tldr":"The rapid development of aspect-based sentiment analysis (ABSA) within recent decades shows great potential for real-world society. The current ABSA works, however, are mostly limited to the scenario of a single text piece, leaving the study in dialogue contexts unexplored. To bridge the gap between...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":77366,"underline_url":"https://underline.io/events/395/posters/15279/poster/77366-back-translation-for-speech-to-text-translation-without-transcripts","video_url":null},{"abstract":"Recent work attributes progress in NLP to large language models (LMs) with increased model size and large quantities of pretraining data. Despite this, current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages. Additionally, previous work on pretrained Hebrew LMs focused on encoder-only models. While the encoder-only architecture is beneficial for classification tasks, it does not cater well for sub-word prediction tasks, such as Named Entity Recognition, when considering the morphologically rich nature of Hebrew. In this paper we argue that sequence-to-sequence generative architectures are more suitable for large LMs in morphologically rich languages (MRLs) such as Hebrew. We demonstrate this by casting tasks in the Hebrew NLP pipeline as text-to-text tasks, for which we can leverage powerful multilingual, pretrained sequence-to-sequence models as mT5, eliminating the need for a separate, specialized, morpheme-based, decoder. Using this approach, our experiments show substantial improvements over previously published results on all existing Hebrew NLP benchmarks. These results suggest that multilingual sequence-to-sequence models present a promising building block for NLP for MRLs.","anthology_url":"https://aclanthology.org/2023.findings-acl.487","authors":["Matan Eyal","Hila Noga","Roee Aharoni","Idan Szpektor","Reut Tsarfaty"],"category":"Findings","demo_url":null,"display_track":"Linguistic Diversity","event_ids":["session-4_-linguistic-diversity-(virtual-poster)"],"id":"P751","is_paper":true,"keywords":["less-resourced languages"],"languages":["hebrew"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.487.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77368/poster_document/9a0a0bf9d2b07896b3886bd736c4c510.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Multilingual Sequence-to-Sequence Models for Hebrew NLP","tldr":"Recent work attributes progress in NLP to large language models (LMs) with increased model size and large quantities of pretraining data. Despite this, current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages. Additionally, previous wo...","track":"Linguistic Diversity","underline_id":77368,"underline_url":"https://underline.io/events/395/posters/15240/poster/77368-multilingual-sequence-to-sequence-models-for-hebrew-nlp","video_url":null},{"abstract":"This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.","anthology_url":"https://aclanthology.org/2023.findings-acl.734","authors":["Xinze Li","Zhenghao Liu","Chenyan Xiong","Shi Yu","Yu Gu","Zhiyuan Liu","Ge Yu"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-1_-information-retrieval-and-text-mining-(virtual-poster)"],"id":"P784","is_paper":true,"keywords":["dense retrieval"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.734.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77370/poster_document/08588e8b4f42440837a5b92ed6e03893.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data","tldr":"This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for st...","track":"Information Retrieval and Text Mining","underline_id":77370,"underline_url":"https://underline.io/events/395/posters/15200/poster/77370-structure-aware-language-model-pretraining-improves-dense-retrieval-on-structured-data","video_url":null},{"abstract":"Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Hence, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of most part of the code, we resort to fuzz testing and propose fuzz tuning to boost the performance of program understanding and code representation learning, given a pre-trained LLM. The effectiveness of the proposed method is verified on two program understanding tasks including code clone detection and code classification, and it outperforms current state-of-the-arts by large margins. Code is available at https://github.com/rabbitjy/FuzzTuning.","anthology_url":"https://aclanthology.org/2023.findings-acl.678","authors":["Jianyu Zhao","Yuyang Rong","Yiwen Guo","Yifeng He","Hao Chen"],"category":"Findings","demo_url":null,"display_track":"NLP Applications","event_ids":["session-1_-nlp-applications-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P788","is_paper":true,"keywords":["code generation and understanding"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.678.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77371/poster_document/4f147f0072506d2b565c97969351ff87.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Understanding Programs by Exploiting (Fuzzing) Test Cases","tldr":"Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs ...","track":"NLP Applications","underline_id":77371,"underline_url":"https://underline.io/events/395/posters/15200/poster/77371-understanding-programs-by-exploiting-fuzzing-test-cases","video_url":null},{"abstract":"Pretrained language models have achieved remarkable success in various natural language processing tasks. However, pretraining has recently shifted toward larger models and larger data, which has resulted in significant computational and energy costs. In this paper, we propose Influence Subset Selection (ISS) for language model, which explicitly utilizes end-task knowledge to select a tiny subset of the pretraining corpus. Specifically, the ISS selects the samples that will provide the most positive influence on the performance of the end task. Furthermore, we design a gradient matching-based influence estimation method, which can drastically reduce the computation time of influence. With only 0.45\\% of the data and a three-orders-of-magnitude lower computational cost, ISS outperformed pretrained models (e.g., RoBERTa) on eight datasets covering four domains.","anthology_url":"https://aclanthology.org/2023.findings-acl.35","authors":["Xiao Wang","Weikang Zhou","Qi Zhang","Jie Zhou","SongYang Gao","Junzhe Wang","Menghan Zhang","Xiang Gao","Yun Wen Chen","Tao Gui"],"category":"Findings","demo_url":null,"display_track":"Large Language Models","event_ids":["session-1_-large-language-models-(virtual-poster)"],"id":"P795","is_paper":true,"keywords":["scaling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.35.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77372/poster_document/f7c8f5bf72b4401df77065bad2b05688.pdf","preview_image":"https://assets.underline.io/lecture/77372/poster/c6716a5c77639e352f546a75b5291675.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model","tldr":"Pretrained language models have achieved remarkable success in various natural language processing tasks. However, pretraining has recently shifted toward larger models and larger data, which has resulted in significant computational and energy costs. In this paper, we propose Influence Subset Selec...","track":"Large Language Models","underline_id":77372,"underline_url":"https://underline.io/events/395/posters/15200/poster/77372-farewell-to-aimless-large-scale-pretraining-influential-subset-selection-for-language-model","video_url":null},{"abstract":"This paper presents a schema-aware end-to-end neural network model for handling task-oriented dialogues based on a dynamic set of slots within a schema. Contrary to existing studies that proposed end-to-end approaches for task-oriented dialogue systems by relying on a unified schema across domains, we design our approach to support a domain covering multiple services where diverse schemas are available. To enable better generalizability among services and domains with different schemas, we supply the schema's context information including slot descriptions and value constraints to the model. The experimental results on a well-known Schema-Guided Dialogue (SGD) dataset demonstrated the performance improvement by the proposed model compared to state-of-the-art baselines in terms of end-to-end modeling, dialogue state tracking task, and generalization on new services and domains using a limited number of dialogues.","anthology_url":"https://aclanthology.org/2023.findings-acl.645","authors":["Wiradee Imrattanatrai","Ken Fukuda"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P809","is_paper":true,"keywords":["task-oriented"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.645.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77375/poster_document/11820a3dabd909a6ca988dbe1b034d94.pdf","preview_image":"https://assets.underline.io/lecture/77375/poster/fe1928cf1f9237985c8b3ed8da2f185c.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77375/slideshow/40769413470cd8abf2303373a7d2c797.pdf","title":"End-to-End Task-Oriented Dialogue Systems Based on Schema","tldr":"This paper presents a schema-aware end-to-end neural network model for handling task-oriented dialogues based on a dynamic set of slots within a schema. Contrary to existing studies that proposed end-to-end approaches for task-oriented dialogue systems by relying on a unified schema across domains, ...","track":"Dialogue and Interactive Systems","underline_id":77375,"underline_url":"https://underline.io/events/395/posters/15200/poster/77375-the-d-wise-tool-suite-multi-modal-machine-learning-powered-tools-supporting-and-enhancing-digital-discourse-analysis","video_url":null},{"abstract":"Hierarchical topic models have been employed to organize a large number of diverse topics from corpora into a latent tree structure. However, existing models yield fragmented topics with overlapping themes whose expected probability becomes exponentially smaller along the depth of the tree.\n\nTo solve this intrinsic problem, we propose a scale-invariant infinite hierarchical topic model (ihLDA). The ihLDA adaptively adjusts the topic creation to make the expected topic probability decay considerably slower than that in existing models. Thus, it facilitates the estimation of deeper topic structures encompassing diverse topics in a corpus. Furthermore, the ihLDA extends a widely used tree-structured prior (Adams et al., 2010) in a hierarchical Bayesian way, which enables drawing an infinite topic tree from the base tree while efficiently sampling the topic assignments for the words.\n\nExperiments demonstrate that the ihLDA has better topic uniqueness and hierarchical diversity than\nexisting approaches, including state-of-the-art neural models.","anthology_url":"https://aclanthology.org/2023.findings-acl.745","authors":["Shusei Eshima","Daichi Mochihashi"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)"],"id":"P844","is_paper":true,"keywords":["generative models"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.745.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77379/poster_document/75b12eb0058fbf86980f79288f2b51f6.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Scale-Invariant Infinite Hierarchical Topic Model","tldr":"Hierarchical topic models have been employed to organize a large number of diverse topics from corpora into a latent tree structure. However, existing models yield fragmented topics with overlapping themes whose expected probability becomes exponentially smaller along the depth of the tree.\n\nTo solv...","track":"Machine Learning for NLP","underline_id":77379,"underline_url":"https://underline.io/events/395/posters/15200/poster/77379-scale-invariant-infinite-hierarchical-topic-model","video_url":null},{"abstract":"This paper proposes a new task of commonsense question generation, which aims to yield deep-level and to-the-point questions from the text. Their answers need to reason over disjoint relevant contexts and external commonsense knowledge, such as encyclopedic facts and causality. The knowledge may not be explicitly mentioned in the text but is used by most humans for problem-shooting. Such complex reasoning with hidden contexts involves deep semantic understanding. Thus, this task has great application value, such as making high-quality quizzes in advanced exams. Due to the lack of modeling complexity, existing methods may produce shallow questions that can be answered by simple word matching. To address these challenges, we propose a new QG model by simultaneously considering asking contents, expressive ways, and answering complexity. We first retrieve text-related commonsense context. Then we disentangle the key factors that control questions in terms of reasoning content and verbalized way. Independence priors and constraints are imposed to facilitate disentanglement. We further develop a discriminator to promote the deep results by considering their answering complexity. Through adversarial inference, we learn the latent factors from data. By sampling the expressive factor from the data distributions, diverse questions can be yielded. Evaluations of two typical data sets show the effectiveness of our approach.","anthology_url":"https://aclanthology.org/2023.findings-acl.30","authors":["Jianxing Yu","Shiqi Wang","Libin Zheng","Qinliang Su","Wei Liu","Baoquan Zhao","Jian Yin"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-1_-question-answering-(virtual-poster)"],"id":"P872","is_paper":true,"keywords":["question generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.30.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77382/poster_document/a6641fcfb345d38ace4605dc45a2ae3e.pdf","preview_image":"https://assets.underline.io/lecture/77382/poster/eb3d2bac7ee1fe1d829753fcfebc6ff4.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77382/slideshow/2e3f17949fbc9378e6bd06ffb3f4cb38.pdf","title":"Generating Deep Questions with Commonsense Reasoning Ability from the Text by Disentangled Adversarial Inference","tldr":"This paper proposes a new task of commonsense question generation, which aims to yield deep-level and to-the-point questions from the text. Their answers need to reason over disjoint relevant contexts and external commonsense knowledge, such as encyclopedic facts and causality. The knowledge may not...","track":"Question Answering","underline_id":77382,"underline_url":"https://underline.io/events/395/posters/15200/poster/77382-context-aware-document-simplification","video_url":null},{"abstract":"The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models.\nIn this work, the transformer's inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models.\nExperiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. {Our code will be publicly available at {https://github.com/LUMIA-Group/FourierTransformer}}","anthology_url":"https://aclanthology.org/2023.findings-acl.570","authors":["Ziwei He","Meng Yang","Minwei Feng","Jingcheng Yin","Xinbing Wang","Jingwen Leng","Zhouhan Lin"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)"],"id":"P882","is_paper":true,"keywords":["representation learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.570.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77383/poster_document/456208778d6b3ac5c8c275c0d3828639.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator","tldr":"The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing ...","track":"Machine Learning for NLP","underline_id":77383,"underline_url":"https://underline.io/events/395/posters/15279/poster/77383-fourier-transformer-fast-long-range-modeling-by-removing-sequence-redundancy-with-fft-operator","video_url":null},{"abstract":"Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddings requires a full forward and backward pass over the entire model. We propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large model's parameters. New language-specific embeddings can then be efficiently trained over the mini-model and plugged into the aligned large model for rapid cross-lingual transfer. We explore two approaches to learn mini-models: MINIJOINT, which jointly pretrains the primary model and the mini-model using a single transformer with a secondary MLM head at a middle layer; and MINIPOST, where we start from a regular pretrained model, build a mini-model by extracting and freezing a few layers, and learn a small number of parameters on top. Experiments on XNLI, MLQA and PAWS-X show that mini-model adaptation matches the performance of the standard approach using up to 2.3x less compute on average.","anthology_url":"https://aclanthology.org/2023.findings-acl.338","authors":["Kelly Marchisio","Patrick Lewis","Yihong Chen","Mikel Artetxe"],"category":"Findings","demo_url":null,"display_track":"Multilingualism and Cross-Lingual NLP","event_ids":["session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P938","is_paper":true,"keywords":["cross-lingual transfer"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.338.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77387/poster_document/ccded1d1c2caa9ef56048ad4ea010d8d.pdf","preview_image":"https://assets.underline.io/lecture/77387/poster/f171390b9590162b461dac0c75a033df.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77387/slideshow/bb83eba5ea35390d4bbf9b1b0fa8bbcd.pdf","title":"Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training","tldr":"Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddin...","track":"Multilingualism and Cross-Lingual NLP","underline_id":77387,"underline_url":"https://underline.io/events/395/posters/15240/poster/77387-mini-model-adaptation-efficiently-extending-pretrained-models-to-new-languages-via-aligned-shallow-training","video_url":null},{"abstract":"Since conventional knowledge embedding models cannot take full advantage of the abundant textual information, there have been extensive research efforts in enhancing knowledge embedding using texts. However, existing enhancement approaches cannot apply to \\textit{temporal knowledge graphs} (tKGs), which contain time-dependent event knowledge with complex temporal dynamics. Specifically, existing enhancement approaches often assume knowledge embedding is time-independent. In contrast, the entity embedding in tKG models usually evolves, which poses the challenge of aligning \\textit{temporally relevant} texts with entities. To this end, we propose to study enhancing temporal knowledge embedding with textual data in this paper. As an approach to this task, we propose Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), which takes the temporal aspect into account and injects textual information into temporal knowledge embedding. To evaluate ECOLA, we introduce three new datasets for training and evaluating ECOLA. Extensive experiments show that ECOLA significantly enhances temporal KG embedding models with up to 287\\% relative improvements regarding Hits@1 on the link prediction task. The code and models are publicly available on https://github.com/mayhugotong/ECOLA.","anthology_url":"https://aclanthology.org/2023.findings-acl.335","authors":["Zhen Han","Ruotong Liao","Jindong Gu","Yao Zhang","Zifeng Ding","Yujia Gu","Heinz Koeppl","Hinrich Sch\u00fctze","Volker Tresp"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-7_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P941","is_paper":true,"keywords":["representation learning"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.335.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77388/poster_document/385eccab6d05c4eb49fcd44b3d147f6b.pdf","preview_image":"https://assets.underline.io/lecture/77388/poster/37b0620d2db76f55216599ddc15c5946.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77388/slideshow/7753ad9f8bed78d959afe75d0010c40f.pdf","title":"ECOLA: Enhancing Temporal Knowledge Embeddings with Contextualized Language Representations","tldr":"Since conventional knowledge embedding models cannot take full advantage of the abundant textual information, there have been extensive research efforts in enhancing knowledge embedding using texts. However, existing enhancement approaches cannot apply to \\textit{temporal knowledge graphs} (tKGs), w...","track":"Machine Learning for NLP","underline_id":77388,"underline_url":"https://underline.io/events/395/posters/15279/poster/77388-ecola-enhancing-temporal-knowledge-embeddings-with-contextualized-language-representations","video_url":null},{"abstract":"Existing dialogue models may encounter scenarios which are not well-represented in the training data, and as a result generate responses that are unnatural, inappropriate, or unhelpful.  We propose the \"Ask an Expert'' framework in which the model is trained with access to an \"expert'' which it can consult at each turn.  Advice is solicited via a structured dialogue with the expert, and the model is optimized to selectively utilize (or ignore) it given the context and dialogue history.  In this work the expert takes the form of an LLM.\nWe evaluate this framework in a mental health support domain, where the structure of the expert conversation is outlined by pre-specified prompts which reflect a reasoning strategy taught to practitioners in the field.  Blenderbot models utilizing \"Ask an Expert'' show quality improvements across all expert sizes, including those with fewer parameters than the dialogue model itself.  Our best model provides a ~10\\% improvement over baselines, approaching human-level scores on \"engingingness'' and \"helpfulness'' metrics.","anthology_url":"https://aclanthology.org/2023.findings-acl.417","authors":["Qiang Zhang","Jason Naradowsky","Yusuke Miyao"],"category":"Findings","demo_url":null,"display_track":"Dialogue and Interactive Systems","event_ids":["session-1_-dialogue-and-interactive-systems-(virtual-poster)"],"id":"P953","is_paper":true,"keywords":["task-oriented","knowledge augmented","commonsense reasoning","conversational modeling"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.417.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77389/poster_document/54793039a733f7173e5a3927b47f8e01.pdf","preview_image":"https://assets.underline.io/lecture/77389/poster/a6e04687477a94885500ae24b60a29e5.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77389/slideshow/9360a2d3c32433b9dd61ac1fec899f03.pdf","title":"Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models","tldr":"Existing dialogue models may encounter scenarios which are not well-represented in the training data, and as a result generate responses that are unnatural, inappropriate, or unhelpful.  We propose the \"Ask an Expert'' framework in which the model is trained with access to an \"expert'' which it can ...","track":"Dialogue and Interactive Systems","underline_id":77389,"underline_url":"https://underline.io/events/395/posters/15200/poster/77389-ask-an-expert-leveraging-language-models-to-improve-strategic-reasoning-in-goal-oriented-dialogue-models","video_url":null},{"abstract":"Humans can classify data of an unseen category by reasoning on its language explanations. This ability is owing to the compositional nature of language: we can combine previously seen attributes to describe the new category. For example, we might describe a sage thrasher as \"it has a slim straight relatively short bill, yellow eyes and a long tail\", so that others can use their knowledge of attributes \"slim straight relatively short bill\u201d, \"yellow eyes\u201d and \"long tail\u201d to recognize a sage thrasher. Inspired by this observation, in this work we tackle zero-shot classification task by logically parsing and reasoning on natural language explanations. To this end, we propose the framework CLORE (Classification by LOgical Reasoning on Explanations). While previous methods usually regard textual information as implicit features, CLORE parses explanations into logical structures and then explicitly reasons along this structure on the input to produce a classification score. Experimental results on explanation-based zero-shot classification benchmarks demonstrate that CLORE is superior to baselines, which we show is mainly due to higher scores on tasks requiring more logical reasoning. We also demonstrate that our framework can be extended to zero-shot classification on visual modality. Alongside classification decisions, CLORE can provide the logical parsing and reasoning process as a clear form of rationale. Through empirical analysis we demonstrate that CLORE is also less affected by linguistic biases than baselines.","anthology_url":"https://aclanthology.org/2023.findings-acl.571","authors":["Chi Han","Hengzhi Pei","Xinya Du","Heng Ji"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)"],"id":"P958","is_paper":true,"keywords":["knowledge-augmented methods","transfer learning / domain adaptation","generalization"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.571.pdf","paper_type":"findings","poster_pdf":null,"preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":null,"title":"Zero-Shot Classification by Logical Reasoning on Natural Language Explanations","tldr":"Humans can classify data of an unseen category by reasoning on its language explanations. This ability is owing to the compositional nature of language: we can combine previously seen attributes to describe the new category. For example, we might describe a sage thrasher as \"it has a slim straight r...","track":"Machine Learning for NLP","underline_id":77390,"underline_url":"https://underline.io/events/395/posters/15200/poster/77390-zero-shot-classification-by-logical-reasoning-on-natural-language-explanations","video_url":null},{"abstract":"Data scarcity has been the main factor that hinders the progress of event extraction. To overcome this issue, we propose a Self-Training with Feedback (STF) framework that leverages the large-scale unlabeled data and acquires feedback for each new event prediction from the unlabeled data by comparing it to the Abstract Meaning Representation (AMR) graph of the same sentence. Specifically, STF consists of (1) a base event extraction model trained on existing event annotations and then applied to large-scale unlabeled corpora to predict new event mentions as pseudo training samples, and (2) a novel scoring model that takes in each new predicted event trigger, an argument, its argument role, as well as their paths in the AMR graph to estimate a compatibility score indicating the correctness of the pseudo label. The compatibility scores further act as feedback to encourage or discourage the model learning on the pseudo labels during self-training. Experimental results on three benchmark datasets, including ACE05-E, ACE05-E+, and ERE, demonstrate the effectiveness of the STF framework on event extraction, especially event argument extraction, with significant performance gain over the base event extraction models and strong baselines. Our experimental analysis further shows that STF is a generic framework as it can be applied to improve most, if not all, event extraction models by leveraging large-scale unlabeled data, even when high-quality AMR graph annotations are not available.","anthology_url":"https://aclanthology.org/2023.findings-acl.662","authors":["Zhiyang Xu","Jay Yoon Lee","Lifu Huang"],"category":"Findings","demo_url":null,"display_track":"Information Extraction","event_ids":["session-4_-information-extraction-(virtual-poster)"],"id":"P982","is_paper":true,"keywords":["event extraction"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.662.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77391/poster_document/8e5da14920b93258ca05e3643cba1749.pdf","preview_image":"https://assets.underline.io/lecture/77391/poster/9a0dd6aac7e639c72f6018466d89e804.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77391/slideshow/e183986d0ccd1588e1ef84809bb4177c.pdf","title":"Learning from a Friend: Improving Event Extraction via Self-Training with Feedback from Abstract Meaning Representation","tldr":"Data scarcity has been the main factor that hinders the progress of event extraction. To overcome this issue, we propose a Self-Training with Feedback (STF) framework that leverages the large-scale unlabeled data and acquires feedback for each new event prediction from the unlabeled data by comparin...","track":"Information Extraction","underline_id":77391,"underline_url":"https://underline.io/events/395/posters/15240/poster/77391-learning-from-a-friend-improving-event-extraction-via-self-training-with-feedback-from-abstract-meaning-representation","video_url":null},{"abstract":"Question and answer generation (QAG) consists of generating a set of question-answer pairs given a context (e.g.\\ a paragraph). This task has a variety of applications, such as data augmentation for question answering (QA) models, information retrieval and education. In this paper, we establish baselines with three different QAG methodologies that leverage sequence-to-sequence language model (LM) fine-tuning. Experiments show that an end-to-end QAG model, which is computationally light at both training and inference times, is generally robust and outperforms other more convoluted approaches. However, there are differences depending on the underlying generative LM. Finally, our analysis shows that QA models fine-tuned solely on generated question-answer pairs can be competitive when compared to supervised QA models trained on human-labeled data.","anthology_url":"https://aclanthology.org/2023.findings-acl.899","authors":["Asahi Ushio","Fernando Alva-Manchego","Jose Camacho-Collados"],"category":"Findings","demo_url":null,"display_track":"Question Answering","event_ids":["session-1_-question-answering-(virtual-poster)","spotlight-session_-spotlight---metropolitan-east-(spotlight)"],"id":"P985","is_paper":true,"keywords":["question generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.899.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77392/poster_document/2edeb5065d3a87f7a9690cf7800dae10.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77392/slideshow/0fe7d1b42ae53daa3d9ddfa45ba629ca.pdf","title":"An Empirical Comparison of LM-based Question and Answer Generation Methods","tldr":"Question and answer generation (QAG) consists of generating a set of question-answer pairs given a context (e.g.\\ a paragraph). This task has a variety of applications, such as data augmentation for question answering (QA) models, information retrieval and education. In this paper, we establish base...","track":"Question Answering","underline_id":77392,"underline_url":"https://underline.io/events/395/posters/15200/poster/77392-an-empirical-comparison-of-lm-based-question-and-answer-generation-methods","video_url":null},{"abstract":"Hierarchical Text Classification (HTC) is an essential and challenging subtask of multi-label text classification with a taxonomic hierarchy. Recent advances in deep learning and pre-trained language models have led to significant breakthroughs in the HTC problem. However, despite their effectiveness, these methods are often restricted by a lack of domain knowledge, which leads them to make mistakes in a variety of situations. Generally, when manually classifying a specific document to the taxonomic hierarchy, experts make inference based on their prior knowledge and experience. For machines to achieve this capability, we propose a novel Knowledge-enabled Hierarchical Text Classification model (K-HTC), which incorporates knowledge graphs into HTC. Specifically, K-HTC innovatively integrates knowledge into both the text representation and hierarchical label learning process, addressing the knowledge limitations of traditional methods. Additionally, a novel knowledge-aware contrastive learning strategy is proposed to further exploit the information inherent in the data. Extensive experiments on two publicly available HTC datasets show the efficacy of our proposed method, and indicate the necessity of incorporating knowledge graphs in HTC tasks.","anthology_url":"https://aclanthology.org/2023.findings-acl.358","authors":["Ye Liu","Kai Zhang","Zhenya Huang","Kehang Wang","Yanghai Zhang","Qi Liu","Enhong Chen"],"category":"Findings","demo_url":null,"display_track":"Information Retrieval and Text Mining","event_ids":["session-1_-information-retrieval-and-text-mining-(virtual-poster)"],"id":"P986","is_paper":true,"keywords":["document representation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.358.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77393/poster_document/6dddd32a5ed6c36dededb5e601eea82f.pdf","preview_image":"https://assets.underline.io/lecture/77393/poster/a92945c26b82b7bc3f33016aa45f5154.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77393/slideshow/a172371cac1a1f917076f5f7cd5639bd.pdf","title":"Enhancing Hierarchical Text Classification through Knowledge Graph Integration","tldr":"Hierarchical Text Classification (HTC) is an essential and challenging subtask of multi-label text classification with a taxonomic hierarchy. Recent advances in deep learning and pre-trained language models have led to significant breakthroughs in the HTC problem. However, despite their effectivenes...","track":"Information Retrieval and Text Mining","underline_id":77393,"underline_url":"https://underline.io/events/395/posters/15200/poster/77393-enhancing-hierarchical-text-classification-through-knowledge-graph-integration","video_url":null}]
