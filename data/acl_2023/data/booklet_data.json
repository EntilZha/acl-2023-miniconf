{"plenaries": [{"id": "two-paths-to-intelligence", "title": "Two Paths to Intelligence", "desc": "Keynote: Geoffrey Hinton\nCohere\n\nMonday, July 10 - Time: 09:30\u201310:30 EDT\n\nAbstract: I will briefly describe the forty year history of neural net\nlanguage models with particular attention to whether they understand\nwhat they are saying. I will then discuss some of the main differences\nbetween digital and biological intelligences and speculate on how the\nbrain could implement something like transformers. I will conclude by\naddressing the contentious issue of whether current multimodal LLMs have\nsubjective experience.\n", "location": "Metropolitan", "start_time": "2023-07-10T09:30:00", "end_time": "2023-07-10T10:30:00", "bio": " Geoffrey Hinton received his PhD in Artificial Intelligence from\nEdinburgh in 1978. After five years as a faculty member at\nCarnegie-Mellon he became a fellow of the Canadian Institute for\nAdvanced Research and moved to the University of Toronto where he is now\nan emeritus professor. He is also the Chief Scientific Adviser at the\nVector Institute.\n\nHe was one of the researchers who introduced the backpropagation\nalgorithm and the first to use backpropagation for learning word\nembeddings. His other contributions to neural network research include\nBoltzmann machines, distributed representations, time-delay neural nets,\nmixtures of experts, variational learning and deep learning. His\nresearch group in Toronto made major breakthroughs in deep learning that\nrevolutionized speech recognition and object classification.\n\nHe is a fellow of the UK Royal Society and a foreign member of the US\nNational Academy of Engineering, the US National Academy of Sciences and\nthe American Academy of Arts and Sciences. His awards include the David\nE. Rumelhart prize, the IJCAI award for research excellence, the Killam\nprize for Engineering, the Royal Society Royal Medal, the NSERC Herzberg\nGold Medal, the IEEE James Clerk Maxwell Gold medal, the NEC C&C award,\nthe BBVA award, the Honda Prize and the Turing Award.\n", "speaker_name": "Geoffrey Hinton", "institution": "University of Toronto (emeritus)", "image": "invited1.jpg"}, {"id": "large-language-models-as-cultural-technologies_-imitation-and-innovation-in-children-and-models", "title": "Large Language Models as Cultural Technologies: Imitation and Innovation in Children and Models", "desc": "Alison Gopnik\nUniversity of California, Berkeley\n\nWednesday, July 12 - Time: 14:00\u201315:00 EDT\n\nAbstract: Its natural to ask whether large language models like LaMDA or\nGPT-3 are intelligent agents. But I argue that this is the wrong\nquestion. Intelligence and agency are the wrong categories for\nunderstanding them. Instead, these Al systems are what we might call\ncultural technologies, like writing, print, libraries, internet search\nengines or even language itself. They are new techniques for passing on\ninformation from one group of people to another. Cultural technologies\narent like intelligent humans, but they are essential for human\nintelligence. Many animals can transmit some information from one\nindividual or one generation to another, but no animal does it as much\nas we do or accumulates as much information over time, . New\ntechnologies that make cultural transmission easier and more effective\nhave been among the greatest engines of human progress, but they have\nalso led to negative as well as positive social consequences. Moreover,\nwhile cultural technologies allow transmission of existing information\ncultural evolution, which is central to human success, also depends on\ninnovation, exploration and causal learning. Comparing LLM\u2019s responses\nin prompts based on developmental psychology experiments to the\nresponses of children may provide insight into which capacities can be\nlearned through language and cultural transmission, and which require\ninnovation and exploration in the physical world. I will present results\nfrom several studies making such comparisons.\n", "location": "Metropolitan", "start_time": "2023-07-12T14:00:00", "end_time": "2023-07-12T15:00:00", "bio": " Alison Gopnik is a professor of psychology and affiliate professor\nof philosophy at the University of California at Berkeley, and a member\nof the Berkeley AI Research Group. She received her BA from McGill\nUniversity and her PhD. from Oxford University. She is a leader in the\nstudy of cognitive science and of children\u2019s learning and development\nand was one of the founders of the field of \u201ctheory of mind\u201d, an\noriginator of the \u201ctheory of cognitive development\u201d, and the first to\napply Bayesian probabilistic models to children\u2019s learning. She has\nreceived both the APS Lifetime Achievement Cattell and William James\nAwards, the Bradford Washburn Award for Science Communication, and the\nSRCD Lifetime Achievement Award for Basic Science in Child Development.\nShe is an elected member of the Society of Experimental Psychologists\nand the American Academy of Arts and Sciences and a Cognitive Science\nSociety, American Association for the Advancement of Science, and\nGuggenheim Fellow. She was 2022-23 President of the Association for\nPsychological Science.\n\nShe is the author or coauthor of over 140 journal articles and several\nbooks including \u201cWords, thoughts and theories\u201d MIT Press, 1997, and the\nbestselling and critically acclaimed popular books \u201cThe Scientist in the\nCrib\u201d William Morrow, 1999, \u201cThe Philosophical Baby; What children\u2019s\nminds tell us about love, truth and the meaning of life\u201d 2009, and \u201cThe\nGardener and the Carpenter\u201d 2016, Farrar, Strauss and Giroux, the latter\ntwo won the Cognitive Development Society Best Book Prize in 2009 and\n2016. Since 2013 she has written the Mind and Matter column for the Wall\nStreet Journal and she has also written widely about cognitive science\nand psychology for The New York Times, The Economist, The Atlantic, The\nNew Yorker, Scientific American, The Times Literary Supplement, The New\nYork Review of Books, New Scientist and Slate, among others. Her TED\ntalk on her work has been viewed more than 5.2 million times. She has\nfrequently appeared on TV, radio and podcasts including \u201cThe Charlie\nRose Show\u201d, \u201cThe Colbert Report\u201d, \u201cRadio Lab\u201d and \u201cThe Ezra Klein Show\u201d.\nShe lives in Berkeley with her husband Alvy Ray Smith and has three\nchildren and five grandchildren.\n", "speaker_name": "Alison Gopnik", "institution": "University of California at Berkeley", "image": "invited2.jpg"}, {"id": "the-future-of-computational-linguistics-in-the-llm-age", "title": "The Future of Computational Linguistics in the LLM Age", "desc": "Chair: Iryna Gurevych\nTechnische Universit\u00e4t Darmstadt\n\nTuesday, July 11 - Time: 14:45-15:45\n\nThis is a panel discussion with:\n\n-   Dan Klein (UC Berkeley)\n\n-   Meg Mitchell (Hugging Face)\n\n-   Roy Schwartz (the Hebrew University of Jerusalem)\n\nThey will present short statements (5 to 7 min.) related to the main\ntopic of the panel\n\n-   New opportunities (e.g., artificial general intelligence,\n    responsible NLP);\n\n-   Technical challenges (e.g., multimodality, instruction-tuning, etc.)\n\n-   Real life problems & societal implications (e.g., hallucinations,\n    biases, future job market);\n\n-   LLMs and the future of NLP; and\n\n-   Open-science vs. commercial LLMs\n\nFollowed by discussion with the panel and audience.\n", "location": "Metropolitan", "start_time": "2023-07-11T14:45:00", "end_time": "2023-07-11T15:45:00", "bio": "", "speaker_name": "Dan Klein (UC Berkeley), Meg Mitchell (Hugging Face), Roy Schwartz (Hebrew University of Jerusalem)", "institution": null, "image": null}, {"id": "memorial", "title": "Memorial", "desc": "[image]\n\nTuesday, July 11, 2023 - Room: Metropolitan - Time: 13:00\u201313:30\n\nDragomir Radev, the A. Bartlett Giamatti Professor of Computer Science\nat Yale University, passed away this year on Wed, March 29th. Drago\ncontributed in substantial ways to research in NLP, to the organization\nof the ACL and to mentoring the next generation of computational\nlinguists. Drago\u2019s role in our ACL community spans four decades. He was\nrecognized for his work over this period through his selection as an ACL\nFellow in 2018 for his significant contributions to text summarization\nand question answering, and through his receipt of the Distinguished ACL\nService Award in 2022. In this session, speakers from different time\nperiods of his life will discuss his contributions to the field and the\nimpact his life had on so many of us.\n", "location": "Metropolitan", "start_time": "2023-07-11T13:00:00", "end_time": "2023-07-11T13:30:00", "bio": "", "speaker_name": null, "institution": null, "image": "drago.jpg"}, {"id": "acl-rolling-review-update-and-discussion", "title": "ACL Rolling Review Update and Discussion", "desc": "Mausam, Professor, IIT Delhi (ARR EIC), Jonathan K. Kummerfeld,\nAssistant Professor, University of Sydney (ARR CTO)\nTuesday, July 11, 2023 - Room: Metropolitan - Time: 14:15\u201314:45\n\nThis session will contain a presentation on progress in ARR over the\npast year and provide an opportunity for community questions and\ndiscussion.\n\nWe will briefly present:\n\nPersonnel Updates New aspects: Tracks, Senior Action Editors\nImprovements, e.g. changes to the review - paper matching process\nStatistics on timeliness and paper outcomes Next steps\n\nWith that context we will open the floor to questions.\n", "location": "Metropolitan", "start_time": "2023-07-11T14:15:00", "end_time": "2023-07-11T14:45:00", "bio": "", "speaker_name": null, "institution": null, "image": null}, {"id": "ethics-panel", "title": "Ethics Panel", "desc": "Kar\u00ebn Fort, Min-Yen Kan and Yulia Tsvetkov (ACL Ethics Committee\nco-chairs) Committee Members: Luciana Benotti, Mark Dredze, Pascale\nFung, Dirk Hovy, Jin-Dong Kim, Malvina Nissim\nTuesday, July 11, 2023 - Room: Pier 4&5 - Time: 16:15\u201317:45\n\nWe present our ACL Ethics Committee\u2019s progress over the last few years.\nOf core interest, we will present the results of the ACL stakeholder\nsurvey about the role of ethics and ethics training exposure. Results\nfrom the survey respondents indicate that ethics is of primary interest\nto the community and that there is a mandate for the further creation\nand dissemination of ethics related training for authors, reviewers and\nevent organisers. We will briefly review the survey results and feature\na lengthed question and answer session in support of extended dialogue\nwith our community. Our session will culminate through a dialogue with\nour session\u2019s participants in a moderated panel that includes\nparticipation from the entire ethics committee.\n", "location": "Pier 4\\&5", "start_time": "2023-07-11T16:15:00", "end_time": "2023-07-11T17:45:00", "bio": "", "speaker_name": null, "institution": null, "image": null}], "tutorials": [{"id": "t1_-goal-awareness-for-conversational-ai_-proactivity,-non-collaborativity,-and-beyond", "title": "T1: Goal Awareness for Conversational AI: Proactivity, Non-collaborativity, and Beyond", "desc": "Yang Deng, Wenqiang Lei, Minlie Huang and Tat-Seng Chua\n\nConversational systems are envisioned to provide social support or\nfunctional service to human users via natural language interactions.\nConventional conversation researches mainly focus on the responseability\nof the system, such as dialogue context understanding and response\ngeneration, but overlooks the design of an essential property in\nintelligent conversations, i.e., goal awareness. The awareness of goals\nmeans the state of not only being responsive to the users but also aware\nof the target conversational goal and capable of leading the\nconversation towards the goal, which is a significant step towards\nhigher-level intelligence and artificial consciousness. It can not only\nlargely improve user engagement and service efficiency in the\nconversation, but also empower the system to handle more complicated\nconversation tasks that involve strategical and motivational\ninteractions. In this tutorial, we will introduce the recent advances on\nthe design of agent\u2019s awareness of goals in a wide range of\nconversational systems.\n\n------------------------------------------------------------------------\n\nYang Deng, Ph.D. Candidate, Chinese University of Hong Kong\nemail: ydeng@se.cuhk.edu.hk\nwebsite: https://dengyang17.github.io\nYang Deng is a final-year Ph.D. candidate in The Chinese University of\nHong Kong. His research lies in natural language processing and\ninformation retrieval, especially for dialogue and QA systems. He has\npublished over 20 papers at top venues such as ACL, EMNLP, SIGIR,WWW,\nTKDE, and TOIS.\nWenqiang Lei, Professor, Sichuan University\nemail: wenqianglei@gmail.com\nwebsite: https://sites.google.com/view/wenqianghome/home\nWenqiang Lei is a Professor in Sichuan University. His research\ninterests focus on conversational AI, including conversational\nrecommendation, dialogue and QA systems. He has published relevant\npapers at top venues such as ACL, EMNLP, KDD, SIGIR, TOIS, and received\nthe ACM MM 2020 best paper award. He has given tutorials on the topic of\nconversational recommendation at RecSys 2021, SIGIR 2020, and\nco-organized special issues about conversational information seeking on\nACM Trans. on Web. Specifically, his tutorial on SIGIR 2020 accepts over\n1600 audiences, being one of the most popular tutorials in SIGIR 2020.\nMinlie Huang, Associate Professor, Tsinghua University\nemail: aihuang@tsinghua.edu.cn\nwebsite: http://coai.cs.tsinghua.edu.cn/hml\nMinlie Huang is an Associate Professor with the Department of Computer\nScience and Technology, Tsinghua University. He has authored or\nco-authored more than 100 papers in premier conferences and journals\n(ACL, EMNLP, TACL, etc). His research interests include natural language\nprocessing, particularly in dialog systems, reading comprehension, and\nsentiment analysis. He is an editor of TACL, CL, TNNLS, the Area Chair\nor SAC of ACL/EMNLP for more than 10 times. He is the recipient of IJCAI\n2018 distinguished paper award, a nominee of ACL 2019 best demo papers,\nand SIGDIAL 2020 best paper award.\nTat-Seng Chua, KITHCT Chair Professor, National University of Singapore\nemail: chuats@comp.nus.edu.sg\nwebsite: https://www.chuatatseng.com\nTat-Seng Chua is the KITHCT Chair Professor with the School of\nComputing, National University of Singapore. His main research interest\ninclude multimedia information retrieval and social media analytics. He\nis the 2015 winner of the prestigious ACM SIGMM Technical Achievement\nAward and receives the best papers (or candidates) over 10 times in top\nconferences (SIGIR, WWW, MM, etc). He serves as the general co-chair of\ntop conferences multiple times (MM 2005, SIGIR 2008, WSDM 2023, etc),\nand the editors of multiple journals (TOIS, TMM, etc). He has given\ninvited keynote talks at multiple top conferences, including the recent\none on the topic of multimodal conversational search and recommendation.\n", "location": "Metropolitan East", "start_time": "2023-07-09T09:00:00", "end_time": "2023-07-09T12:30:00", "hosts": ["Minlie Huang", "Tat-Seng Chua", "Wenqiang Lei", "Yang Deng"]}, {"id": "t2_-complex-reasoning-in-natural-language", "title": "T2: Complex Reasoning in Natural Language", "desc": "Wenting Zhao, Mor Geva, Bill Yuchen Lin, Michihiro Yasunaga, Aman Madaan\nand Tao Yu\n\nhttps://wenting-zhao.github.io/complex-reasoning-tutorial\n\nTeaching machines to reason over texts has been a long-standing goal of\nnatural language processing (NLP). To this end, researchers have\ndesigned a diverse set of complex reasoning tasks that involve\ncompositional reasoning, knowledge retrieval, grounding, commonsense\nreasoning, etc.\n\nA standard choice for building systems that perform a desired type of\nreasoning is to fine-tune a pretrained language model (LM) on specific\ndownstream tasks. However, recent research has demonstrated that such a\nstraightforward approach is often brittle. For example, Elazar et al.\n(2021) and Branco et al. (2021) show that, on question-answering (QA)\ntasks, similar performance can be achieved with questions removed from\nthe inputs. Min et al. (2019), Chen and Durrett (2019), and Tang et al.\n(2021) show that models trained on multi-hop QA do not generalize to\nanswer single-hop questions. The reasoning capabilities of these models\nthus remain at a surface level, i.e., exploiting data patterns.\nConsequently, augmenting LMs with techniques that make them robust and\neffective becomes an active research area.\n\nWe will start the tutorial by providing an overview of complex reasoning\ntasks where the standard application of pretrained language models\nfails. This tutorial then reviews recent promising directions for\ntackling these tasks. Specifically, we focus on the following groups of\napproaches that explicitly consider problem structures: (1)\nknowledge-augmented methods, where the knowledge is either incorporated\nduring fine-tuning or pretraining; (2) few-shot prompting methods, which\neffectively guide the models to follow instructions; (3) neuro-symbolic\nmethods, which produce explicit intermediate representations; and, (4)\nrationale-based methods, one of the most popular forms of the\nneuro-symbolic methods, which highlight subsets of input as explanations\nfor individual model predictions.\n\n------------------------------------------------------------------------\n\nWenting Zhao, Ph.D. student, Cornell University\nemail: wzhao@cs.cornell.edu\nWenting Zhao is a Ph.D. student in Computer Science at Cornell\nUniversity. Her research focuses on the intersection of reasoning and\nNLP. She is especially interested in developing explainable methods for\ncomplex reasoning problems. He has published over 20 papers at top\nvenues such as ACL, EMNLP, SIGIR,WWW, TKDE, and TOIS.\nMor Geva, Postdoctoral Researcher, Google Research\nemail: pipek@google.com\nMor Geva is a postdoctoral researcher, now at Google Research and\npreviously at the Allen Institute for AI. Her research focuses on\ndebugging the inner workings of black-box NLP models, to increase their\ntransparency, control their operation, and improve their reasoning\nabilities. She is organizing the next edition of the Workshop on\nCommonsense Reasoning and Representation.\nBill Yuchen Lin, Postdoctoral Researcher, Allen Institute for AI\nemail: yuchenl@allenai.org\nBill Yuchen Lin is a postdoctoral researcher at the Allen Institute for\nAI. He obtained his Ph.D. at USC advised by Prof. Xiang Ren. His\nresearch goal is to teach machines to think, talk, and act with\ncommonsense knowledge and commonsense reasoning ability as humans do. He\nwas a co-author of the tutorial on Knowledge-Augmented Methods for\nNatural Language Processing and the Workshop on Commonsense\nRepresentation and Reasoning at ACL 2022.\nMichihiro Yasunaga, Ph.D. Student, Stanford University\nemail: myasu@cs.stanford.edu\nMichihiro Yasunaga is a Ph.D. student in Computer Science at Stanford\nUniversity. His research interest is in developing generalizable models\nwith knowledge, including commonsense, science, and reasoning abilities.\nHe co-organized the Workshop on Structured and Unstructured Knowledge\nIntegration (SUKI) at NAACL 2022.\nAman Madaan, Ph.D. Student, Carnegie Mellon University\nemail: amadaan@cs.cmu.edu\nAman Madaan is a Ph.D. student at the School of Computer Science,\nCarnegie Mellon University. He is interested in large language models,\nfeedback-driven generation, and the intersection of code generation and\nnatural language reasoning. He helped organize the 1st and 2nd Workshops\non Natural Language Generation, Evaluation, and Metrics (GEM) at ACL\n2021 and EMNLP 2022.\nTao Yu, Assistant Professor, University of Hong Kong\nemail: tyu@cs.hku.hk\nTao Yu is an assistant professor of computer science at The University\nof Hong Kong. He completed his Ph.D. at Yale University and was a\npostdoctoral fellow at the University of Washington. He works on\nexecutable language understanding, such as semantic parsing and code\ngeneration, and large LMs. Tao is the recipient of an Amazon Research\nAward. He co-organized multiple workshops in Semantic Parsing and\nStructured and Unstructured Knowledge Integration at EMNLP and NAACL.\n", "location": "Metropolitan Centre", "start_time": "2023-07-09T09:00:00", "end_time": "2023-07-09T12:30:00", "hosts": ["Aman Madaan", "Bill Yuchen Lin", "Michihiro Yasunaga", "Mor Geva", "Tao Yu", "Wenting Zhao"]}, {"id": "t3_-everything-you-need-to-know-about-multilingual-llms_-towards-fair,-performant-and-reliable-models-for-languages-of-the-world", "title": "T3: Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world", "desc": "Sunayana Sitaram, Monojit Choudhury, Barun Patra, Vishrav Chaudhary,\nKabir Ahuja and Kalika Bali\n\nThis tutorial will describe various aspects of scaling up language\ntechnologies to many of the world\u2019s languages by describing the latest\nresearch in Massively Multilingual Language Models (MMLMs). We will\ncover topics such as data collection, training and fine-tuning of\nmodels, Responsible AI issues such as fairness, bias and toxicity,\nlinguistic diversity and evaluation in the context of MMLMs,\nspecifically focusing on issues in non-English and low-resource\nlanguages. Further, we will also talk about some of the real-world\nchallenges in deploying these models in language communities in the\nfield. With the performance of MMLMs improving in the zero-shot setting\nfor many languages, it is now becoming feasible to use them for building\nlanguage technologies in many languages of the world, and this tutorial\nwill provide the computational linguistics community with unique\ninsights from the latest research in multilingual models\n\n------------------------------------------------------------------------\n\nSunayana Sitaram, Senior Researcher, Microsoft Research India\nemail: sunayana,sitaram@microsoft.com\nSunayana Sitaram is a Senior Researcher at Microsoft Research India,\nwhere she works on multilingual speech and NLP. Her current research\ninterests include training and evaluation of Massively Multilingual\nLanguage Models and Responsible AI for NLP. Prior to coming to MSRI as a\nPost Doc, Sunayana completed her MS and PhD at the Language Technologies\nInstitute, Carnegie Mellon University in 2015. Sunayana\u2019s research has\nbeen published in top NLP and Speech conferences including ACL, NAACL,\nEMNLP, Interspeech, ICASSP. She has organized special sessions and\nworkshops on under-resourced languages, code-switching, multilingual\nevaluation and speech for social good. She has also led the creation of\nseveral benchmarks and datasets in code-switching, ASR, NLI and TTS that\nhave been used by research groups all over the world.\nMonojit Choudhury, Principal Applied Scientist, Microsoft Turing\nemail: monojitc@microsoft.com\nMonojit Choudhury is a Principal Applied Scientist at Microsoft Turing,\nprior to which he was a Principal Researcher at Microsoft Research\nIndia. He is also a Professor of Practice at Plaksha University, and had\nheld adjunct faculty positions at Ashoka University, IIIT Hyderabad and\nIIT Kharagpur. Over the past 15 years, Monojit has worked on several\nimpactful projects on processing of code-mixed text, evaluation and\nlinguistic fairness of large language models, and social impact through\nparticipatory design of technology for under-resourced languages like\nGondi, Mundari, Idu Mishmi and Swahili. Monojit has served as Senior\nArea Chair and Area chair in leading NLP and AI conferences including\nEMNLP, ACL, NAACL, IJCNLP and AAAI. He has organized several successful\nworkshops in *ACL conferences (SUMEval 2022, CALCS series, TextGraph\nseries, etc.) and has delivered a tutorial on Code-mixed text processing\nat EMNLP 2019. He is the general chair of the Panini Linguistics\nOlympiad and the founding co-chair of Asia Pacific Linguistics Olympiad\n\u2013- programs to introduce bright young students to linguistics and\ncomputational linguistics through puzzles. Dr. Choudhury holds PhD and\nB.Tech degrees in Computer Science and Engineering from IIT Kharagpur.\nVishrav Chaudhary, Principal Researcher, Microsoft Turing\nemail: vchaudhary@microsoft.com\nVishrav Chaudhary is a Principal Researcher at Microsoft Turing where he\nworks on scaling and building efficient Multilingual and Multimodal\nrepresentation and generation models. Prior to Microsoft, Vishrav was a\nLead Researcher at FAIR and focused on several aspects of Machine\nTranslation, Quality Estimation and Cross-lingual understanding. Over\nthe past 10 years, Vishrav\u2019s research work has been published in several\nleading NLP and AI conferences and journals including ACL, EMNLP, NAACL,\nEACL, AACL, TACL, JMLR and AMTA. He has also organized several workshops\nsuccessfully including SUMEval 2022, AmericasNLP 2021, WMT 2021 etc. He\nhas also served as an Area Chair for EMNLP 2022. Vishrav has also led\ncreation of benchmarks and datasets targeting 100+ languages which have\nbeen used to train state-of-the-art Cross-Lingual Representation and\nMachine Translation models.\nBarun Patra, Applied Scientist, Microsoft Turing\nemail: bapatra@microsoft.com\nBarun Patra is an Applied Scientist at Microsoft Turing. His research\ninterest revolves around building better foundational models that can\nhelp support numerous NLP tasks across different languages. Barun\u2019s\nresearch work focuses on improving the quality and efficiency of\ntraining these large multilingual foundational models, helping achieve\nstate-of-the-art performance on crosslingual NLP tasks.\nKabir Ahuja, Research Fellow, Microsoft Research India\nemail: t-kabirahuja@microsoft.com\nKabir Ahuja is a Research Fellow at Microsoft Research India, where he\nworks on building linguistically fair multilingual models covering\ndifferent aspects around their performance, calibration, evaluation,\ninterpretation, and data collection. He is also interested in the\nanalysis and interpretability of the computation mechanisms utilized by\nneural sequence models for solving different tasks.\nKalika Balia, Principal Researcher, Microsoft Research India\nemail: kalikab@microsoft.com\nKalika Bali is a Principal Researcher at Microsoft Research India\nworking in the areas of Machine Learning, Natural Language Systems and\nApplications, as well as Technology for Emerging Markets. Her research\ninterests lie broadly in the area of Speech and Language Technology\nespecially in the use of linguistic models for building technology that\noffers a more natural Human- Computer as well as Computer-Mediated\ninteractions.\n", "location": "Metropolitan West", "start_time": "2023-07-09T09:00:00", "end_time": "2023-07-09T12:30:00", "hosts": ["Barun Patra", "Kabir Ahuja", "Kalika Balia", "Monojit Choudhury", "Sunayana Sitaram", "Vishrav Chaudhary"]}, {"id": "t4_-generating-text-from-language-models", "title": "T4: Generating Text from Language Models", "desc": "Afra Amini, Ryan Cotterell, John Hewitt, Clara Meister and Tiago\nPimentel\n\nAn increasingly large percentage of natural language processing (NLP)\ntasks center around the generation of text from probabilistic language\nmodels. Despite this trend, techniques for improving or specifying\npreferences in these generated texts rely mostly on intuition-based\nheuristics. Further, there lacks a unified presentation of their\nmotivations, practical implementation, successes and pitfalls.\nPractitioners must, therefore, choose somewhat blindly between\ngeneration algorithms\u2014like top-p sampling or beam search\u2014which can lead\nto wildly different results. At the same time, language generation\nresearch continues to criticize and improve the standard toolboxes,\nfurther adding entropy to the state of the field. In this tutorial, we\nwill provide a centralized and cohesive discussion of critical\nconsiderations when choosing how to generate from a language model. We\nwill cover a wide range of empirically-observed problems (like\ndegradation, hallucination, repetition) and their corresponding proposed\nalgorithmic solutions from recent research (like top-p sampling and its\nsuccessors). We will then discuss a subset of these algorithms under a\nunified light; most stochastic generation strategies can be framed as\nlocally adapting the probabilities of a model to avoid failure cases.\nFinally, we will then cover methods in controlled generation, that go\nbeyond just ensuring coherence to ensure text exhibits specific desired\nproperties. We aim for NLP practitioners and researchers to leave our\ntutorial with a unified framework which they can use to evaluate and\ncontribute to the latest research in language generation.\n\n------------------------------------------------------------------------\n\nAfra Amini, Ph.D. Student, ETH Z\u0171rich\nemail: afra.amini@inf.ethz.ch\nAfra Amini is a PhD student at ETH Z\u0171rich in the ETH AI Center. Her\ncurrent foci include language generation and parsing.\nRyan Cotterell, Assistant Professor, ETH Z\u0171rich\nemail: ryan.cotterell@inf.ethz.ch\nRyan Cotterell is an assistant professor at ETH Z\u0171rich in the Institute\nfor Machine Learning.\nJohn Hewitt, Ph.D. Student, Stanford University\nemail: johnhew@cs.stanford.edu\nJohn Hewitt Is a PhD student at Stanford University. His research\ntackles basic problems in learning models from broad distributions over\nlanguage, characterizing and understanding those models, and building\nsmaller, simpler models.\nClara Meister, Ph.D. Student, ETH Z\u0171rich\nemail: clara.meister@inf.ethz.ch\nClara Meister is a PhD student at ETH Zi\u0171rich in the Institute for\nMachine Learning and a Google PhD Fellow. Her current foci include\nlanguage generation, pyscholinguistics, and the general application of\nstatistical methods to natural language processing.\nTiago Pimentel, Ph.D. Student, University of Cambridge\nemail: tp472@cam.ac.uk\nTiago Pimentel is a PhD student at the University of Cambridge and a\nFacebook Fellow. His research focuses on information theory, and its\napplications to the analysis of pre-trained language models and natural\nlanguages.\n", "location": "Metropolitan East", "start_time": "2023-07-09T14:00:00", "end_time": "2023-07-09T17:30:00", "hosts": ["Afra Amini}, Ph.D. Student, ETH Z\\H{u", "Clara Meister}, Ph.D. Student, ETH Z\\H{u", "John Hewitt", "Ryan Cotterell}, Assistant Professor, ETH Z\\H{u", "Tiago Pimentel"]}, {"id": "t5_-indirectly-supervised-natural-language-processing", "title": "T5: Indirectly Supervised Natural Language Processing", "desc": "Wenpeng Yin, Muhao Chen, Ben Zhou, Qiang Ning, Kai-Wei Chang and Dan\nRoth\n\nThis tutorial targets researchers and practitioners who are interested\nin ML technologies for NLP from indirect supervision. In particular, we\nwill present a diverse thread of indirect supervision studies that try\nto answer the following questions: (i) when and how can we provide\nsupervision for a target task T, if all we have is data that corresponds\nto a related task T? (ii) humans do not use exhaustive supervision; they\nrely on occasional feedback, and learn from incidental signals from\nvarious sources; how can we effectively incorporate such supervision in\nmachine learning? (iii) how can we leverage multi-modal supervision to\nhelp NLP? To the end, we will discuss several lines of research that\naddress those challenges, including (i) indirect supervision from T\u2019\nthat handles T with outputs spanning from a moderate size to an open\nspace, (ii) the use of sparsely occurring and incidental signals, such\nas partial labels, noisy labels, knowledge-based constraints, and\ncross-domain or cross-task annotations\u2014all having statistical\nassociations with the task, (iii) principled ways to measure and\nunderstand why these incidental signals can contribute to our target\ntasks, and (iv) indirect supervision from vision-language signals. We\nwill conclude the tutorial by outlining directions for further\ninvestigation.\n\n------------------------------------------------------------------------\n\nWenpeng Yin, Assistant Professor, Penn State University\nemail: wenpeng@psu.edu\nwebsite: http://www.wenpengyin.org\nWenpeng Yin is an Assistant Professor in the Department of Computer\nScience and Engineering at Penn State University. Prior to joining Penn\nState, he was a tenure-track faculty member at Temple University\n(1/2022-12/2022), Senior Research Scientist at Salesforce Research\n(8/2019-12/2021), a postdoctoral researcher at UPenn (10/2017-7/2019),\nand got his Ph.D. degree from the Ludwig Maximilian University of\nMunich, Germany, in 2017. Dr. Yin\u2019s research focuses on natural language\nprocessing with three sub-areas: (i) learning from task instructions;\n(ii) information extraction; (iii) learning with limited supervision.\nMuhao Chen, Assistant Research Professor, USC\nemail: muhaoche@usc.edu\nwebsite: http://luka-group.github.io/\nMuhao Chen is an Assistant Research Professor of Computer Science at\nUSC, where he directs the Language Understanding and Knowledge\nAcquisition (LUKA) Group. His research focuses on data-driven machine\nlearning approaches for natural language understanding and knowledge\nacquisition. His work has been recognized with an NSF CRII Award, a\nCisco Faculty Research Award, an ACM SIGBio Best Student Paper Award,\nand a Best Paper Nomination at CoNLL. Muhao obtained his PhD degree from\nUCLA Department of Computer Science in 2019, and was a postdoctoral\nresearcher at UPenn prior to joining USC.\nBen Zhou, Ph.D. Student, University of Pennsylvania\nemail: xyzhou@seas.upenn.edu\nwebsite: http://xuanyu.me/\nBen Zhou is a fourth-year Ph.D. student at the Department of Computer\nand Information Science, University of Pennsylvania. Ben\u2019s research\ninterests are distant supervision extraction and experiential knowledge\nreasoning, and he has more than 5 recent papers on related topics. He is\na recipient of the ENIAC fellowship from the University of Pennsylvania,\nand a finalist of the CRA outstanding undergraduate researcher award.\nQiang Ning, Senior Applied Scientist, Amazon AWS AI\nemail: qning@amazon.com\nwebsite: https://www.qiangning.info/\nQiang Ning is currently a senior applied scientist at AWS AI (2022-).\nPrior to that, Qiang was an applied scientist at Alexa AI (2020-2022)\nand a research scientist at the Allen Institute for AI (2019-2020).\nQiang received his Ph.D. from the University of Illinois at\nUrbana-Champaign in 2019 in Electrical and Computer Engineering. Qiang\u2019s\nresearch interests span in information extraction, question answering,\nand the application of weak supervision methods in these NLP problems in\nboth theoretical and practical aspects.\nKai-Wei Chang, Associate Professor, University of California Los Angeles\nemail: kwchang@cs.ucla.edu\nwebsite: http://kwchang.net/\nKai-Wei Chang is an associate professor in the Department of Computer\nScience at the University of California Los Angeles. His research\ninterests include designing robust, fair, and accountable machine\nlearning methods for building reliable NLP systems. His awards include\nthe EMNLP Best Long Paper Award (2017), the KDD Best Paper Award (2010),\nand the Sloan Resaerch Fellowship (2021). Kai-Wei has given tutorials at\nNAACL 15, AAAI 16, FAccT18, EMNLP 19, AAAI 20, EMNLP 21, MLSS 21 on\ndifferent research topics.\nDan Roth, Eduardo D. Glandt Distinguished Professor, UPenn\nemail: danroth@seas.upenn.edu\nwebsite: http://www.cis.upenn.edu/\u00a0danroth\nDan Roth is the Eduardo D. Glandt Distinguished Professor at the\nDepartment of Computer and Information Science, UPenn, the NLP Lead at\nAWS AI Labs, and a Fellow of the AAAS, ACM, AAAI, and ACL. In 2017 Roth\nwas awarded the John McCarthy Award, the highest award the AI community\ngives to mid-career AI researchers. Roth was recognized \u201cfor major\nconceptual and theoretical advances in the modeling of natural language\nunderstanding, machine learning, and reasoning.\u201d Roth has published\nbroadly in machine learning, NLP, KRR, and learning theory, and has\ngiven keynote talks and tutorials in all ACL and AAAI major conferences.\nRoth was the Editor-in-Chief of JAIR until 2017, and was the program\nchair of AAAI\u201911, ACL\u201903 and CoNLL\u201902; he serves regularly as an area\nchair and senior program committee member in the major conferences in\nhis research areas.\n", "location": "Metropolitan Centre", "start_time": "2023-07-09T14:00:00", "end_time": "2023-07-09T17:30:00", "hosts": ["Ben Zhou", "Dan Roth", "Kai-Wei Chang", "Muhao Chen", "Qiang Ning", "Wenpeng Yin"]}, {"id": "t6_-retrieval-based-language-models-and-applications", "title": "T6: Retrieval-based Language Models and Applications", "desc": "Akari Asai, Sewon Min, Zexuan Zhong and Danqi Chen\n\nRetrieval-based language models (LMs) have shown impressive performance\non diverse NLP tasks. In this tutorial, we will provide a comprehensive\nand coherent overview of recent advances in retrieval-based LMs. We will\nstart by providing preliminaries covering the foundation of LMs (e.g.,\nmasked LMs, autoregressive LMs) and retrieval systems (e.g.,\nnearest-neighbor search). We will then detail recent progress in\nretrieval-based models, focusing on their model architectures and\nlearning approaches. Finally, we will show how retrieval-based LMs are\nadapted to downstream applications, and extended to multilingual and\nmulti-modal settings. Finally, we will use an exercise to showcase the\neffectiveness of retrieval-based LMs.\n\n------------------------------------------------------------------------\n\nAkari Asai, Ph.D. Student, University of Washington\nemail: akari@cs.washington.edu\nAkari Asai is a Ph.D. student in the Paul G. Allen School of Computer\nScience & Engineering at the University of Washington, advised by Prof.\nHannaneh Hajishirzi. Her research lies in natural language processing\nand machine learning. Her recent research focuses on question answering,\nretrieval-based LMs, multilingual NLP, and entity-aware representations.\nShe received the IBM Fellowship in 2022. She is a lead organizer of the\nWorkshop on Multilingual Information Access (NAACL 2022) and serves as\nan area chair in question answering at EACL 2023.\nSewon Min, Ph.D. Student, University of Washington\nemail: sewon@cs.washington.edu\nSewon Min is a Ph.D. student in the Paul G. Allen School of Computer\nScience & Engineering at the University of Washington, and a visiting\nresearcher at Meta AI. Her research spans question answering,\nrepresentation and retrieval of factoid knowledge, and language\nmodeling. She was a co-instructor and a co-organizer of multiple\ntutorials and workshops at ACL, NAACL-HLT, EMNLP, NeurIPS and AKBC,\nincluding a tutorial on Few-Shot NLP with Pretrained Language Models\n(ACL 2022), a tutorial on NLP for Long Sequences (NAACL-HLT 2021), and\nthe Workshop on Semiparametric Methods in NLP (ACL 2022).\nZexuan Zhong, Ph.D. Student, University of Washington\nemail: zzhong@cs.princeton.edu\nZexuan Zhong is a Ph.D. student in the Department of Computer Science at\nPrinceton University, advised by Prof. Danqi Chen. His research\ninterests lie in natural language processing and machine learning. His\nrecent research focuses on retrieval-based LMs, generalization of\nretrieval models, and efficient models in NLP. He received a J.P. Morgan\nPhD Fellowship in 2022.\nDanqi Chen, Assistant Professor, Princeton University\nemail: danqic@cs.princeton.edu\nDanqi Chen is an Assistant Professor of Computer Science at Princeton\nUniversity and co-leads the Princeton NLP Group. Her recent research\nfocuses on training, adapting, and understanding large LMs, and\ndeveloping scalable and generalizable NLP systems for question\nanswering, information extraction, and conversational agents. Danqi is a\nrecipient of a Sloan Fellowship, a Samsung AI Researcher of the Year\naward, outstanding paper awards from ACL 2016, EMNLP 2017 and ACL 2022,\nand multiple industry faculty awards. Danqi served as the program chair\nfor AKBC 2021 and (senior) area chairs for many *ACL conferences. She\ntaught a tutorial on \u201cOpen-domain Question Answering\u201d at ACL 2020.\n", "location": "Metropolitan West", "start_time": "2023-07-09T14:00:00", "end_time": "2023-07-09T17:30:00", "hosts": ["Akari Asai", "Danqi Chen", "Sewon Min", "Zexuan Zhong"]}], "workshops": [{"id": "workshop_1", "title": "W1 - The 17th International Workshop on Semantic Evaluation (SemEval)", "desc": "The 17th edition of SemEval features 12 TASKS on a range of topics, including tasks on idiomaticy detection and embedding, sarcasm detection, multilingual news similarity, and linking mathematical symbols to their descriptions. Several tasks are multilingual, and others ask for multimodal approaches.", "location": "Queens Quay", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://semeval.github.io/SemEval2023/", "chair": "Ritesh Kumar, Atul Kr. Ojha, A. Seza Do\u011fru\u00f6z, Giovanni Da San Martino, Harish Tayyar Madabushi"}, {"id": "workshop_2", "title": "W2 - The 12th Joint Conference on Lexical and Computational Semantics (*SEM)", "desc": "The 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023) is organized and sponsored by SIGLEX, the Special Interest Group of the ACL. *SEM brings together researchers interested in the semantics of natural languages and its computational modeling. The conference embraces data-driven, neural, and probabilistic approaches, as well as symbolic approaches and everything in between; practical applications as well as theoretical contributions are welcome. The long-term goal of *SEM is to provide a stable forum for the growing number of NLP researchers working on all aspects of semantics of (many and diverse!) natural languages.", "location": "Pier 5", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://sites.google.com/view/starsem2023", "chair": "Mohammad Taher Pilehvar, Jose Camacho-Collados, Alexis Palmer, Malihe Alikhani, Mert Inan"}, {"id": "workshop_3", "title": "W3 - The 4th Workshop on Computational Approaches to Discourse (CODI)", "desc": "The last ten years have seen a dramatic improvement in the ability of NLP systems to understand and produce words and sentences. This development has created a renewed interest in discourse phenomena as researchers move towards the processing of long-form text and conversations. There is a surge of activity in discourse parsing, coherence models, text summarization, corpora for discourse level reading comprehension, and discourse related/aided representation learning, to name a few, but the problems in computational approaches to discourse are still substantial. At this juncture, we have organized three Workshops on Computational Approaches to Discourse (CODI) at EMNLP 2020, EMNLP 2021 and COLING 2022 to bring together discourse experts and upcoming researchers. These workshops have catalyzed work to improve the speed and knowledge needed to solve such problems and have served as a forum for the discussion of suitable datasets and reliable evaluation methods.", "location": "Pier 9", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://sites.google.com/view/codi-2023/", "chair": "Chlo\u00e9 Braud, Christian Hardmeier, Junyi Jessy Li, Sharid Lo\u00e1iciga, Michael Strube, Amir Zeldes"}, {"id": "workshop_4", "title": "W4 - The 20th International Conference on Spoken Language Translation (IWSLT)", "desc": "The International Conference on Spoken Language Translation (IWSLT) is an annual scientific conference, associated with an open evaluation campaign on spoken language translation, where both scientific papers and system descriptions are presented.", "location": "Dockside 1", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://iwslt.org/2023/", "chair": "Marine Carpuat, Marcello Federico, Alex Waibel, Jan Niehues, Sebastian St\u00fcker, Elizabeth Salesky, Atul Kr. Ojha"}, {"id": "workshop_5", "title": "W5 - The 8th Workshop on Representation Learning for NLP (RepL4NLP)", "desc": "The 8th Workshop on Representation Learning for NLP aims to continue the success of the Repl4NLP workshop series, with the 1st Workshop on Representation Learning for NLP having received about 50 submissions and over 250 attendees - the second most attended collocated event at ACL'16 after WMT. The workshop was introduced as a synthesis of several years of independent *CL workshops focusing on vector space models of meaning, compositionality, and the application of deep neural networks and spectral methods to NLP. It provides a forum for discussing recent advances on these topics, as well as future research directions in linguistically motivated vector-based models in NLP. The workshop will take place in a hybrid setting, and, as in previous years, feature interdisciplinary keynotes, paper presentations, posters, as well as a panel discussion.", "location": "Harbour B", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://sites.google.com/view/repl4nlp2023", "chair": "Burcu Can, Maximilian Mozes, Samuel Cahyawijaya, Naomi Saphra, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Chen Zhao"}, {"id": "workshop_6", "title": "W6 - The 4th Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)", "desc": "The Natural Language Processing (NLP) community has, in recent years, demonstrated a notable focus on improving higher scores on standard benchmarks and taking the lead on community-wide leaderboards (e.g., GLUE, SentEval). While this aspiration has led to improvements in benchmark performance of (predominantly neural) models, it has also came at a cost, i.e., increased model complexity and the ever-growing amount of computational resources required for training and using the current state-of-the-art models. Moreover, the recent research efforts have, for the most part, failed to identify sources of empirical gains in models, often failing to empirically justify the model complexity beyond benchmark performance. \\newline Because of these easily observable trends, we have proposed the SustaiNLP workshop with the goal of promoting more sustainable NLP research and practices, with two main objectives: (1) encouraging development of more efficient NLP models; and (2) providing simpler architectures and empirical justification of model complexity. For both aspects, we will encourage submissions from all topical areas of NLP.", "location": "Harbour C", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://sites.google.com/view/sustainlp2023", "chair": "Nafise Sadat Moosavi, Iryna Gurevych, Yufang Hou, Gyuwan Kim, Young Jin Kim, Tal Schuster, Ameeta Agrawal"}, {"id": "workshop_7", "title": "W7 - The 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA)", "desc": "The BEA Workshop is a leading venue for NLP innovation in the context of educational applications. It is one of the largest one-day workshops in the ACL community with over 100 registered attendees in the past several years. The growing interest in educational applications and a diverse community of researchers involved resulted in the creation of the Special Interest Group in Educational Applications (SIGEDU) in 2017, which currently has over 300 members.", "location": "Harbour A", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://sig-edu.org/bea/2023", "chair": "Ekaterina Kochmar, Jill Burstein, Andrea Horbach, Ronja Laarmann-Quante, Nitin Madnani, Ana\u00efs Tack, Victoria Yaneva, Zheng Yuan, Torsten Zesch"}, {"id": "workshop_8", "title": "W8 - The 1st Workshop on Natural Language Reasoning and Structured Explanations", "desc": "With recent scaling of large pre-trained Transformer language models (LLMs), the scope of feasible NLP tasks has broadened. Significant recent work has focused on tasks that require some kind of natural language reasoning. A trajectory in question answering has led us from extraction-oriented datasets like SQuAD to \u201cmulti-hop\u201d reasoning datasets like HotpotQA and StrategyQA. Although LLMs have shown remarkable performance on most NLP tasks, it is often unclear why their answers follow from what they know. To address this gap, a new class of explanation techniques has emerged which play an integral part in structuring the reasoning necessary to solve these datasets. For example, the chain-of-thought paradigm leverages explanations as vehicles for LLMs to mimic human reasoning processes. Entailment trees offer a way to ground multi-step reasoning in a collection of verifiable steps. Frameworks like SayCan bridge high-level planning in language and with low-level action trajectories. As a result, we see a confluence of methods blending explainable machine learning/NLP, classical AI (especially theorem proving), and cognitive science (how do humans structure explanations?). This workshop aims to bring together a diverse set of perspectives from these different traditions and attempt to establish common ground for how these various kinds of explanation structures can tackle a broad class of reasoning problems in natural language and beyond.", "location": "Pier 4", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://nl-reasoning-workshop.github.io/", "chair": "Peter Clark, Ellie Pavlick, Denny Zhou, Noah Goodman, Sarah Wiegreffe, Felix Hill"}, {"id": "workshop_9", "title": "W9 - The 7th Workshop on Online Abuse and Harms (WOAH)", "desc": "The goal of The Workshop on Online Abuse and Harms (WOAH) is to advance research that develops, interrogates and applies computational methods for detecting, classifying and modelling online abuse.", "location": "Pier 7 and 8", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://www.workshopononlineabuse.com/", "chair": "Yi-Ling Chung, Aida Mostafazadeh Davani, Debora Nozza, Paul R\u00f6ttger, Zeerak Talat"}, {"id": "workshop_10", "title": "W10 - The 3rd Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc)", "desc": "The DialDoc workshop focuses on Document-Grounded Dialogue and Conversational Question Answering. Given the vast amount of content created every day in various mediums, it is a meaningful yet challenging task not only to make such content accessible to end users via various conversational interfaces, but also to make sure the responses provided by the models are grounded and faithful with respect to the knowledge sources.", "location": "Dockside 2", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://doc2dial.github.io/workshop2023/", "chair": "Roee Aharoni, Nouha Dziri, Song Feng, Yongbin Li, Yu Li, Hui Wan"}, {"id": "workshop_11", "title": "W11 - The 1st Workshop on Matching From Unstructured and Structured Data (MATCHING)", "desc": "Matching Entities from structured and unstructured sources is an important task in many  domains and applications such as HR and E-commerce. For example, in HR platforms/services, it is important to match resumes to job descriptions and job seekers to companies. Similarly in web platforms/services, it is important to match customers to businesses such as hotels and restaurant, among others. In such domains, it is also relevant to match \u201ctextual customer reviews\u201d to customers queries, and sentences (or phrases) as answers to customer questions. Recent advances in Natural Language Processing, Natural Language Understanding, Conversational AI, Language Generation, Machine Learning, Deep Learning, Data Management, Information Extraction, Knowledge Bases/Graphs, (MultiSingle Hop/Commonsense) Inference/Reasoning, Recommendation Systems, and others, have demonstrated promising results in different Matching tasks related (but not limited) to the previously mentioned domains. We believe that there is tremendous opportunity to further exploit and explore the use of advanced NLP (and language related) techniques applied to Matching tasks. Therefore, the goal of this workshop is to bring together the research communities (from academia and industry) of these related areas, that are interested in the development and the application of novel natural-language-based approaches/models/systems to address challenges around different Matching tasks.", "location": "Dockside 3", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://megagon.ai/matching-2023/", "chair": "Dunia Mladeni\u0107, Estevam Hruschka, Marko Grobelnik, Sajjadur Rahman, Tom Mitchell"}, {"id": "workshop_12", "title": "W12 - The 17th Workshop on Linguistic Annotation (LAW)", "desc": "Linguistic annotation of natural language corpora is the backbone of supervised methods of statistical natural language processing. The Linguistic Annotation Workshop (LAW) is the annual workshop of the ACL Special Interest Group on Annotation (SIGANN), and it provides a forum for the presentation and discussion of innovative research on all aspects of linguistic annotation, including the creation and evaluation of annotation schemes, methods for automatic and manual annotation, use and evaluation of annotation software and frameworks, representation of linguistic data and annotations, semi-supervised \u201chuman in the loop\u201d methods of annotation, crowd-sourcing approaches, and more. As in the past, the LAW will provide a forum for annotation researchers to work towards standardization, best practices, and interoperability of annotation information and software.", "location": "Pier 3", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://sigann.github.io/LAW-XVII-2023/", "chair": "Annemarie Friedrich, Jakob Prange, Amir Zeldes, Ines Rehbein"}, {"id": "workshop_13", "title": "W13 - The 22nd Workshop on Biomedical Natural Language Processing and Shared Tasks (BioNLP-ST)", "desc": "The BioNLP workshop associated with the ACL SIGBIOMED special interest group has established itself as the primary venue for presenting foundational research in language processing for the biological and medical domains. The workshop is running every year since 2002 and continues getting stronger. BioNLP welcomes and encourages work on languages other than English, and inclusion and diversity. BioNLP truly encompasses the breadth of the domain and brings together researchers in bio- and clinical NLP from all over the world. The workshop will continue presenting work on a broad and interesting range of topics in NLP. The interest to biomedical language has broadened significantly due to the COVID-19 pandemic and continues to grow: as access to information becomes easier and more people generate and access health-related text, it becomes clearer that only language technologies can enable and support adequate use of the biomedical text.", "location": "Pier 2", "start_time": "2023-07-13T09:00:00", "end_time": null, "url": "https://aclweb.org/aclwiki/BioNLP_Workshop", "chair": "Kevin Bretonnel Cohen, Dina Demner-Fushman, Sophia Ananiadou, Jun-ichi Tsujii"}, {"id": "workshop_14", "title": "W14 - The 5th Workshop on NLP for Conversational AI", "desc": "Over the past decades, mathematicians, linguists, and computer scientists have dedicated their efforts towards empowering human-machine communication in natural language. While in recent years the emergence of virtual personal assistants such as Siri, Alexa, Google Assistant, Cortana, and ChatGPT has pushed the field forward, they may still have numerous challenges. \\newline Following the success of the 4th NLP for Conversational AI workshop at ACL, The 5th NLP4ConvAI will be a one-day workshop, co-located with ACL 2023 in Toronto, Canada. The goal of this workshop is to bring together researchers and practitioners to discuss impactful research problems in this area, share findings from real-world applications, and generate ideas for future research directions. \\newline The workshop will include keynotes, posters, panel sessions, and a shared task. In keynote talks, senior technical leaders from industry and academia will share insights on the latest developments in the field. We would like to encourage researchers and students to share their prospects and latest discoveries. There will also be a panel discussion with noted conversational AI leaders focused on the state of the field, future directions, and open problems across academia and industry.", "location": "Harbour B", "start_time": "2023-07-14T09:00:00", "end_time": null, "url": "https://sites.google.com/view/5thnlp4convai/", "chair": "Abhinav Rastogi, Georgios Spithourakis, Yun-Nung (Vivian) Chen, Bing Liu, Yu Li, Elnaz Nouri, Alon Albalak, Alexandros Papangelis"}, {"id": "workshop_15", "title": "W15 - The 3rd Workshop on Trustworthy NLP (TrustNLP)", "desc": "Recent advances in Natural Language Processing, and the emergence of pretrained Large Language Models (LLM) specifically, have made NLP systems omnipresent in various aspects of our everyday life. In addition to traditional examples such as personal voice assistants, recommender systems, etc, more recent developments include content-generation models such as ChatGPT, text-to-image models (Dall-E), and so on. While these emergent technologies have an unquestionable potential to power various innovative NLP and AI applications, they also pose a number of challenges in terms of their safe and ethical use. To address such challenges, NLP researchers have formulated various objectives, e.g., intended to make models more fair, safe, and privacy-preserving. However, these objectives are often considered separately, which is a major limitation since it is often important to understand the interplay and/or tension between them. For instance, meeting a fairness objective might require access to users\u2019 demographic information, which creates tension with privacy objectives. The goal of this workshop is to move toward a more comprehensive notion of Trustworthy NLP, by bringing together researchers working on those distinct yet related topics, as well as their intersection.", "location": "Pier 4", "start_time": "2023-07-14T09:00:00", "end_time": null, "url": "https://trustnlpworkshop.github.io/", "chair": "Yada Pruksachatkun, Ninareh Mehrabi, Kai-Wei Chang, Aram Galystan, Jwala Dhamala, Anaelia Ovalle, Apurv Verma, Yang Trista Cao, Anoop Kumar, Rahul Gupta"}, {"id": "workshop_16", "title": "W16 - The 13th Workshop on Computational Approaches to Subjectivity, Sentiment \\& Social Media Analysis (WASSA)", "desc": "Subjectivity and Sentiment Analysis has become a highly developed research area, ranging from binary classification of reviews to the detection of complex emotion structures between entities found in text. This field has expanded both on a practical level, finding numerous successful applications in business, as well as on a theoretical level, allowing researchers to explore more complex research questions related to affective computing. Its continuing importance is also shown by the interest it generates in other disciplines such as Economics, Sociology, Psychology, Marketing, Crisis Management \\& Digital Humanities. \\\\newline The aim of WASSA 2023 is to bring together researchers working on Subjectivity, Sentiment Analysis, Emotion Detection and Classification and their applications to other NLP or real-world tasks (e.g. public health messaging, fake news, media impact analysis, social media mining, computational literary studies) and researchers working on interdisciplinary aspects of affect computation from text.", "location": "Hourbour C", "start_time": "2023-07-14T09:00:00", "end_time": null, "url": "https://wassa-workshop.github.io/", "chair": "Jeremy Barnes, Orph\u00e9e De Clercq, Roman Klinger, Valentin Barriere, Salvatore Giorgi, Joa\u00f5 Sedoc, Shabnam Tafreshi, Iqra Ameer, Necva B\u00f6l\u00fcc\u00fc, Hua Xu, Ali Al Bataineh"}, {"id": "workshop_17", "title": "W17 - The 5th Clinical Natural Language Processing Workshop (Clinical NLP)", "desc": "Clinical text is growing rapidly as electronic health records become pervasive. Much of the information recorded in a clinical encounter is located exclusively in provider narrative notes, which makes them indispensable for supplementing structured clinical data in order to better understand patient state and care provided. The methods and tools developed for the clinical domain have historically lagged behind the scientific advances in the general-domain NLP. Despite the substantial recent strides in clinical NLP, a substantial gap remains. The goal of this workshop is to address this gap by establishing a regular event in CL conferences that brings together researchers interested in developing state-of-the-art methods for the clinical domain. The focus is on improving NLP technology to enable clinical applications, and specifically, information extraction and modeling of narrative provider notes from electronic health records, patient encounter transcripts, and other clinical narratives.", "location": "Pier 7 and 8", "start_time": "2023-07-14T09:00:00", "end_time": null, "url": "https://clinical-nlp.github.io/2023/", "chair": "Asma Ben Abacha, Steven Bethard, Tristan Naumann, Kirk Roberts, Anna Rumshisky"}, {"id": "workshop_18", "title": "W18 - The 1st Workshop on Social Influence in Conversations (SICon)", "desc": "Social influence is the change in an individual's thoughts, feelings, attitudes, or behaviors that results from interaction with another individual or a group. For example, a buyer uses social influence skills to engage in trade-offs and build rapport when bargaining with a seller. A therapist uses social influence skills like persuasion to motivate a patient towards physical exercise. Social influence is a core function of human communication, and such scenarios are ubiquitous in everyday life, from negotiations to argumentation to behavioral interventions. Consequently, realistic human-machine conversations must reflect these social influence dynamics, making it essential to systematically model and understand them in dialogue research. This requires perspectives not only from NLP and AI research but also from game theory, emotion, communication, and psychology. \\\\newline We are excited to host the First Workshop on Social Influence in Conversations (SICon 2023). SICon 2023 will be a one-day hybrid event, co-located with ACL 2023. It would be the first venue that uniquely fosters a dedicated discussion on social influence within NLP while involving researchers from other disciplines such as affective computing and the social sciences. SICon 2023 features keynote talks, panel discussions, poster sessions, and lightning talks for accepted papers. We hope to bring together researchers and practitioners from a wide variety of disciplines to discuss important problems related to social influence, as well as share findings and recent advances. We encourage researchers of all stages and backgrounds to share their exciting work!", "location": "Harbour A", "start_time": "2023-07-14T09:00:00", "end_time": null, "url": "https://sites.google.com/view/sicon-2023/home", "chair": "Kushal Chawla, Weiyan Shi, Maximillian Chen, Liang Qiu, Yu Li, James Hale, Alexandros Papangelis, Gale Lucas, Zhou Yu"}, {"id": "workshop_19", "title": "W19 - The 1st Workshop on Computation and Written Language (CAWL)", "desc": "Most work on NLP focuses on language in its canonical written form. This has often led researchers to ignore the differences between written and spoken language or, worse, to conflate the two. Instances of conflation are statements like \u201cChinese is a logographic language\" or \u201cPersian is a right-to-left language\", variants of which can be found frequently in the ACL anthology. These statements confuse properties of the language with properties of its writing system. Ignoring differences between written and spoken language leads, among other things, to conflating different words that are spelled the same (e.g., English bass), or treating as different, words that have multiple spellings. \\\\newline text enFurthermore, methods for dealing with written language issues (e.g., various kinds of normalization or conversion) or for recognizing text input (e.g. OCR \\& handwriting recognition or text entry methods) are often regarded as precursors to NLP rather than as fundamental parts of the enterprise, despite the fact that most NLP methods rely centrally on representations derived from text rather than (spoken) language. This general lack of consideration of writing has led to much of the research on such topics to largely appear outside of ACL venues, in conferences or journals of neighboring fields such as speech technology (e.g., text normalization) or human-computer interaction (e.g., text entry). \\\\newline We will invite submissions on the relationship between written and spoken language, the properties of written language, the ways in which writing systems encode language, and applications specifically focused on characteristics of writing systems.", "location": "Pier 2", "start_time": "2023-07-14T09:00:00", "end_time": null, "url": "https://cawl.wellformedness.com/", "chair": "Kyle Gorman, Brian Roark, Richard Sproat"}, {"id": "workshop_20", "title": "W20 - The 3rd Workshop on NLP for Indigenous Languages of the Americas (AmericasNLP)", "desc": "AmericasNLP aims to (a) encourage research on NLP, computational linguistics, corpus linguistics, and speech around the globe to work on native American languages; (b) )connect researchers and professionals from underrepresented communities and native speakers of endangered languages with the machine learning and natural language processing communities; and (c) )promote research on both neural and non-neural machine learning approaches suitable for low-resource languages.", "location": "Pier 3", "start_time": "2023-07-14T09:00:00", "end_time": null, "url": "https://turing.iimas.unam.mx/americasnlp/", "chair": "Manuel Mager, Arturo Oncevay, Enora Rice, Abteen Ebrahimi, Shruti Rijhwani, Alexis Palmer, Katharina Kann"}, {"id": "workshop_21", "title": "W21 - The 5th Workshop on Narrative Understanding (WNU)", "desc": "This is the 5th iteration of the Narrative Understanding Workshop, which brings together an interdisciplinary group of researchers from AI, ML, NLP, Computer Vision and other related fields, as well as scholars from the humanities to discuss methods to improve automatic narrative understanding capabilities. The workshop will consist of talks from invited speakers, a panel of researchers and writers, and talks and posters from accepted papers.", "location": "Dockside 2", "start_time": "2023-07-14T09:00:00", "end_time": null, "url": "https://sites.google.com/umass.edu/wnu2023", "chair": "Nader Akoury, Faeze Brahman, Khyathi Chandu, Snigdha Chaturvedi, Elizabeth Clark, Mohit Iyyer"}, {"id": "workshop_22", "title": "W22 - The 20th Workshop on Computational Morphology and Phonology (SIGMORPHON)", "desc": "SIGMORPHON aims to bring together researchers interested in applying computational techniques to problems in morphology, phonology, and phonetics. Work that addresses orthographic issues is also welcome. Papers will be on substantial, original, and unpublished research on these topics, potentially including strong work in progress.", "location": "Dockside 3", "start_time": "2023-07-14T09:00:00", "end_time": null, "url": "https://sigmorphon.github.io/workshops/2023/", "chair": "Garrett Nicolai, Eleanor Chodroff, \u00c7a\u011fr\u0131 \u00c7\u00f6ltekin, Fred Mailhot"}], "social_events": [{"title": "Welcome Reception", "id": "welcome-reception", "day": "Sunday, July 9, 2023", "venue": "Westin Harbour Castle Hotel", "start_time": "2023-07-09T19:00:00", "end_time": "2023-07-09T21:30:00", "desc": "One entry ticket will be included with each full conference registration. To get admission into the event you will need to have your name badge on your person as the QR code that is located on your badge is how the ACL Staff member(s) Scan and account for admission(s). No name badge, no entrance.", "location": "Westin Harbour Castle Hotel: Harbour Ballroom"}, {"title": "Social Event", "id": "social-event", "day": "Tuesday, July 11, 2023", "venue": "Steam Whistle Brewing Company", "start_time": "2023-07-11T18:30:00", "end_time": "2023-07-11T22:00:00", "desc": "One entry ticket will be included with each full conference registration. To get admission into the event you will need to have your name badge on your person as the QR code that is located on your badge is how the ACL Staff member(s) Scan and account for admission(s). No name badge, no entrance.", "location": "Steam Whistle Brewing Company: Steam Whistle Brewing Company located at The Roundhouse, 255 Bremner Blvd, Toronto ON M5V 3M9"}]}